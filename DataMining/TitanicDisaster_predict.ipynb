{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TitanicDisaster_predict.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3Aabr-W34fJn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "63b4e5fd-4546-40cf-de4a-c5f587511eb9"
      },
      "source": [
        "# 12151411 심경수\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# read data (each train, test set)\n",
        "train = pd.read_csv(\"Titanic_train.csv\")\n",
        "test = pd.read_csv(\"Titanic_test.csv\")\n",
        "\n",
        "# train set, test set의 첫 5개 데이터\n",
        "# train.head()\n",
        "# test.head()\n",
        "\n",
        "# Null값 분석\n",
        "#print(\"train data NaN counts\\n===================================\")\n",
        "#train.isnull().sum()\n",
        "#print(\"test data NaN counts\\n===================================\")\n",
        "#test.isnull().sum()"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PassengerId</th>\n",
              "      <th>Pclass</th>\n",
              "      <th>Name</th>\n",
              "      <th>Sex</th>\n",
              "      <th>Age</th>\n",
              "      <th>SibSp</th>\n",
              "      <th>Parch</th>\n",
              "      <th>Ticket</th>\n",
              "      <th>Fare</th>\n",
              "      <th>Cabin</th>\n",
              "      <th>Embarked</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>892</td>\n",
              "      <td>3</td>\n",
              "      <td>Kelly, Mr. James</td>\n",
              "      <td>male</td>\n",
              "      <td>34.5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>330911</td>\n",
              "      <td>7.8292</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Q</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>893</td>\n",
              "      <td>3</td>\n",
              "      <td>Wilkes, Mrs. James (Ellen Needs)</td>\n",
              "      <td>female</td>\n",
              "      <td>47.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>363272</td>\n",
              "      <td>7.0000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>894</td>\n",
              "      <td>2</td>\n",
              "      <td>Myles, Mr. Thomas Francis</td>\n",
              "      <td>male</td>\n",
              "      <td>62.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>240276</td>\n",
              "      <td>9.6875</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Q</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>895</td>\n",
              "      <td>3</td>\n",
              "      <td>Wirz, Mr. Albert</td>\n",
              "      <td>male</td>\n",
              "      <td>27.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>315154</td>\n",
              "      <td>8.6625</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>896</td>\n",
              "      <td>3</td>\n",
              "      <td>Hirvonen, Mrs. Alexander (Helga E Lindqvist)</td>\n",
              "      <td>female</td>\n",
              "      <td>22.0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3101298</td>\n",
              "      <td>12.2875</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   PassengerId  Pclass  ... Cabin Embarked\n",
              "0          892       3  ...   NaN        Q\n",
              "1          893       3  ...   NaN        S\n",
              "2          894       2  ...   NaN        Q\n",
              "3          895       3  ...   NaN        S\n",
              "4          896       3  ...   NaN        S\n",
              "\n",
              "[5 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFsd1pR2HkQs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 성별 처리 : 남성(0), 여성(1)\n",
        "train['Sex'] = train['Sex'].replace(['male', 'female'], [0, 1])\n",
        "test['Sex'] = test['Sex'].replace(['male', 'female'], [0, 1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5odinVXQ8Tob",
        "colab": {}
      },
      "source": [
        "# 특정 속성과 생존여부와의 연관성 분석용 함수\n",
        "def comp_with_survive(feature) :\n",
        "    survived = train[train['Survived'] == 1][feature].value_counts()\n",
        "    dead = train[train['Survived'] == 0][feature].value_counts()\n",
        "    df = pd.DataFrame([survived, dead])\n",
        "    df.index = ['Survived', 'Dead']\n",
        "    df.plot(kind = 'bar', stacked = True, figsize = (5, 5))\n",
        "\n",
        "# 분포 확인\n",
        "# comp_with_survive('SibSp')\n",
        "# comp_with_survive('Parch')\n",
        "\n",
        "# 동반인원 집계\n",
        "train['Family'] = train['SibSp'] + train['Parch']\n",
        "test['Family'] = test['SibSp'] + test['Parch']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2r-pwlcFij8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train, test 데이터에 호칭 속성 'Title' 추가\n",
        "combine = [train, test]\n",
        "\n",
        "for dataset in combine :\n",
        "    dataset['Title'] = dataset.Name.str.extract('([A-Za-z]+)\\.', expand=False)\n",
        "\n",
        "# pd.crosstab(train['Sex'], train['Title'])\n",
        "# pd.crosstab(test['Sex'], test['Title'])\n",
        "\n",
        "# 분류한 호칭 별 생존자 평균 분석\n",
        "# train[['Survived', 'Title']].groupby(['Title'], as_index = False).count()\n",
        "\n",
        "# 호칭 전처리\n",
        "for dataset in combine :\n",
        "    dataset['Title'] = dataset['Title'].replace(['Capt', 'Col', 'Dr', 'Major', 'Rev', 'Sir'], 'Rare')\n",
        "    dataset['Title'] = dataset['Title'].replace(['Don', 'Jonkheer'], 'Royal')\n",
        "    dataset['Title'] = dataset['Title'].replace(['Countess', 'Lady', 'Dona'], 'RoyalFemale')\n",
        "    dataset['Title'] = dataset['Title'].replace(['Mlle', 'Ms'], 'Miss')\n",
        "    dataset['Title'] = dataset['Title'].replace( 'Mme', 'Mrs')\n",
        "    \n",
        "\n",
        "# 호칭을 매핑한 후 전처리하여 속성으로 사용\n",
        "train['Title'] = train['Title'].replace(['Mr', 'Mrs', 'Miss', 'Master', 'Royal', 'RoyalFemale', 'Rare'], [1, 2, 3, 4, 5, 6, 7])\n",
        "test['Title']  = test['Title'].replace(['Mr', 'Mrs', 'Miss', 'Master', 'Royal', 'RoyalFemale', 'Rare'], [1, 2, 3, 4, 5, 6, 7])\n",
        "\n",
        "# 만약 있을 Null Value를 대비하여 처리\n",
        "train['Title'] = train['Title'].fillna(0)\n",
        "test['Title'] = test['Title'].fillna(0)\n",
        "\n",
        "# 분류한 호칭 별 생존자 분포 분석\n",
        "# comp_with_survive('Title')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMoksEAmFmE5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Cabin의 첫 글자 (알파벳) 분석\n",
        "for dataset in combine :\n",
        "    dataset['Room'] = dataset.Cabin.str.extract('([A-Z])', expand=False)\n",
        "    dataset['Room'].fillna('a', inplace = True)\n",
        "    dataset['Room'] = dataset['Room'].replace(['a', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M',\n",
        "                                               'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z'], \n",
        "                                              [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, \n",
        "                                               15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26])\n",
        "    \n",
        "# comp_with_survive('Room')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yF1knzBJJHoH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 승선항구 처리 : S(1), Q(2), C(3)\n",
        "# 결측값은 수가 많은 집단인 S로 편입한다.\n",
        "train = train.fillna({\"Embarked\" : 'S'})\n",
        "train['Embarked'] = train['Embarked'].replace(['S', 'Q', 'C'], [1, 2, 3]).astype('int64')\n",
        "test['Embarked']  = test['Embarked'].replace(['S', 'Q', 'C'], [1, 2, 3]).astype('int64')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BC5oCBlFqw4F",
        "colab": {}
      },
      "source": [
        "# Age의 결측값 처리 : 평균값으로 대체\n",
        "train['Age'].fillna(train['Age'].mean(), inplace = True)\n",
        "test['Age'].fillna(test['Age'].mean(), inplace = True)\n",
        "\n",
        "# Fare의 결측값 처리 : Pclass의 평균값으로 대체\n",
        "# print(test[['Pclass', 'Fare']].groupby(['Pclass'], as_index = False).mean())\n",
        "# print(\"===========================\")\n",
        "# print(test[test[\"Fare\"].isnull()][\"Pclass\"])\n",
        "\n",
        "test['Fare'].fillna(test[test['Pclass'] ==3]['Fare'].mean(), inplace = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "h_ZwjXty0_tj",
        "colab": {}
      },
      "source": [
        "# 사용하지 않을 속성 제거\n",
        "x_train = train.drop(['PassengerId', 'Survived', 'Name', 'Ticket', 'Cabin', 'SibSp', 'Parch'], axis = 1)\n",
        "y_train = train['Survived']\n",
        "\n",
        "x_test = test.drop(['PassengerId', 'Name', 'Ticket', 'Cabin', 'SibSp', 'Parch'], axis = 1)\n",
        "\n",
        "# 정규화(Normalization) 도구 사용\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train_scale = RobustScaler().fit_transform(x_train)\n",
        "x_test_scale = RobustScaler().fit_transform(x_test)\n",
        "\n",
        "x_training, x_testing, y_training, y_testing = train_test_split(x_train_scale, y_train, test_size = 0.25, random_state = 66)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-X8FoCXt9HYy",
        "outputId": "56bce1c4-e984-4326-b822-9f84b4e62c81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# 1. Logistic Regression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# evaluate result\n",
        "LogReg = LogisticRegression().fit(x_training, y_training)\n",
        "result = (LogReg.predict(x_testing) == y_testing).sum() / len(y_testing)\n",
        "print(\"Logistic Regression(Standard)\\t:\" , result)"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Logistic Regression(Standard)\t: 0.7757847533632287\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7Ie66RpyNeNj",
        "outputId": "393a57a6-23d8-4462-d17a-ff92c7db18b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "# 2. SVM (Support Vector Machine)\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# 선형 SVM에 대해 StandardScaler, RobustScaler 데이터 적용\n",
        "SVM_Linear = SVC(kernel = 'linear', C = 1)\n",
        "SVM_Linear.fit(x_training, y_training)\n",
        "\n",
        "# 비선형 SVM에 대해 StandardScaler, RobustScaler 데이터 적용\n",
        "SVM_Nonlinear = SVC(kernel = 'rbf', C = 1.0, gamma = 10.0)\n",
        "SVM_Nonlinear.fit(x_training, y_training)\n",
        "\n",
        "pred_L = SVM_Linear.predict(x_testing)\n",
        "pred_NL = SVM_Nonlinear.predict(x_testing)\n",
        "\n",
        "# evaluate result\n",
        "print(\"SVM as Linear\\t\\t:\", (pred_L == y_testing).sum() / len(y_testing))\n",
        "print(\"SVM as Non_linear\\t:\", (pred_NL == y_testing).sum() / len(y_testing))"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SVM as Linear\t\t: 0.7757847533632287\n",
            "SVM as Non_linear\t: 0.757847533632287\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QXRTgOXL9OGS",
        "colab": {}
      },
      "source": [
        "# 3. DNN\n",
        "import keras\n",
        "from keras.layers import Dense, Dropout\n",
        "\n",
        "# Create DNN model\n",
        "def DNN_create() :\n",
        "    DNN = keras.Sequential()        # DNN 모델 생성\n",
        "    # Input layer\n",
        "    DNN.add(Dense(16, activation = 'relu', kernel_initializer='he_normal', input_dim = 8))\n",
        "    # Hidden Layer\n",
        "    DNN.add(Dense(32, activation = 'relu'))\n",
        "    DNN.add(Dropout(0.2))           # 무작위로 20%의 노드 비활성화\n",
        "    DNN.add(Dense(48, activation = 'relu'))\n",
        "    DNN.add(Dropout(0.2))           # 무작위로 20%의 노드 비활성화\n",
        "    DNN.add(Dense(64, activation = 'relu'))\n",
        "    DNN.add(Dropout(0.2))           # 무작위로 20%의 노드 비활성화\n",
        "    DNN.add(Dense(80, activation = 'relu'))\n",
        "    DNN.add(Dropout(0.2))           # 무작위로 20%의 노드 비활성화\n",
        "    DNN.add(Dense(64, activation = 'relu'))\n",
        "    DNN.add(Dropout(0.2))           # 무작위로 20%의 노드 비활성화\n",
        "    DNN.add(Dense(48, activation = 'relu'))\n",
        "    DNN.add(Dropout(0.2))           # 무작위로 20%의 노드 비활성화\n",
        "    DNN.add(Dense(32, activation = 'relu'))\n",
        "    DNN.add(Dropout(0.2))           # 무작위로 20%의 노드 비활성화\n",
        "    DNN.add(Dense(16, activation = 'relu'))\n",
        "    DNN.add(Dropout(0.2))           # 무작위로 20%의 노드 비활성화\n",
        "    # Output Layer\n",
        "    DNN.add(Dense(1, activation = 'sigmoid'))\n",
        "\n",
        "    # Compile DNN\n",
        "    DNN.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
        "    return DNN"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1aVXhtCswewW",
        "outputId": "69ffbfd9-ab59-4861-c93a-e3ee83905ce3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "DNN = DNN_create()\n",
        "\n",
        "log_DNN = DNN.fit(x_training, y_training, epochs = 1000, validation_data = (x_testing, y_testing))"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 668 samples, validate on 223 samples\n",
            "Epoch 1/1000\n",
            "668/668 [==============================] - 1s 798us/step - loss: 0.6981 - accuracy: 0.5479 - val_loss: 0.6842 - val_accuracy: 0.6547\n",
            "Epoch 2/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.6819 - accuracy: 0.6153 - val_loss: 0.6685 - val_accuracy: 0.6457\n",
            "Epoch 3/1000\n",
            "668/668 [==============================] - 0s 84us/step - loss: 0.6725 - accuracy: 0.6123 - val_loss: 0.6491 - val_accuracy: 0.6457\n",
            "Epoch 4/1000\n",
            "668/668 [==============================] - 0s 87us/step - loss: 0.6631 - accuracy: 0.6183 - val_loss: 0.6219 - val_accuracy: 0.6547\n",
            "Epoch 5/1000\n",
            "668/668 [==============================] - 0s 87us/step - loss: 0.6385 - accuracy: 0.6527 - val_loss: 0.5706 - val_accuracy: 0.7803\n",
            "Epoch 6/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.5902 - accuracy: 0.7066 - val_loss: 0.4885 - val_accuracy: 0.7848\n",
            "Epoch 7/1000\n",
            "668/668 [==============================] - 0s 101us/step - loss: 0.5474 - accuracy: 0.7290 - val_loss: 0.4541 - val_accuracy: 0.8027\n",
            "Epoch 8/1000\n",
            "668/668 [==============================] - 0s 92us/step - loss: 0.5677 - accuracy: 0.7305 - val_loss: 0.4682 - val_accuracy: 0.8072\n",
            "Epoch 9/1000\n",
            "668/668 [==============================] - 0s 109us/step - loss: 0.5383 - accuracy: 0.7350 - val_loss: 0.4350 - val_accuracy: 0.7892\n",
            "Epoch 10/1000\n",
            "668/668 [==============================] - 0s 93us/step - loss: 0.5205 - accuracy: 0.7814 - val_loss: 0.4380 - val_accuracy: 0.8072\n",
            "Epoch 11/1000\n",
            "668/668 [==============================] - 0s 98us/step - loss: 0.4955 - accuracy: 0.7799 - val_loss: 0.4372 - val_accuracy: 0.7892\n",
            "Epoch 12/1000\n",
            "668/668 [==============================] - 0s 92us/step - loss: 0.5174 - accuracy: 0.7605 - val_loss: 0.4448 - val_accuracy: 0.7937\n",
            "Epoch 13/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.4937 - accuracy: 0.7844 - val_loss: 0.4304 - val_accuracy: 0.8027\n",
            "Epoch 14/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.5093 - accuracy: 0.7979 - val_loss: 0.4261 - val_accuracy: 0.8027\n",
            "Epoch 15/1000\n",
            "668/668 [==============================] - 0s 95us/step - loss: 0.4910 - accuracy: 0.7904 - val_loss: 0.4218 - val_accuracy: 0.7982\n",
            "Epoch 16/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.4700 - accuracy: 0.7949 - val_loss: 0.4140 - val_accuracy: 0.8027\n",
            "Epoch 17/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.4891 - accuracy: 0.8024 - val_loss: 0.4168 - val_accuracy: 0.8027\n",
            "Epoch 18/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.4597 - accuracy: 0.8174 - val_loss: 0.4204 - val_accuracy: 0.7892\n",
            "Epoch 19/1000\n",
            "668/668 [==============================] - 0s 88us/step - loss: 0.4619 - accuracy: 0.8114 - val_loss: 0.4138 - val_accuracy: 0.8161\n",
            "Epoch 20/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.4665 - accuracy: 0.8159 - val_loss: 0.4166 - val_accuracy: 0.8117\n",
            "Epoch 21/1000\n",
            "668/668 [==============================] - 0s 92us/step - loss: 0.4434 - accuracy: 0.8054 - val_loss: 0.4088 - val_accuracy: 0.8027\n",
            "Epoch 22/1000\n",
            "668/668 [==============================] - 0s 91us/step - loss: 0.4764 - accuracy: 0.8069 - val_loss: 0.4123 - val_accuracy: 0.8117\n",
            "Epoch 23/1000\n",
            "668/668 [==============================] - 0s 108us/step - loss: 0.4370 - accuracy: 0.8234 - val_loss: 0.4087 - val_accuracy: 0.8161\n",
            "Epoch 24/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.4518 - accuracy: 0.8219 - val_loss: 0.4055 - val_accuracy: 0.8341\n",
            "Epoch 25/1000\n",
            "668/668 [==============================] - 0s 92us/step - loss: 0.4348 - accuracy: 0.8219 - val_loss: 0.4051 - val_accuracy: 0.8206\n",
            "Epoch 26/1000\n",
            "668/668 [==============================] - 0s 92us/step - loss: 0.4698 - accuracy: 0.8293 - val_loss: 0.4073 - val_accuracy: 0.8430\n",
            "Epoch 27/1000\n",
            "668/668 [==============================] - 0s 96us/step - loss: 0.4449 - accuracy: 0.8278 - val_loss: 0.4069 - val_accuracy: 0.8251\n",
            "Epoch 28/1000\n",
            "668/668 [==============================] - 0s 93us/step - loss: 0.4520 - accuracy: 0.8204 - val_loss: 0.3970 - val_accuracy: 0.8386\n",
            "Epoch 29/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.4561 - accuracy: 0.8234 - val_loss: 0.3998 - val_accuracy: 0.8386\n",
            "Epoch 30/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.4327 - accuracy: 0.8249 - val_loss: 0.3924 - val_accuracy: 0.8430\n",
            "Epoch 31/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.4482 - accuracy: 0.8054 - val_loss: 0.3931 - val_accuracy: 0.8475\n",
            "Epoch 32/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.4470 - accuracy: 0.8144 - val_loss: 0.3885 - val_accuracy: 0.8475\n",
            "Epoch 33/1000\n",
            "668/668 [==============================] - 0s 92us/step - loss: 0.4281 - accuracy: 0.8293 - val_loss: 0.3851 - val_accuracy: 0.8475\n",
            "Epoch 34/1000\n",
            "668/668 [==============================] - 0s 105us/step - loss: 0.4331 - accuracy: 0.8383 - val_loss: 0.3844 - val_accuracy: 0.8565\n",
            "Epoch 35/1000\n",
            "668/668 [==============================] - 0s 105us/step - loss: 0.4352 - accuracy: 0.8174 - val_loss: 0.3860 - val_accuracy: 0.8296\n",
            "Epoch 36/1000\n",
            "668/668 [==============================] - 0s 93us/step - loss: 0.4294 - accuracy: 0.8249 - val_loss: 0.3875 - val_accuracy: 0.8475\n",
            "Epoch 37/1000\n",
            "668/668 [==============================] - 0s 92us/step - loss: 0.4288 - accuracy: 0.8219 - val_loss: 0.3838 - val_accuracy: 0.8386\n",
            "Epoch 38/1000\n",
            "668/668 [==============================] - 0s 93us/step - loss: 0.4257 - accuracy: 0.8308 - val_loss: 0.3920 - val_accuracy: 0.8386\n",
            "Epoch 39/1000\n",
            "668/668 [==============================] - 0s 105us/step - loss: 0.4300 - accuracy: 0.8174 - val_loss: 0.3899 - val_accuracy: 0.8386\n",
            "Epoch 40/1000\n",
            "668/668 [==============================] - 0s 94us/step - loss: 0.3972 - accuracy: 0.8428 - val_loss: 0.3863 - val_accuracy: 0.8386\n",
            "Epoch 41/1000\n",
            "668/668 [==============================] - 0s 110us/step - loss: 0.4288 - accuracy: 0.8368 - val_loss: 0.3869 - val_accuracy: 0.8430\n",
            "Epoch 42/1000\n",
            "668/668 [==============================] - 0s 96us/step - loss: 0.4100 - accuracy: 0.8338 - val_loss: 0.3825 - val_accuracy: 0.8520\n",
            "Epoch 43/1000\n",
            "668/668 [==============================] - 0s 91us/step - loss: 0.4304 - accuracy: 0.8278 - val_loss: 0.3863 - val_accuracy: 0.8610\n",
            "Epoch 44/1000\n",
            "668/668 [==============================] - 0s 97us/step - loss: 0.4227 - accuracy: 0.8368 - val_loss: 0.3866 - val_accuracy: 0.8430\n",
            "Epoch 45/1000\n",
            "668/668 [==============================] - 0s 88us/step - loss: 0.4119 - accuracy: 0.8443 - val_loss: 0.3923 - val_accuracy: 0.8430\n",
            "Epoch 46/1000\n",
            "668/668 [==============================] - 0s 92us/step - loss: 0.4247 - accuracy: 0.8353 - val_loss: 0.3801 - val_accuracy: 0.8610\n",
            "Epoch 47/1000\n",
            "668/668 [==============================] - 0s 98us/step - loss: 0.4124 - accuracy: 0.8383 - val_loss: 0.3797 - val_accuracy: 0.8655\n",
            "Epoch 48/1000\n",
            "668/668 [==============================] - 0s 94us/step - loss: 0.4092 - accuracy: 0.8383 - val_loss: 0.3797 - val_accuracy: 0.8475\n",
            "Epoch 49/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.4045 - accuracy: 0.8458 - val_loss: 0.3868 - val_accuracy: 0.8430\n",
            "Epoch 50/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.4130 - accuracy: 0.8473 - val_loss: 0.3832 - val_accuracy: 0.8386\n",
            "Epoch 51/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.4119 - accuracy: 0.8278 - val_loss: 0.3918 - val_accuracy: 0.8341\n",
            "Epoch 52/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.4226 - accuracy: 0.8353 - val_loss: 0.3881 - val_accuracy: 0.8475\n",
            "Epoch 53/1000\n",
            "668/668 [==============================] - 0s 93us/step - loss: 0.4041 - accuracy: 0.8398 - val_loss: 0.3845 - val_accuracy: 0.8520\n",
            "Epoch 54/1000\n",
            "668/668 [==============================] - 0s 92us/step - loss: 0.3972 - accuracy: 0.8548 - val_loss: 0.3819 - val_accuracy: 0.8475\n",
            "Epoch 55/1000\n",
            "668/668 [==============================] - 0s 103us/step - loss: 0.3948 - accuracy: 0.8368 - val_loss: 0.3761 - val_accuracy: 0.8565\n",
            "Epoch 56/1000\n",
            "668/668 [==============================] - 0s 91us/step - loss: 0.4139 - accuracy: 0.8398 - val_loss: 0.3725 - val_accuracy: 0.8565\n",
            "Epoch 57/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.4116 - accuracy: 0.8338 - val_loss: 0.3772 - val_accuracy: 0.8610\n",
            "Epoch 58/1000\n",
            "668/668 [==============================] - 0s 88us/step - loss: 0.3932 - accuracy: 0.8443 - val_loss: 0.3752 - val_accuracy: 0.8565\n",
            "Epoch 59/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.3827 - accuracy: 0.8488 - val_loss: 0.3745 - val_accuracy: 0.8565\n",
            "Epoch 60/1000\n",
            "668/668 [==============================] - 0s 94us/step - loss: 0.3949 - accuracy: 0.8458 - val_loss: 0.3777 - val_accuracy: 0.8475\n",
            "Epoch 61/1000\n",
            "668/668 [==============================] - 0s 87us/step - loss: 0.4117 - accuracy: 0.8368 - val_loss: 0.3821 - val_accuracy: 0.8520\n",
            "Epoch 62/1000\n",
            "668/668 [==============================] - 0s 88us/step - loss: 0.4071 - accuracy: 0.8398 - val_loss: 0.3714 - val_accuracy: 0.8655\n",
            "Epoch 63/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.3908 - accuracy: 0.8398 - val_loss: 0.3658 - val_accuracy: 0.8610\n",
            "Epoch 64/1000\n",
            "668/668 [==============================] - 0s 88us/step - loss: 0.3962 - accuracy: 0.8398 - val_loss: 0.3680 - val_accuracy: 0.8655\n",
            "Epoch 65/1000\n",
            "668/668 [==============================] - 0s 88us/step - loss: 0.4049 - accuracy: 0.8323 - val_loss: 0.3704 - val_accuracy: 0.8610\n",
            "Epoch 66/1000\n",
            "668/668 [==============================] - 0s 91us/step - loss: 0.4079 - accuracy: 0.8383 - val_loss: 0.3666 - val_accuracy: 0.8655\n",
            "Epoch 67/1000\n",
            "668/668 [==============================] - 0s 93us/step - loss: 0.4025 - accuracy: 0.8368 - val_loss: 0.3645 - val_accuracy: 0.8655\n",
            "Epoch 68/1000\n",
            "668/668 [==============================] - 0s 88us/step - loss: 0.3913 - accuracy: 0.8413 - val_loss: 0.3626 - val_accuracy: 0.8655\n",
            "Epoch 69/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.3911 - accuracy: 0.8533 - val_loss: 0.3632 - val_accuracy: 0.8700\n",
            "Epoch 70/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.3776 - accuracy: 0.8548 - val_loss: 0.3669 - val_accuracy: 0.8520\n",
            "Epoch 71/1000\n",
            "668/668 [==============================] - 0s 104us/step - loss: 0.3998 - accuracy: 0.8443 - val_loss: 0.3661 - val_accuracy: 0.8520\n",
            "Epoch 72/1000\n",
            "668/668 [==============================] - 0s 91us/step - loss: 0.3851 - accuracy: 0.8368 - val_loss: 0.3650 - val_accuracy: 0.8610\n",
            "Epoch 73/1000\n",
            "668/668 [==============================] - 0s 108us/step - loss: 0.3904 - accuracy: 0.8383 - val_loss: 0.3713 - val_accuracy: 0.8475\n",
            "Epoch 74/1000\n",
            "668/668 [==============================] - 0s 108us/step - loss: 0.3932 - accuracy: 0.8503 - val_loss: 0.3694 - val_accuracy: 0.8700\n",
            "Epoch 75/1000\n",
            "668/668 [==============================] - 0s 108us/step - loss: 0.3802 - accuracy: 0.8488 - val_loss: 0.3594 - val_accuracy: 0.8610\n",
            "Epoch 76/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.3825 - accuracy: 0.8518 - val_loss: 0.3607 - val_accuracy: 0.8475\n",
            "Epoch 77/1000\n",
            "668/668 [==============================] - 0s 108us/step - loss: 0.3807 - accuracy: 0.8548 - val_loss: 0.3619 - val_accuracy: 0.8565\n",
            "Epoch 78/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.3790 - accuracy: 0.8533 - val_loss: 0.3667 - val_accuracy: 0.8610\n",
            "Epoch 79/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.3857 - accuracy: 0.8458 - val_loss: 0.3614 - val_accuracy: 0.8655\n",
            "Epoch 80/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.3740 - accuracy: 0.8548 - val_loss: 0.3567 - val_accuracy: 0.8700\n",
            "Epoch 81/1000\n",
            "668/668 [==============================] - 0s 91us/step - loss: 0.3900 - accuracy: 0.8443 - val_loss: 0.3657 - val_accuracy: 0.8655\n",
            "Epoch 82/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.3780 - accuracy: 0.8548 - val_loss: 0.3568 - val_accuracy: 0.8700\n",
            "Epoch 83/1000\n",
            "668/668 [==============================] - 0s 92us/step - loss: 0.3877 - accuracy: 0.8563 - val_loss: 0.3604 - val_accuracy: 0.8700\n",
            "Epoch 84/1000\n",
            "668/668 [==============================] - 0s 94us/step - loss: 0.3761 - accuracy: 0.8578 - val_loss: 0.3585 - val_accuracy: 0.8744\n",
            "Epoch 85/1000\n",
            "668/668 [==============================] - 0s 92us/step - loss: 0.3824 - accuracy: 0.8533 - val_loss: 0.3546 - val_accuracy: 0.8744\n",
            "Epoch 86/1000\n",
            "668/668 [==============================] - 0s 94us/step - loss: 0.3614 - accuracy: 0.8488 - val_loss: 0.3563 - val_accuracy: 0.8744\n",
            "Epoch 87/1000\n",
            "668/668 [==============================] - 0s 97us/step - loss: 0.3840 - accuracy: 0.8443 - val_loss: 0.3636 - val_accuracy: 0.8655\n",
            "Epoch 88/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.3870 - accuracy: 0.8593 - val_loss: 0.3621 - val_accuracy: 0.8655\n",
            "Epoch 89/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.3936 - accuracy: 0.8503 - val_loss: 0.3544 - val_accuracy: 0.8700\n",
            "Epoch 90/1000\n",
            "668/668 [==============================] - 0s 106us/step - loss: 0.3769 - accuracy: 0.8503 - val_loss: 0.3616 - val_accuracy: 0.8655\n",
            "Epoch 91/1000\n",
            "668/668 [==============================] - 0s 88us/step - loss: 0.3706 - accuracy: 0.8518 - val_loss: 0.3617 - val_accuracy: 0.8700\n",
            "Epoch 92/1000\n",
            "668/668 [==============================] - 0s 91us/step - loss: 0.3631 - accuracy: 0.8548 - val_loss: 0.3579 - val_accuracy: 0.8700\n",
            "Epoch 93/1000\n",
            "668/668 [==============================] - 0s 93us/step - loss: 0.3763 - accuracy: 0.8548 - val_loss: 0.3672 - val_accuracy: 0.8610\n",
            "Epoch 94/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.3822 - accuracy: 0.8458 - val_loss: 0.3707 - val_accuracy: 0.8610\n",
            "Epoch 95/1000\n",
            "668/668 [==============================] - 0s 91us/step - loss: 0.3888 - accuracy: 0.8413 - val_loss: 0.3621 - val_accuracy: 0.8700\n",
            "Epoch 96/1000\n",
            "668/668 [==============================] - 0s 104us/step - loss: 0.3750 - accuracy: 0.8623 - val_loss: 0.3634 - val_accuracy: 0.8700\n",
            "Epoch 97/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.3677 - accuracy: 0.8548 - val_loss: 0.3608 - val_accuracy: 0.8700\n",
            "Epoch 98/1000\n",
            "668/668 [==============================] - 0s 93us/step - loss: 0.3728 - accuracy: 0.8488 - val_loss: 0.3639 - val_accuracy: 0.8655\n",
            "Epoch 99/1000\n",
            "668/668 [==============================] - 0s 91us/step - loss: 0.3727 - accuracy: 0.8488 - val_loss: 0.3686 - val_accuracy: 0.8610\n",
            "Epoch 100/1000\n",
            "668/668 [==============================] - 0s 88us/step - loss: 0.3828 - accuracy: 0.8578 - val_loss: 0.3796 - val_accuracy: 0.8700\n",
            "Epoch 101/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.3711 - accuracy: 0.8443 - val_loss: 0.3708 - val_accuracy: 0.8700\n",
            "Epoch 102/1000\n",
            "668/668 [==============================] - 0s 93us/step - loss: 0.3795 - accuracy: 0.8518 - val_loss: 0.3700 - val_accuracy: 0.8700\n",
            "Epoch 103/1000\n",
            "668/668 [==============================] - 0s 91us/step - loss: 0.3712 - accuracy: 0.8563 - val_loss: 0.3659 - val_accuracy: 0.8700\n",
            "Epoch 104/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.3642 - accuracy: 0.8593 - val_loss: 0.3636 - val_accuracy: 0.8655\n",
            "Epoch 105/1000\n",
            "668/668 [==============================] - 0s 88us/step - loss: 0.3636 - accuracy: 0.8563 - val_loss: 0.3674 - val_accuracy: 0.8655\n",
            "Epoch 106/1000\n",
            "668/668 [==============================] - 0s 87us/step - loss: 0.3713 - accuracy: 0.8458 - val_loss: 0.3586 - val_accuracy: 0.8655\n",
            "Epoch 107/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.3663 - accuracy: 0.8413 - val_loss: 0.3673 - val_accuracy: 0.8655\n",
            "Epoch 108/1000\n",
            "668/668 [==============================] - 0s 104us/step - loss: 0.3723 - accuracy: 0.8473 - val_loss: 0.3611 - val_accuracy: 0.8700\n",
            "Epoch 109/1000\n",
            "668/668 [==============================] - 0s 103us/step - loss: 0.3655 - accuracy: 0.8548 - val_loss: 0.3666 - val_accuracy: 0.8700\n",
            "Epoch 110/1000\n",
            "668/668 [==============================] - 0s 88us/step - loss: 0.3607 - accuracy: 0.8518 - val_loss: 0.3684 - val_accuracy: 0.8700\n",
            "Epoch 111/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.3749 - accuracy: 0.8548 - val_loss: 0.3627 - val_accuracy: 0.8655\n",
            "Epoch 112/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.3790 - accuracy: 0.8578 - val_loss: 0.3667 - val_accuracy: 0.8655\n",
            "Epoch 113/1000\n",
            "668/668 [==============================] - 0s 88us/step - loss: 0.3538 - accuracy: 0.8548 - val_loss: 0.3736 - val_accuracy: 0.8610\n",
            "Epoch 114/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.3589 - accuracy: 0.8668 - val_loss: 0.3722 - val_accuracy: 0.8655\n",
            "Epoch 115/1000\n",
            "668/668 [==============================] - 0s 97us/step - loss: 0.3652 - accuracy: 0.8638 - val_loss: 0.3627 - val_accuracy: 0.8700\n",
            "Epoch 116/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.3574 - accuracy: 0.8623 - val_loss: 0.3611 - val_accuracy: 0.8655\n",
            "Epoch 117/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.3712 - accuracy: 0.8563 - val_loss: 0.3627 - val_accuracy: 0.8655\n",
            "Epoch 118/1000\n",
            "668/668 [==============================] - 0s 91us/step - loss: 0.3460 - accuracy: 0.8533 - val_loss: 0.3713 - val_accuracy: 0.8655\n",
            "Epoch 119/1000\n",
            "668/668 [==============================] - 0s 88us/step - loss: 0.3481 - accuracy: 0.8668 - val_loss: 0.3683 - val_accuracy: 0.8700\n",
            "Epoch 120/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.3711 - accuracy: 0.8593 - val_loss: 0.3631 - val_accuracy: 0.8700\n",
            "Epoch 121/1000\n",
            "668/668 [==============================] - 0s 86us/step - loss: 0.3707 - accuracy: 0.8548 - val_loss: 0.3659 - val_accuracy: 0.8700\n",
            "Epoch 122/1000\n",
            "668/668 [==============================] - 0s 92us/step - loss: 0.3540 - accuracy: 0.8593 - val_loss: 0.3691 - val_accuracy: 0.8655\n",
            "Epoch 123/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.3523 - accuracy: 0.8623 - val_loss: 0.3712 - val_accuracy: 0.8655\n",
            "Epoch 124/1000\n",
            "668/668 [==============================] - 0s 88us/step - loss: 0.3584 - accuracy: 0.8608 - val_loss: 0.3637 - val_accuracy: 0.8610\n",
            "Epoch 125/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.3702 - accuracy: 0.8593 - val_loss: 0.3594 - val_accuracy: 0.8610\n",
            "Epoch 126/1000\n",
            "668/668 [==============================] - 0s 88us/step - loss: 0.3501 - accuracy: 0.8608 - val_loss: 0.3688 - val_accuracy: 0.8610\n",
            "Epoch 127/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.3736 - accuracy: 0.8413 - val_loss: 0.3701 - val_accuracy: 0.8565\n",
            "Epoch 128/1000\n",
            "668/668 [==============================] - 0s 93us/step - loss: 0.3625 - accuracy: 0.8548 - val_loss: 0.3653 - val_accuracy: 0.8655\n",
            "Epoch 129/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.3462 - accuracy: 0.8683 - val_loss: 0.3601 - val_accuracy: 0.8700\n",
            "Epoch 130/1000\n",
            "668/668 [==============================] - 0s 91us/step - loss: 0.3491 - accuracy: 0.8593 - val_loss: 0.3682 - val_accuracy: 0.8700\n",
            "Epoch 131/1000\n",
            "668/668 [==============================] - 0s 88us/step - loss: 0.3441 - accuracy: 0.8563 - val_loss: 0.3617 - val_accuracy: 0.8655\n",
            "Epoch 132/1000\n",
            "668/668 [==============================] - 0s 92us/step - loss: 0.3459 - accuracy: 0.8533 - val_loss: 0.3742 - val_accuracy: 0.8700\n",
            "Epoch 133/1000\n",
            "668/668 [==============================] - 0s 93us/step - loss: 0.3389 - accuracy: 0.8683 - val_loss: 0.3636 - val_accuracy: 0.8655\n",
            "Epoch 134/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.3536 - accuracy: 0.8713 - val_loss: 0.3723 - val_accuracy: 0.8700\n",
            "Epoch 135/1000\n",
            "668/668 [==============================] - 0s 101us/step - loss: 0.3685 - accuracy: 0.8578 - val_loss: 0.3671 - val_accuracy: 0.8565\n",
            "Epoch 136/1000\n",
            "668/668 [==============================] - 0s 87us/step - loss: 0.3393 - accuracy: 0.8578 - val_loss: 0.3777 - val_accuracy: 0.8565\n",
            "Epoch 137/1000\n",
            "668/668 [==============================] - 0s 87us/step - loss: 0.3498 - accuracy: 0.8608 - val_loss: 0.3631 - val_accuracy: 0.8700\n",
            "Epoch 138/1000\n",
            "668/668 [==============================] - 0s 88us/step - loss: 0.3474 - accuracy: 0.8698 - val_loss: 0.3784 - val_accuracy: 0.8655\n",
            "Epoch 139/1000\n",
            "668/668 [==============================] - 0s 87us/step - loss: 0.3385 - accuracy: 0.8533 - val_loss: 0.3815 - val_accuracy: 0.8655\n",
            "Epoch 140/1000\n",
            "668/668 [==============================] - 0s 88us/step - loss: 0.3283 - accuracy: 0.8683 - val_loss: 0.3823 - val_accuracy: 0.8655\n",
            "Epoch 141/1000\n",
            "668/668 [==============================] - 0s 94us/step - loss: 0.3496 - accuracy: 0.8638 - val_loss: 0.3687 - val_accuracy: 0.8700\n",
            "Epoch 142/1000\n",
            "668/668 [==============================] - 0s 88us/step - loss: 0.3497 - accuracy: 0.8698 - val_loss: 0.3675 - val_accuracy: 0.8655\n",
            "Epoch 143/1000\n",
            "668/668 [==============================] - 0s 88us/step - loss: 0.3424 - accuracy: 0.8608 - val_loss: 0.3576 - val_accuracy: 0.8565\n",
            "Epoch 144/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.3352 - accuracy: 0.8668 - val_loss: 0.3720 - val_accuracy: 0.8565\n",
            "Epoch 145/1000\n",
            "668/668 [==============================] - 0s 87us/step - loss: 0.3508 - accuracy: 0.8623 - val_loss: 0.3747 - val_accuracy: 0.8610\n",
            "Epoch 146/1000\n",
            "668/668 [==============================] - 0s 93us/step - loss: 0.3359 - accuracy: 0.8653 - val_loss: 0.3632 - val_accuracy: 0.8655\n",
            "Epoch 147/1000\n",
            "668/668 [==============================] - 0s 114us/step - loss: 0.3366 - accuracy: 0.8623 - val_loss: 0.3857 - val_accuracy: 0.8655\n",
            "Epoch 148/1000\n",
            "668/668 [==============================] - 0s 93us/step - loss: 0.3513 - accuracy: 0.8668 - val_loss: 0.3681 - val_accuracy: 0.8744\n",
            "Epoch 149/1000\n",
            "668/668 [==============================] - 0s 95us/step - loss: 0.3495 - accuracy: 0.8533 - val_loss: 0.3752 - val_accuracy: 0.8655\n",
            "Epoch 150/1000\n",
            "668/668 [==============================] - 0s 94us/step - loss: 0.3335 - accuracy: 0.8683 - val_loss: 0.3862 - val_accuracy: 0.8610\n",
            "Epoch 151/1000\n",
            "668/668 [==============================] - 0s 109us/step - loss: 0.3321 - accuracy: 0.8683 - val_loss: 0.3750 - val_accuracy: 0.8744\n",
            "Epoch 152/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.3332 - accuracy: 0.8713 - val_loss: 0.3878 - val_accuracy: 0.8700\n",
            "Epoch 153/1000\n",
            "668/668 [==============================] - 0s 94us/step - loss: 0.3440 - accuracy: 0.8578 - val_loss: 0.3702 - val_accuracy: 0.8700\n",
            "Epoch 154/1000\n",
            "668/668 [==============================] - 0s 94us/step - loss: 0.3426 - accuracy: 0.8683 - val_loss: 0.3848 - val_accuracy: 0.8610\n",
            "Epoch 155/1000\n",
            "668/668 [==============================] - 0s 94us/step - loss: 0.3231 - accuracy: 0.8683 - val_loss: 0.3995 - val_accuracy: 0.8520\n",
            "Epoch 156/1000\n",
            "668/668 [==============================] - 0s 93us/step - loss: 0.3576 - accuracy: 0.8623 - val_loss: 0.3938 - val_accuracy: 0.8520\n",
            "Epoch 157/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.3538 - accuracy: 0.8578 - val_loss: 0.3760 - val_accuracy: 0.8655\n",
            "Epoch 158/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.3357 - accuracy: 0.8638 - val_loss: 0.3886 - val_accuracy: 0.8655\n",
            "Epoch 159/1000\n",
            "668/668 [==============================] - 0s 92us/step - loss: 0.3371 - accuracy: 0.8713 - val_loss: 0.3740 - val_accuracy: 0.8700\n",
            "Epoch 160/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.3426 - accuracy: 0.8638 - val_loss: 0.3810 - val_accuracy: 0.8565\n",
            "Epoch 161/1000\n",
            "668/668 [==============================] - 0s 107us/step - loss: 0.3329 - accuracy: 0.8533 - val_loss: 0.3748 - val_accuracy: 0.8655\n",
            "Epoch 162/1000\n",
            "668/668 [==============================] - 0s 95us/step - loss: 0.3250 - accuracy: 0.8668 - val_loss: 0.3867 - val_accuracy: 0.8565\n",
            "Epoch 163/1000\n",
            "668/668 [==============================] - 0s 93us/step - loss: 0.3485 - accuracy: 0.8488 - val_loss: 0.3968 - val_accuracy: 0.8565\n",
            "Epoch 164/1000\n",
            "668/668 [==============================] - 0s 93us/step - loss: 0.3435 - accuracy: 0.8578 - val_loss: 0.3858 - val_accuracy: 0.8610\n",
            "Epoch 165/1000\n",
            "668/668 [==============================] - 0s 87us/step - loss: 0.3345 - accuracy: 0.8533 - val_loss: 0.3676 - val_accuracy: 0.8655\n",
            "Epoch 166/1000\n",
            "668/668 [==============================] - 0s 118us/step - loss: 0.3515 - accuracy: 0.8608 - val_loss: 0.3747 - val_accuracy: 0.8610\n",
            "Epoch 167/1000\n",
            "668/668 [==============================] - 0s 93us/step - loss: 0.3399 - accuracy: 0.8638 - val_loss: 0.3791 - val_accuracy: 0.8610\n",
            "Epoch 168/1000\n",
            "668/668 [==============================] - 0s 91us/step - loss: 0.3300 - accuracy: 0.8683 - val_loss: 0.4038 - val_accuracy: 0.8655\n",
            "Epoch 169/1000\n",
            "668/668 [==============================] - 0s 91us/step - loss: 0.3248 - accuracy: 0.8713 - val_loss: 0.3888 - val_accuracy: 0.8655\n",
            "Epoch 170/1000\n",
            "668/668 [==============================] - 0s 97us/step - loss: 0.3349 - accuracy: 0.8548 - val_loss: 0.4184 - val_accuracy: 0.8565\n",
            "Epoch 171/1000\n",
            "668/668 [==============================] - 0s 88us/step - loss: 0.3257 - accuracy: 0.8668 - val_loss: 0.4241 - val_accuracy: 0.8655\n",
            "Epoch 172/1000\n",
            "668/668 [==============================] - 0s 88us/step - loss: 0.3109 - accuracy: 0.8787 - val_loss: 0.4400 - val_accuracy: 0.8610\n",
            "Epoch 173/1000\n",
            "668/668 [==============================] - 0s 87us/step - loss: 0.3370 - accuracy: 0.8563 - val_loss: 0.4244 - val_accuracy: 0.8565\n",
            "Epoch 174/1000\n",
            "668/668 [==============================] - 0s 87us/step - loss: 0.3385 - accuracy: 0.8578 - val_loss: 0.3983 - val_accuracy: 0.8610\n",
            "Epoch 175/1000\n",
            "668/668 [==============================] - 0s 88us/step - loss: 0.3249 - accuracy: 0.8638 - val_loss: 0.4050 - val_accuracy: 0.8655\n",
            "Epoch 176/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.3204 - accuracy: 0.8638 - val_loss: 0.4105 - val_accuracy: 0.8610\n",
            "Epoch 177/1000\n",
            "668/668 [==============================] - 0s 93us/step - loss: 0.3267 - accuracy: 0.8743 - val_loss: 0.4166 - val_accuracy: 0.8610\n",
            "Epoch 178/1000\n",
            "668/668 [==============================] - 0s 93us/step - loss: 0.3132 - accuracy: 0.8683 - val_loss: 0.4278 - val_accuracy: 0.8700\n",
            "Epoch 179/1000\n",
            "668/668 [==============================] - 0s 92us/step - loss: 0.3128 - accuracy: 0.8802 - val_loss: 0.4237 - val_accuracy: 0.8655\n",
            "Epoch 180/1000\n",
            "668/668 [==============================] - 0s 88us/step - loss: 0.3318 - accuracy: 0.8728 - val_loss: 0.4376 - val_accuracy: 0.8341\n",
            "Epoch 181/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.3319 - accuracy: 0.8623 - val_loss: 0.4219 - val_accuracy: 0.8565\n",
            "Epoch 182/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.3399 - accuracy: 0.8668 - val_loss: 0.4029 - val_accuracy: 0.8610\n",
            "Epoch 183/1000\n",
            "668/668 [==============================] - 0s 103us/step - loss: 0.3279 - accuracy: 0.8713 - val_loss: 0.4059 - val_accuracy: 0.8700\n",
            "Epoch 184/1000\n",
            "668/668 [==============================] - 0s 92us/step - loss: 0.3163 - accuracy: 0.8713 - val_loss: 0.4035 - val_accuracy: 0.8610\n",
            "Epoch 185/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.3069 - accuracy: 0.8713 - val_loss: 0.4029 - val_accuracy: 0.8565\n",
            "Epoch 186/1000\n",
            "668/668 [==============================] - 0s 92us/step - loss: 0.3225 - accuracy: 0.8743 - val_loss: 0.3869 - val_accuracy: 0.8565\n",
            "Epoch 187/1000\n",
            "668/668 [==============================] - 0s 88us/step - loss: 0.3433 - accuracy: 0.8608 - val_loss: 0.3814 - val_accuracy: 0.8610\n",
            "Epoch 188/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.3457 - accuracy: 0.8638 - val_loss: 0.3836 - val_accuracy: 0.8610\n",
            "Epoch 189/1000\n",
            "668/668 [==============================] - 0s 105us/step - loss: 0.3215 - accuracy: 0.8713 - val_loss: 0.3843 - val_accuracy: 0.8610\n",
            "Epoch 190/1000\n",
            "668/668 [==============================] - 0s 95us/step - loss: 0.3321 - accuracy: 0.8638 - val_loss: 0.4177 - val_accuracy: 0.8565\n",
            "Epoch 191/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.3273 - accuracy: 0.8638 - val_loss: 0.4384 - val_accuracy: 0.8700\n",
            "Epoch 192/1000\n",
            "668/668 [==============================] - 0s 92us/step - loss: 0.3513 - accuracy: 0.8623 - val_loss: 0.4081 - val_accuracy: 0.8700\n",
            "Epoch 193/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.3060 - accuracy: 0.8772 - val_loss: 0.4435 - val_accuracy: 0.8655\n",
            "Epoch 194/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.2987 - accuracy: 0.8772 - val_loss: 0.4437 - val_accuracy: 0.8610\n",
            "Epoch 195/1000\n",
            "668/668 [==============================] - 0s 93us/step - loss: 0.3217 - accuracy: 0.8698 - val_loss: 0.4318 - val_accuracy: 0.8610\n",
            "Epoch 196/1000\n",
            "668/668 [==============================] - 0s 106us/step - loss: 0.3139 - accuracy: 0.8743 - val_loss: 0.4362 - val_accuracy: 0.8700\n",
            "Epoch 197/1000\n",
            "668/668 [==============================] - 0s 95us/step - loss: 0.3299 - accuracy: 0.8698 - val_loss: 0.3924 - val_accuracy: 0.8610\n",
            "Epoch 198/1000\n",
            "668/668 [==============================] - 0s 102us/step - loss: 0.3328 - accuracy: 0.8638 - val_loss: 0.4015 - val_accuracy: 0.8744\n",
            "Epoch 199/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.3177 - accuracy: 0.8757 - val_loss: 0.4209 - val_accuracy: 0.8744\n",
            "Epoch 200/1000\n",
            "668/668 [==============================] - 0s 95us/step - loss: 0.3194 - accuracy: 0.8698 - val_loss: 0.4499 - val_accuracy: 0.8700\n",
            "Epoch 201/1000\n",
            "668/668 [==============================] - 0s 93us/step - loss: 0.3264 - accuracy: 0.8623 - val_loss: 0.4376 - val_accuracy: 0.8610\n",
            "Epoch 202/1000\n",
            "668/668 [==============================] - 0s 93us/step - loss: 0.3058 - accuracy: 0.8787 - val_loss: 0.4352 - val_accuracy: 0.8610\n",
            "Epoch 203/1000\n",
            "668/668 [==============================] - 0s 92us/step - loss: 0.3092 - accuracy: 0.8772 - val_loss: 0.4408 - val_accuracy: 0.8655\n",
            "Epoch 204/1000\n",
            "668/668 [==============================] - 0s 92us/step - loss: 0.3043 - accuracy: 0.8772 - val_loss: 0.4549 - val_accuracy: 0.8565\n",
            "Epoch 205/1000\n",
            "668/668 [==============================] - 0s 94us/step - loss: 0.3198 - accuracy: 0.8743 - val_loss: 0.4547 - val_accuracy: 0.8565\n",
            "Epoch 206/1000\n",
            "668/668 [==============================] - 0s 93us/step - loss: 0.3256 - accuracy: 0.8653 - val_loss: 0.4442 - val_accuracy: 0.8700\n",
            "Epoch 207/1000\n",
            "668/668 [==============================] - 0s 94us/step - loss: 0.3089 - accuracy: 0.8743 - val_loss: 0.4376 - val_accuracy: 0.8700\n",
            "Epoch 208/1000\n",
            "668/668 [==============================] - 0s 102us/step - loss: 0.3175 - accuracy: 0.8653 - val_loss: 0.4502 - val_accuracy: 0.8610\n",
            "Epoch 209/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.3253 - accuracy: 0.8757 - val_loss: 0.4070 - val_accuracy: 0.8655\n",
            "Epoch 210/1000\n",
            "668/668 [==============================] - 0s 93us/step - loss: 0.2997 - accuracy: 0.8757 - val_loss: 0.4432 - val_accuracy: 0.8655\n",
            "Epoch 211/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.3069 - accuracy: 0.8787 - val_loss: 0.4864 - val_accuracy: 0.8655\n",
            "Epoch 212/1000\n",
            "668/668 [==============================] - 0s 91us/step - loss: 0.3083 - accuracy: 0.8713 - val_loss: 0.4883 - val_accuracy: 0.8655\n",
            "Epoch 213/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.3183 - accuracy: 0.8683 - val_loss: 0.4582 - val_accuracy: 0.8610\n",
            "Epoch 214/1000\n",
            "668/668 [==============================] - 0s 99us/step - loss: 0.3255 - accuracy: 0.8653 - val_loss: 0.4472 - val_accuracy: 0.8610\n",
            "Epoch 215/1000\n",
            "668/668 [==============================] - 0s 93us/step - loss: 0.3081 - accuracy: 0.8743 - val_loss: 0.4736 - val_accuracy: 0.8655\n",
            "Epoch 216/1000\n",
            "668/668 [==============================] - 0s 96us/step - loss: 0.3312 - accuracy: 0.8668 - val_loss: 0.4589 - val_accuracy: 0.8655\n",
            "Epoch 217/1000\n",
            "668/668 [==============================] - 0s 91us/step - loss: 0.3139 - accuracy: 0.8653 - val_loss: 0.4673 - val_accuracy: 0.8565\n",
            "Epoch 218/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.3172 - accuracy: 0.8728 - val_loss: 0.4904 - val_accuracy: 0.8520\n",
            "Epoch 219/1000\n",
            "668/668 [==============================] - 0s 93us/step - loss: 0.3140 - accuracy: 0.8713 - val_loss: 0.4795 - val_accuracy: 0.8475\n",
            "Epoch 220/1000\n",
            "668/668 [==============================] - 0s 86us/step - loss: 0.3157 - accuracy: 0.8787 - val_loss: 0.4194 - val_accuracy: 0.8700\n",
            "Epoch 221/1000\n",
            "668/668 [==============================] - 0s 88us/step - loss: 0.3203 - accuracy: 0.8638 - val_loss: 0.4400 - val_accuracy: 0.8655\n",
            "Epoch 222/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.3132 - accuracy: 0.8847 - val_loss: 0.4504 - val_accuracy: 0.8655\n",
            "Epoch 223/1000\n",
            "668/668 [==============================] - 0s 91us/step - loss: 0.3136 - accuracy: 0.8728 - val_loss: 0.4628 - val_accuracy: 0.8655\n",
            "Epoch 224/1000\n",
            "668/668 [==============================] - 0s 92us/step - loss: 0.2926 - accuracy: 0.8772 - val_loss: 0.4737 - val_accuracy: 0.8610\n",
            "Epoch 225/1000\n",
            "668/668 [==============================] - 0s 96us/step - loss: 0.3278 - accuracy: 0.8623 - val_loss: 0.4681 - val_accuracy: 0.8610\n",
            "Epoch 226/1000\n",
            "668/668 [==============================] - 0s 104us/step - loss: 0.3111 - accuracy: 0.8668 - val_loss: 0.4783 - val_accuracy: 0.8565\n",
            "Epoch 227/1000\n",
            "668/668 [==============================] - 0s 92us/step - loss: 0.3122 - accuracy: 0.8698 - val_loss: 0.5108 - val_accuracy: 0.8520\n",
            "Epoch 228/1000\n",
            "668/668 [==============================] - 0s 94us/step - loss: 0.2962 - accuracy: 0.8743 - val_loss: 0.5396 - val_accuracy: 0.8565\n",
            "Epoch 229/1000\n",
            "668/668 [==============================] - 0s 96us/step - loss: 0.2911 - accuracy: 0.8847 - val_loss: 0.5522 - val_accuracy: 0.8430\n",
            "Epoch 230/1000\n",
            "668/668 [==============================] - 0s 104us/step - loss: 0.3025 - accuracy: 0.8713 - val_loss: 0.4918 - val_accuracy: 0.8520\n",
            "Epoch 231/1000\n",
            "668/668 [==============================] - 0s 96us/step - loss: 0.3059 - accuracy: 0.8787 - val_loss: 0.5087 - val_accuracy: 0.8655\n",
            "Epoch 232/1000\n",
            "668/668 [==============================] - 0s 95us/step - loss: 0.3047 - accuracy: 0.8653 - val_loss: 0.6102 - val_accuracy: 0.8610\n",
            "Epoch 233/1000\n",
            "668/668 [==============================] - 0s 92us/step - loss: 0.3204 - accuracy: 0.8638 - val_loss: 0.5214 - val_accuracy: 0.8744\n",
            "Epoch 234/1000\n",
            "668/668 [==============================] - 0s 92us/step - loss: 0.3128 - accuracy: 0.8757 - val_loss: 0.4755 - val_accuracy: 0.8655\n",
            "Epoch 235/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.2947 - accuracy: 0.8683 - val_loss: 0.5197 - val_accuracy: 0.8655\n",
            "Epoch 236/1000\n",
            "668/668 [==============================] - 0s 88us/step - loss: 0.3007 - accuracy: 0.8743 - val_loss: 0.5242 - val_accuracy: 0.8610\n",
            "Epoch 237/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.3231 - accuracy: 0.8757 - val_loss: 0.4786 - val_accuracy: 0.8655\n",
            "Epoch 238/1000\n",
            "668/668 [==============================] - 0s 96us/step - loss: 0.3170 - accuracy: 0.8623 - val_loss: 0.4961 - val_accuracy: 0.8655\n",
            "Epoch 239/1000\n",
            "668/668 [==============================] - 0s 85us/step - loss: 0.3140 - accuracy: 0.8653 - val_loss: 0.4950 - val_accuracy: 0.8700\n",
            "Epoch 240/1000\n",
            "668/668 [==============================] - 0s 101us/step - loss: 0.3448 - accuracy: 0.8683 - val_loss: 0.4826 - val_accuracy: 0.8520\n",
            "Epoch 241/1000\n",
            "668/668 [==============================] - 0s 116us/step - loss: 0.3164 - accuracy: 0.8728 - val_loss: 0.5156 - val_accuracy: 0.8520\n",
            "Epoch 242/1000\n",
            "668/668 [==============================] - 0s 91us/step - loss: 0.3058 - accuracy: 0.8698 - val_loss: 0.4803 - val_accuracy: 0.8565\n",
            "Epoch 243/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.3137 - accuracy: 0.8698 - val_loss: 0.4503 - val_accuracy: 0.8430\n",
            "Epoch 244/1000\n",
            "668/668 [==============================] - 0s 97us/step - loss: 0.3071 - accuracy: 0.8698 - val_loss: 0.4711 - val_accuracy: 0.8655\n",
            "Epoch 245/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.3043 - accuracy: 0.8728 - val_loss: 0.4511 - val_accuracy: 0.8655\n",
            "Epoch 246/1000\n",
            "668/668 [==============================] - 0s 113us/step - loss: 0.2973 - accuracy: 0.8877 - val_loss: 0.4968 - val_accuracy: 0.8565\n",
            "Epoch 247/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.3045 - accuracy: 0.8757 - val_loss: 0.5268 - val_accuracy: 0.8744\n",
            "Epoch 248/1000\n",
            "668/668 [==============================] - 0s 94us/step - loss: 0.3290 - accuracy: 0.8683 - val_loss: 0.4452 - val_accuracy: 0.8520\n",
            "Epoch 249/1000\n",
            "668/668 [==============================] - 0s 91us/step - loss: 0.3054 - accuracy: 0.8653 - val_loss: 0.4848 - val_accuracy: 0.8565\n",
            "Epoch 250/1000\n",
            "668/668 [==============================] - 0s 92us/step - loss: 0.3027 - accuracy: 0.8728 - val_loss: 0.5142 - val_accuracy: 0.8610\n",
            "Epoch 251/1000\n",
            "668/668 [==============================] - 0s 93us/step - loss: 0.3034 - accuracy: 0.8757 - val_loss: 0.5421 - val_accuracy: 0.8565\n",
            "Epoch 252/1000\n",
            "668/668 [==============================] - 0s 91us/step - loss: 0.3006 - accuracy: 0.8832 - val_loss: 0.4813 - val_accuracy: 0.8475\n",
            "Epoch 253/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.2889 - accuracy: 0.8892 - val_loss: 0.5185 - val_accuracy: 0.8700\n",
            "Epoch 254/1000\n",
            "668/668 [==============================] - 0s 91us/step - loss: 0.3591 - accuracy: 0.8653 - val_loss: 0.4754 - val_accuracy: 0.8655\n",
            "Epoch 255/1000\n",
            "668/668 [==============================] - 0s 94us/step - loss: 0.2989 - accuracy: 0.8802 - val_loss: 0.5147 - val_accuracy: 0.8475\n",
            "Epoch 256/1000\n",
            "668/668 [==============================] - 0s 96us/step - loss: 0.3238 - accuracy: 0.8772 - val_loss: 0.5547 - val_accuracy: 0.8341\n",
            "Epoch 257/1000\n",
            "668/668 [==============================] - 0s 93us/step - loss: 0.2906 - accuracy: 0.8772 - val_loss: 0.5670 - val_accuracy: 0.8341\n",
            "Epoch 258/1000\n",
            "668/668 [==============================] - 0s 99us/step - loss: 0.3000 - accuracy: 0.8728 - val_loss: 0.5413 - val_accuracy: 0.8520\n",
            "Epoch 259/1000\n",
            "668/668 [==============================] - 0s 92us/step - loss: 0.3174 - accuracy: 0.8653 - val_loss: 0.5387 - val_accuracy: 0.8475\n",
            "Epoch 260/1000\n",
            "668/668 [==============================] - 0s 93us/step - loss: 0.2885 - accuracy: 0.8862 - val_loss: 0.5550 - val_accuracy: 0.8655\n",
            "Epoch 261/1000\n",
            "668/668 [==============================] - 0s 94us/step - loss: 0.2869 - accuracy: 0.8847 - val_loss: 0.5990 - val_accuracy: 0.8655\n",
            "Epoch 262/1000\n",
            "668/668 [==============================] - 0s 94us/step - loss: 0.2939 - accuracy: 0.8832 - val_loss: 0.6279 - val_accuracy: 0.8565\n",
            "Epoch 263/1000\n",
            "668/668 [==============================] - 0s 91us/step - loss: 0.2957 - accuracy: 0.8757 - val_loss: 0.5969 - val_accuracy: 0.8565\n",
            "Epoch 264/1000\n",
            "668/668 [==============================] - 0s 87us/step - loss: 0.3037 - accuracy: 0.8787 - val_loss: 0.4747 - val_accuracy: 0.8475\n",
            "Epoch 265/1000\n",
            "668/668 [==============================] - 0s 91us/step - loss: 0.3015 - accuracy: 0.8832 - val_loss: 0.4724 - val_accuracy: 0.8700\n",
            "Epoch 266/1000\n",
            "668/668 [==============================] - 0s 91us/step - loss: 0.2939 - accuracy: 0.8817 - val_loss: 0.5095 - val_accuracy: 0.8610\n",
            "Epoch 267/1000\n",
            "668/668 [==============================] - 0s 88us/step - loss: 0.3030 - accuracy: 0.8757 - val_loss: 0.5239 - val_accuracy: 0.8565\n",
            "Epoch 268/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.3008 - accuracy: 0.8787 - val_loss: 0.5098 - val_accuracy: 0.8700\n",
            "Epoch 269/1000\n",
            "668/668 [==============================] - 0s 88us/step - loss: 0.3050 - accuracy: 0.8862 - val_loss: 0.5240 - val_accuracy: 0.8610\n",
            "Epoch 270/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.2944 - accuracy: 0.8743 - val_loss: 0.5025 - val_accuracy: 0.8655\n",
            "Epoch 271/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.2887 - accuracy: 0.8832 - val_loss: 0.5074 - val_accuracy: 0.8700\n",
            "Epoch 272/1000\n",
            "668/668 [==============================] - 0s 97us/step - loss: 0.3173 - accuracy: 0.8817 - val_loss: 0.4431 - val_accuracy: 0.8296\n",
            "Epoch 273/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.2825 - accuracy: 0.8892 - val_loss: 0.4758 - val_accuracy: 0.8700\n",
            "Epoch 274/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.3051 - accuracy: 0.8757 - val_loss: 0.5313 - val_accuracy: 0.8610\n",
            "Epoch 275/1000\n",
            "668/668 [==============================] - 0s 91us/step - loss: 0.2990 - accuracy: 0.8787 - val_loss: 0.5421 - val_accuracy: 0.8565\n",
            "Epoch 276/1000\n",
            "668/668 [==============================] - 0s 88us/step - loss: 0.2849 - accuracy: 0.8713 - val_loss: 0.5894 - val_accuracy: 0.8520\n",
            "Epoch 277/1000\n",
            "668/668 [==============================] - 0s 92us/step - loss: 0.3365 - accuracy: 0.8743 - val_loss: 0.5570 - val_accuracy: 0.8475\n",
            "Epoch 278/1000\n",
            "668/668 [==============================] - 0s 107us/step - loss: 0.3435 - accuracy: 0.8757 - val_loss: 0.4603 - val_accuracy: 0.8520\n",
            "Epoch 279/1000\n",
            "668/668 [==============================] - 0s 87us/step - loss: 0.3031 - accuracy: 0.8698 - val_loss: 0.4098 - val_accuracy: 0.8565\n",
            "Epoch 280/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.3192 - accuracy: 0.8728 - val_loss: 0.4493 - val_accuracy: 0.8610\n",
            "Epoch 281/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.2891 - accuracy: 0.8832 - val_loss: 0.4878 - val_accuracy: 0.8520\n",
            "Epoch 282/1000\n",
            "668/668 [==============================] - 0s 93us/step - loss: 0.2860 - accuracy: 0.8862 - val_loss: 0.5332 - val_accuracy: 0.8520\n",
            "Epoch 283/1000\n",
            "668/668 [==============================] - 0s 88us/step - loss: 0.3070 - accuracy: 0.8713 - val_loss: 0.5009 - val_accuracy: 0.8520\n",
            "Epoch 284/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.2978 - accuracy: 0.8817 - val_loss: 0.5187 - val_accuracy: 0.8430\n",
            "Epoch 285/1000\n",
            "668/668 [==============================] - 0s 91us/step - loss: 0.2934 - accuracy: 0.8832 - val_loss: 0.5341 - val_accuracy: 0.8341\n",
            "Epoch 286/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.3138 - accuracy: 0.8698 - val_loss: 0.5210 - val_accuracy: 0.8386\n",
            "Epoch 287/1000\n",
            "668/668 [==============================] - 0s 91us/step - loss: 0.3040 - accuracy: 0.8683 - val_loss: 0.5301 - val_accuracy: 0.8296\n",
            "Epoch 288/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.2936 - accuracy: 0.8668 - val_loss: 0.5366 - val_accuracy: 0.8610\n",
            "Epoch 289/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.3022 - accuracy: 0.8728 - val_loss: 0.5433 - val_accuracy: 0.8475\n",
            "Epoch 290/1000\n",
            "668/668 [==============================] - 0s 92us/step - loss: 0.2989 - accuracy: 0.8817 - val_loss: 0.5328 - val_accuracy: 0.8341\n",
            "Epoch 291/1000\n",
            "668/668 [==============================] - 0s 94us/step - loss: 0.2863 - accuracy: 0.8877 - val_loss: 0.5346 - val_accuracy: 0.8520\n",
            "Epoch 292/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.2908 - accuracy: 0.8787 - val_loss: 0.5360 - val_accuracy: 0.8565\n",
            "Epoch 293/1000\n",
            "668/668 [==============================] - 0s 91us/step - loss: 0.3056 - accuracy: 0.8757 - val_loss: 0.5239 - val_accuracy: 0.8520\n",
            "Epoch 294/1000\n",
            "668/668 [==============================] - 0s 104us/step - loss: 0.3107 - accuracy: 0.8608 - val_loss: 0.5268 - val_accuracy: 0.8386\n",
            "Epoch 295/1000\n",
            "668/668 [==============================] - 0s 87us/step - loss: 0.2943 - accuracy: 0.8802 - val_loss: 0.5637 - val_accuracy: 0.8475\n",
            "Epoch 296/1000\n",
            "668/668 [==============================] - 0s 91us/step - loss: 0.3003 - accuracy: 0.8728 - val_loss: 0.5692 - val_accuracy: 0.8475\n",
            "Epoch 297/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.2939 - accuracy: 0.8743 - val_loss: 0.5724 - val_accuracy: 0.8610\n",
            "Epoch 298/1000\n",
            "668/668 [==============================] - 0s 110us/step - loss: 0.3293 - accuracy: 0.8623 - val_loss: 0.5167 - val_accuracy: 0.8565\n",
            "Epoch 299/1000\n",
            "668/668 [==============================] - 0s 92us/step - loss: 0.2904 - accuracy: 0.8862 - val_loss: 0.5606 - val_accuracy: 0.8565\n",
            "Epoch 300/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.2860 - accuracy: 0.8847 - val_loss: 0.5369 - val_accuracy: 0.8565\n",
            "Epoch 301/1000\n",
            "668/668 [==============================] - 0s 85us/step - loss: 0.3042 - accuracy: 0.8802 - val_loss: 0.5653 - val_accuracy: 0.8610\n",
            "Epoch 302/1000\n",
            "668/668 [==============================] - 0s 95us/step - loss: 0.2943 - accuracy: 0.8922 - val_loss: 0.5327 - val_accuracy: 0.8520\n",
            "Epoch 303/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.3036 - accuracy: 0.8892 - val_loss: 0.5018 - val_accuracy: 0.8565\n",
            "Epoch 304/1000\n",
            "668/668 [==============================] - 0s 88us/step - loss: 0.3015 - accuracy: 0.8877 - val_loss: 0.5409 - val_accuracy: 0.8341\n",
            "Epoch 305/1000\n",
            "668/668 [==============================] - 0s 87us/step - loss: 0.3059 - accuracy: 0.8757 - val_loss: 0.5524 - val_accuracy: 0.8520\n",
            "Epoch 306/1000\n",
            "668/668 [==============================] - 0s 88us/step - loss: 0.2916 - accuracy: 0.8787 - val_loss: 0.6108 - val_accuracy: 0.8655\n",
            "Epoch 307/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.3117 - accuracy: 0.8683 - val_loss: 0.5129 - val_accuracy: 0.8655\n",
            "Epoch 308/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.2884 - accuracy: 0.8772 - val_loss: 0.5409 - val_accuracy: 0.8610\n",
            "Epoch 309/1000\n",
            "668/668 [==============================] - 0s 91us/step - loss: 0.2727 - accuracy: 0.8892 - val_loss: 0.5632 - val_accuracy: 0.8565\n",
            "Epoch 310/1000\n",
            "668/668 [==============================] - 0s 104us/step - loss: 0.3079 - accuracy: 0.8802 - val_loss: 0.5371 - val_accuracy: 0.8565\n",
            "Epoch 311/1000\n",
            "668/668 [==============================] - 0s 94us/step - loss: 0.3127 - accuracy: 0.8892 - val_loss: 0.4891 - val_accuracy: 0.8341\n",
            "Epoch 312/1000\n",
            "668/668 [==============================] - 0s 93us/step - loss: 0.3049 - accuracy: 0.8772 - val_loss: 0.5330 - val_accuracy: 0.8386\n",
            "Epoch 313/1000\n",
            "668/668 [==============================] - 0s 85us/step - loss: 0.2855 - accuracy: 0.8743 - val_loss: 0.5750 - val_accuracy: 0.8475\n",
            "Epoch 314/1000\n",
            "668/668 [==============================] - 0s 87us/step - loss: 0.2988 - accuracy: 0.8713 - val_loss: 0.5631 - val_accuracy: 0.8430\n",
            "Epoch 315/1000\n",
            "668/668 [==============================] - 0s 96us/step - loss: 0.3172 - accuracy: 0.8698 - val_loss: 0.5419 - val_accuracy: 0.8565\n",
            "Epoch 316/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.3145 - accuracy: 0.8743 - val_loss: 0.4234 - val_accuracy: 0.8520\n",
            "Epoch 317/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.3105 - accuracy: 0.8787 - val_loss: 0.4240 - val_accuracy: 0.8341\n",
            "Epoch 318/1000\n",
            "668/668 [==============================] - 0s 88us/step - loss: 0.2942 - accuracy: 0.8847 - val_loss: 0.4409 - val_accuracy: 0.8565\n",
            "Epoch 319/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.2922 - accuracy: 0.8847 - val_loss: 0.4443 - val_accuracy: 0.8520\n",
            "Epoch 320/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.2940 - accuracy: 0.8802 - val_loss: 0.4508 - val_accuracy: 0.8565\n",
            "Epoch 321/1000\n",
            "668/668 [==============================] - 0s 102us/step - loss: 0.2728 - accuracy: 0.8862 - val_loss: 0.4772 - val_accuracy: 0.8610\n",
            "Epoch 322/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.2998 - accuracy: 0.8787 - val_loss: 0.4850 - val_accuracy: 0.8565\n",
            "Epoch 323/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.2994 - accuracy: 0.8728 - val_loss: 0.5005 - val_accuracy: 0.8296\n",
            "Epoch 324/1000\n",
            "668/668 [==============================] - 0s 88us/step - loss: 0.2870 - accuracy: 0.8802 - val_loss: 0.5184 - val_accuracy: 0.8296\n",
            "Epoch 325/1000\n",
            "668/668 [==============================] - 0s 107us/step - loss: 0.2796 - accuracy: 0.8757 - val_loss: 0.5715 - val_accuracy: 0.8520\n",
            "Epoch 326/1000\n",
            "668/668 [==============================] - 0s 112us/step - loss: 0.3014 - accuracy: 0.8787 - val_loss: 0.5373 - val_accuracy: 0.8610\n",
            "Epoch 327/1000\n",
            "668/668 [==============================] - 0s 95us/step - loss: 0.2886 - accuracy: 0.8832 - val_loss: 0.5710 - val_accuracy: 0.8610\n",
            "Epoch 328/1000\n",
            "668/668 [==============================] - 0s 91us/step - loss: 0.2841 - accuracy: 0.8832 - val_loss: 0.5634 - val_accuracy: 0.8475\n",
            "Epoch 329/1000\n",
            "668/668 [==============================] - 0s 100us/step - loss: 0.2766 - accuracy: 0.8922 - val_loss: 0.5770 - val_accuracy: 0.8520\n",
            "Epoch 330/1000\n",
            "668/668 [==============================] - 0s 92us/step - loss: 0.2819 - accuracy: 0.8832 - val_loss: 0.5804 - val_accuracy: 0.8475\n",
            "Epoch 331/1000\n",
            "668/668 [==============================] - 0s 92us/step - loss: 0.2763 - accuracy: 0.8817 - val_loss: 0.5929 - val_accuracy: 0.8251\n",
            "Epoch 332/1000\n",
            "668/668 [==============================] - 0s 93us/step - loss: 0.2833 - accuracy: 0.8892 - val_loss: 0.5650 - val_accuracy: 0.8296\n",
            "Epoch 333/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.2881 - accuracy: 0.8847 - val_loss: 0.5453 - val_accuracy: 0.8386\n",
            "Epoch 334/1000\n",
            "668/668 [==============================] - 0s 91us/step - loss: 0.3249 - accuracy: 0.8772 - val_loss: 0.5422 - val_accuracy: 0.8475\n",
            "Epoch 335/1000\n",
            "668/668 [==============================] - 0s 104us/step - loss: 0.3031 - accuracy: 0.8817 - val_loss: 0.5727 - val_accuracy: 0.8430\n",
            "Epoch 336/1000\n",
            "668/668 [==============================] - 0s 94us/step - loss: 0.3047 - accuracy: 0.8847 - val_loss: 0.5115 - val_accuracy: 0.8520\n",
            "Epoch 337/1000\n",
            "668/668 [==============================] - 0s 92us/step - loss: 0.2874 - accuracy: 0.8787 - val_loss: 0.5280 - val_accuracy: 0.8386\n",
            "Epoch 338/1000\n",
            "668/668 [==============================] - 0s 91us/step - loss: 0.2979 - accuracy: 0.8772 - val_loss: 0.5141 - val_accuracy: 0.8565\n",
            "Epoch 339/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.2827 - accuracy: 0.8817 - val_loss: 0.5096 - val_accuracy: 0.8520\n",
            "Epoch 340/1000\n",
            "668/668 [==============================] - 0s 91us/step - loss: 0.3016 - accuracy: 0.8787 - val_loss: 0.5449 - val_accuracy: 0.8386\n",
            "Epoch 341/1000\n",
            "668/668 [==============================] - 0s 92us/step - loss: 0.2720 - accuracy: 0.8892 - val_loss: 0.5865 - val_accuracy: 0.8430\n",
            "Epoch 342/1000\n",
            "668/668 [==============================] - 0s 101us/step - loss: 0.2776 - accuracy: 0.8907 - val_loss: 0.5839 - val_accuracy: 0.8475\n",
            "Epoch 343/1000\n",
            "668/668 [==============================] - 0s 112us/step - loss: 0.2826 - accuracy: 0.8802 - val_loss: 0.5990 - val_accuracy: 0.8520\n",
            "Epoch 344/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.3009 - accuracy: 0.8743 - val_loss: 0.5424 - val_accuracy: 0.8386\n",
            "Epoch 345/1000\n",
            "668/668 [==============================] - 0s 88us/step - loss: 0.2791 - accuracy: 0.8877 - val_loss: 0.5251 - val_accuracy: 0.8520\n",
            "Epoch 346/1000\n",
            "668/668 [==============================] - 0s 92us/step - loss: 0.2822 - accuracy: 0.8832 - val_loss: 0.5488 - val_accuracy: 0.8475\n",
            "Epoch 347/1000\n",
            "668/668 [==============================] - 0s 124us/step - loss: 0.2868 - accuracy: 0.8907 - val_loss: 0.5705 - val_accuracy: 0.8341\n",
            "Epoch 348/1000\n",
            "668/668 [==============================] - 0s 123us/step - loss: 0.3355 - accuracy: 0.8698 - val_loss: 0.4824 - val_accuracy: 0.8341\n",
            "Epoch 349/1000\n",
            "668/668 [==============================] - 0s 92us/step - loss: 0.3026 - accuracy: 0.8817 - val_loss: 0.5020 - val_accuracy: 0.8520\n",
            "Epoch 350/1000\n",
            "668/668 [==============================] - 0s 92us/step - loss: 0.3163 - accuracy: 0.8832 - val_loss: 0.5227 - val_accuracy: 0.8386\n",
            "Epoch 351/1000\n",
            "668/668 [==============================] - 0s 91us/step - loss: 0.2876 - accuracy: 0.8862 - val_loss: 0.5330 - val_accuracy: 0.8475\n",
            "Epoch 352/1000\n",
            "668/668 [==============================] - 0s 88us/step - loss: 0.2754 - accuracy: 0.8892 - val_loss: 0.5763 - val_accuracy: 0.8520\n",
            "Epoch 353/1000\n",
            "668/668 [==============================] - 0s 88us/step - loss: 0.2690 - accuracy: 0.8982 - val_loss: 0.5679 - val_accuracy: 0.8565\n",
            "Epoch 354/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.3000 - accuracy: 0.8847 - val_loss: 0.5268 - val_accuracy: 0.8565\n",
            "Epoch 355/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.2845 - accuracy: 0.8817 - val_loss: 0.5188 - val_accuracy: 0.8520\n",
            "Epoch 356/1000\n",
            "668/668 [==============================] - 0s 87us/step - loss: 0.2858 - accuracy: 0.8832 - val_loss: 0.5553 - val_accuracy: 0.8565\n",
            "Epoch 357/1000\n",
            "668/668 [==============================] - 0s 104us/step - loss: 0.2962 - accuracy: 0.8832 - val_loss: 0.5526 - val_accuracy: 0.8386\n",
            "Epoch 358/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.2960 - accuracy: 0.8862 - val_loss: 0.5493 - val_accuracy: 0.8565\n",
            "Epoch 359/1000\n",
            "668/668 [==============================] - 0s 88us/step - loss: 0.2757 - accuracy: 0.8817 - val_loss: 0.6189 - val_accuracy: 0.8475\n",
            "Epoch 360/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.2906 - accuracy: 0.8787 - val_loss: 0.6088 - val_accuracy: 0.8520\n",
            "Epoch 361/1000\n",
            "668/668 [==============================] - 0s 94us/step - loss: 0.2893 - accuracy: 0.8757 - val_loss: 0.5404 - val_accuracy: 0.8520\n",
            "Epoch 362/1000\n",
            "668/668 [==============================] - 0s 87us/step - loss: 0.2851 - accuracy: 0.8847 - val_loss: 0.6061 - val_accuracy: 0.8475\n",
            "Epoch 363/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.2915 - accuracy: 0.8952 - val_loss: 0.6179 - val_accuracy: 0.8475\n",
            "Epoch 364/1000\n",
            "668/668 [==============================] - 0s 91us/step - loss: 0.2882 - accuracy: 0.8728 - val_loss: 0.5875 - val_accuracy: 0.8341\n",
            "Epoch 365/1000\n",
            "668/668 [==============================] - 0s 92us/step - loss: 0.2796 - accuracy: 0.8922 - val_loss: 0.5880 - val_accuracy: 0.8430\n",
            "Epoch 366/1000\n",
            "668/668 [==============================] - 0s 91us/step - loss: 0.2912 - accuracy: 0.8847 - val_loss: 0.6002 - val_accuracy: 0.8430\n",
            "Epoch 367/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.2630 - accuracy: 0.8997 - val_loss: 0.6647 - val_accuracy: 0.8520\n",
            "Epoch 368/1000\n",
            "668/668 [==============================] - 0s 92us/step - loss: 0.2580 - accuracy: 0.8922 - val_loss: 0.6821 - val_accuracy: 0.8610\n",
            "Epoch 369/1000\n",
            "668/668 [==============================] - 0s 88us/step - loss: 0.2677 - accuracy: 0.8907 - val_loss: 0.6640 - val_accuracy: 0.8655\n",
            "Epoch 370/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.2548 - accuracy: 0.8877 - val_loss: 0.6998 - val_accuracy: 0.8520\n",
            "Epoch 371/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.2748 - accuracy: 0.8922 - val_loss: 0.7263 - val_accuracy: 0.8206\n",
            "Epoch 372/1000\n",
            "668/668 [==============================] - 0s 94us/step - loss: 0.2797 - accuracy: 0.8952 - val_loss: 0.6910 - val_accuracy: 0.8251\n",
            "Epoch 373/1000\n",
            "668/668 [==============================] - 0s 100us/step - loss: 0.2940 - accuracy: 0.8877 - val_loss: 0.5915 - val_accuracy: 0.8296\n",
            "Epoch 374/1000\n",
            "668/668 [==============================] - 0s 87us/step - loss: 0.2923 - accuracy: 0.8862 - val_loss: 0.5759 - val_accuracy: 0.8341\n",
            "Epoch 375/1000\n",
            "668/668 [==============================] - 0s 88us/step - loss: 0.2722 - accuracy: 0.8937 - val_loss: 0.5845 - val_accuracy: 0.8341\n",
            "Epoch 376/1000\n",
            "668/668 [==============================] - 0s 88us/step - loss: 0.2918 - accuracy: 0.8802 - val_loss: 0.5638 - val_accuracy: 0.8341\n",
            "Epoch 377/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.2661 - accuracy: 0.8997 - val_loss: 0.6423 - val_accuracy: 0.8475\n",
            "Epoch 378/1000\n",
            "668/668 [==============================] - 0s 92us/step - loss: 0.2908 - accuracy: 0.8817 - val_loss: 0.6217 - val_accuracy: 0.8520\n",
            "Epoch 379/1000\n",
            "668/668 [==============================] - 0s 91us/step - loss: 0.2819 - accuracy: 0.8862 - val_loss: 0.6396 - val_accuracy: 0.8475\n",
            "Epoch 380/1000\n",
            "668/668 [==============================] - 0s 91us/step - loss: 0.2885 - accuracy: 0.8787 - val_loss: 0.5856 - val_accuracy: 0.8341\n",
            "Epoch 381/1000\n",
            "668/668 [==============================] - 0s 92us/step - loss: 0.2801 - accuracy: 0.8772 - val_loss: 0.5508 - val_accuracy: 0.8475\n",
            "Epoch 382/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.2861 - accuracy: 0.8832 - val_loss: 0.5874 - val_accuracy: 0.8520\n",
            "Epoch 383/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.2629 - accuracy: 0.9057 - val_loss: 0.5496 - val_accuracy: 0.8520\n",
            "Epoch 384/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.2674 - accuracy: 0.8922 - val_loss: 0.5886 - val_accuracy: 0.8565\n",
            "Epoch 385/1000\n",
            "668/668 [==============================] - 0s 88us/step - loss: 0.2648 - accuracy: 0.8952 - val_loss: 0.6050 - val_accuracy: 0.8610\n",
            "Epoch 386/1000\n",
            "668/668 [==============================] - 0s 97us/step - loss: 0.2895 - accuracy: 0.8802 - val_loss: 0.5968 - val_accuracy: 0.8520\n",
            "Epoch 387/1000\n",
            "668/668 [==============================] - 0s 110us/step - loss: 0.2875 - accuracy: 0.8668 - val_loss: 0.6108 - val_accuracy: 0.8610\n",
            "Epoch 388/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.2932 - accuracy: 0.8877 - val_loss: 0.5919 - val_accuracy: 0.8520\n",
            "Epoch 389/1000\n",
            "668/668 [==============================] - 0s 104us/step - loss: 0.2881 - accuracy: 0.8713 - val_loss: 0.5522 - val_accuracy: 0.8565\n",
            "Epoch 390/1000\n",
            "668/668 [==============================] - 0s 88us/step - loss: 0.2768 - accuracy: 0.8922 - val_loss: 0.6375 - val_accuracy: 0.8565\n",
            "Epoch 391/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.2606 - accuracy: 0.8952 - val_loss: 0.6516 - val_accuracy: 0.8655\n",
            "Epoch 392/1000\n",
            "668/668 [==============================] - 0s 88us/step - loss: 0.2724 - accuracy: 0.8907 - val_loss: 0.6158 - val_accuracy: 0.8520\n",
            "Epoch 393/1000\n",
            "668/668 [==============================] - 0s 95us/step - loss: 0.2796 - accuracy: 0.8922 - val_loss: 0.6146 - val_accuracy: 0.8520\n",
            "Epoch 394/1000\n",
            "668/668 [==============================] - 0s 91us/step - loss: 0.2733 - accuracy: 0.8937 - val_loss: 0.6201 - val_accuracy: 0.8430\n",
            "Epoch 395/1000\n",
            "668/668 [==============================] - 0s 93us/step - loss: 0.2738 - accuracy: 0.8802 - val_loss: 0.6615 - val_accuracy: 0.8430\n",
            "Epoch 396/1000\n",
            "668/668 [==============================] - 0s 93us/step - loss: 0.2926 - accuracy: 0.8757 - val_loss: 0.6551 - val_accuracy: 0.8341\n",
            "Epoch 397/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.2823 - accuracy: 0.8832 - val_loss: 0.6827 - val_accuracy: 0.8341\n",
            "Epoch 398/1000\n",
            "668/668 [==============================] - 0s 92us/step - loss: 0.2790 - accuracy: 0.8847 - val_loss: 0.6716 - val_accuracy: 0.8296\n",
            "Epoch 399/1000\n",
            "668/668 [==============================] - 0s 93us/step - loss: 0.2710 - accuracy: 0.8892 - val_loss: 0.6524 - val_accuracy: 0.8520\n",
            "Epoch 400/1000\n",
            "668/668 [==============================] - 0s 94us/step - loss: 0.2650 - accuracy: 0.8877 - val_loss: 0.6692 - val_accuracy: 0.8565\n",
            "Epoch 401/1000\n",
            "668/668 [==============================] - 0s 96us/step - loss: 0.2730 - accuracy: 0.8847 - val_loss: 0.6685 - val_accuracy: 0.8475\n",
            "Epoch 402/1000\n",
            "668/668 [==============================] - 0s 92us/step - loss: 0.2611 - accuracy: 0.8877 - val_loss: 0.7000 - val_accuracy: 0.8386\n",
            "Epoch 403/1000\n",
            "668/668 [==============================] - 0s 92us/step - loss: 0.2739 - accuracy: 0.8847 - val_loss: 0.7005 - val_accuracy: 0.8475\n",
            "Epoch 404/1000\n",
            "668/668 [==============================] - 0s 112us/step - loss: 0.2622 - accuracy: 0.8982 - val_loss: 0.7019 - val_accuracy: 0.8475\n",
            "Epoch 405/1000\n",
            "668/668 [==============================] - 0s 114us/step - loss: 0.2742 - accuracy: 0.8862 - val_loss: 0.6999 - val_accuracy: 0.8296\n",
            "Epoch 406/1000\n",
            "668/668 [==============================] - 0s 93us/step - loss: 0.2746 - accuracy: 0.8847 - val_loss: 0.7336 - val_accuracy: 0.8520\n",
            "Epoch 407/1000\n",
            "668/668 [==============================] - 0s 96us/step - loss: 0.2767 - accuracy: 0.8922 - val_loss: 0.6904 - val_accuracy: 0.8341\n",
            "Epoch 408/1000\n",
            "668/668 [==============================] - 0s 92us/step - loss: 0.2781 - accuracy: 0.8817 - val_loss: 0.6543 - val_accuracy: 0.8341\n",
            "Epoch 409/1000\n",
            "668/668 [==============================] - 0s 92us/step - loss: 0.2888 - accuracy: 0.8802 - val_loss: 0.6735 - val_accuracy: 0.8386\n",
            "Epoch 410/1000\n",
            "668/668 [==============================] - 0s 91us/step - loss: 0.2940 - accuracy: 0.8728 - val_loss: 0.6037 - val_accuracy: 0.8296\n",
            "Epoch 411/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.2953 - accuracy: 0.8653 - val_loss: 0.5938 - val_accuracy: 0.8386\n",
            "Epoch 412/1000\n",
            "668/668 [==============================] - 0s 87us/step - loss: 0.2966 - accuracy: 0.8698 - val_loss: 0.6260 - val_accuracy: 0.8386\n",
            "Epoch 413/1000\n",
            "668/668 [==============================] - 0s 92us/step - loss: 0.2996 - accuracy: 0.8757 - val_loss: 0.5978 - val_accuracy: 0.8161\n",
            "Epoch 414/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.2947 - accuracy: 0.8772 - val_loss: 0.5258 - val_accuracy: 0.8341\n",
            "Epoch 415/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.2756 - accuracy: 0.8802 - val_loss: 0.5976 - val_accuracy: 0.8341\n",
            "Epoch 416/1000\n",
            "668/668 [==============================] - 0s 91us/step - loss: 0.3019 - accuracy: 0.8817 - val_loss: 0.5856 - val_accuracy: 0.8430\n",
            "Epoch 417/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.2729 - accuracy: 0.8862 - val_loss: 0.6179 - val_accuracy: 0.8430\n",
            "Epoch 418/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.2723 - accuracy: 0.8877 - val_loss: 0.6378 - val_accuracy: 0.8475\n",
            "Epoch 419/1000\n",
            "668/668 [==============================] - 0s 88us/step - loss: 0.2726 - accuracy: 0.8877 - val_loss: 0.7068 - val_accuracy: 0.8430\n",
            "Epoch 420/1000\n",
            "668/668 [==============================] - 0s 88us/step - loss: 0.2818 - accuracy: 0.8832 - val_loss: 0.6956 - val_accuracy: 0.8386\n",
            "Epoch 421/1000\n",
            "668/668 [==============================] - 0s 104us/step - loss: 0.2661 - accuracy: 0.8832 - val_loss: 0.6562 - val_accuracy: 0.8251\n",
            "Epoch 422/1000\n",
            "668/668 [==============================] - 0s 88us/step - loss: 0.2800 - accuracy: 0.8728 - val_loss: 0.7669 - val_accuracy: 0.8430\n",
            "Epoch 423/1000\n",
            "668/668 [==============================] - 0s 107us/step - loss: 0.2713 - accuracy: 0.8862 - val_loss: 0.6285 - val_accuracy: 0.8430\n",
            "Epoch 424/1000\n",
            "668/668 [==============================] - 0s 94us/step - loss: 0.2781 - accuracy: 0.8907 - val_loss: 0.6424 - val_accuracy: 0.8206\n",
            "Epoch 425/1000\n",
            "668/668 [==============================] - 0s 93us/step - loss: 0.2894 - accuracy: 0.8772 - val_loss: 0.6399 - val_accuracy: 0.8430\n",
            "Epoch 426/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.2761 - accuracy: 0.8877 - val_loss: 0.6248 - val_accuracy: 0.8430\n",
            "Epoch 427/1000\n",
            "668/668 [==============================] - 0s 97us/step - loss: 0.2606 - accuracy: 0.8922 - val_loss: 0.6623 - val_accuracy: 0.8251\n",
            "Epoch 428/1000\n",
            "668/668 [==============================] - 0s 93us/step - loss: 0.2865 - accuracy: 0.8832 - val_loss: 0.6701 - val_accuracy: 0.8341\n",
            "Epoch 429/1000\n",
            "668/668 [==============================] - 0s 91us/step - loss: 0.2643 - accuracy: 0.8847 - val_loss: 0.6811 - val_accuracy: 0.8296\n",
            "Epoch 430/1000\n",
            "668/668 [==============================] - 0s 91us/step - loss: 0.2988 - accuracy: 0.8862 - val_loss: 0.6254 - val_accuracy: 0.8655\n",
            "Epoch 431/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.2817 - accuracy: 0.8847 - val_loss: 0.6239 - val_accuracy: 0.8520\n",
            "Epoch 432/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.3070 - accuracy: 0.8847 - val_loss: 0.6495 - val_accuracy: 0.8430\n",
            "Epoch 433/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.2836 - accuracy: 0.8832 - val_loss: 0.6372 - val_accuracy: 0.8610\n",
            "Epoch 434/1000\n",
            "668/668 [==============================] - 0s 91us/step - loss: 0.2706 - accuracy: 0.8937 - val_loss: 0.6813 - val_accuracy: 0.8565\n",
            "Epoch 435/1000\n",
            "668/668 [==============================] - 0s 93us/step - loss: 0.2618 - accuracy: 0.8952 - val_loss: 0.6488 - val_accuracy: 0.8386\n",
            "Epoch 436/1000\n",
            "668/668 [==============================] - 0s 88us/step - loss: 0.2712 - accuracy: 0.8922 - val_loss: 0.7028 - val_accuracy: 0.8430\n",
            "Epoch 437/1000\n",
            "668/668 [==============================] - 0s 102us/step - loss: 0.2899 - accuracy: 0.8877 - val_loss: 0.6719 - val_accuracy: 0.8296\n",
            "Epoch 438/1000\n",
            "668/668 [==============================] - 0s 87us/step - loss: 0.2863 - accuracy: 0.8847 - val_loss: 0.6731 - val_accuracy: 0.8430\n",
            "Epoch 439/1000\n",
            "668/668 [==============================] - 0s 88us/step - loss: 0.3054 - accuracy: 0.8698 - val_loss: 0.6372 - val_accuracy: 0.8430\n",
            "Epoch 440/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.2881 - accuracy: 0.8877 - val_loss: 0.4937 - val_accuracy: 0.8610\n",
            "Epoch 441/1000\n",
            "668/668 [==============================] - 0s 92us/step - loss: 0.2625 - accuracy: 0.8997 - val_loss: 0.6192 - val_accuracy: 0.8475\n",
            "Epoch 442/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.2800 - accuracy: 0.8832 - val_loss: 0.5931 - val_accuracy: 0.8610\n",
            "Epoch 443/1000\n",
            "668/668 [==============================] - 0s 92us/step - loss: 0.2918 - accuracy: 0.8952 - val_loss: 0.5775 - val_accuracy: 0.8610\n",
            "Epoch 444/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.2719 - accuracy: 0.8877 - val_loss: 0.6175 - val_accuracy: 0.8520\n",
            "Epoch 445/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.2644 - accuracy: 0.8922 - val_loss: 0.6273 - val_accuracy: 0.8565\n",
            "Epoch 446/1000\n",
            "668/668 [==============================] - 0s 96us/step - loss: 0.2717 - accuracy: 0.8892 - val_loss: 0.6475 - val_accuracy: 0.8430\n",
            "Epoch 447/1000\n",
            "668/668 [==============================] - 0s 93us/step - loss: 0.2791 - accuracy: 0.8952 - val_loss: 0.6406 - val_accuracy: 0.8565\n",
            "Epoch 448/1000\n",
            "668/668 [==============================] - 0s 101us/step - loss: 0.3108 - accuracy: 0.8907 - val_loss: 0.5852 - val_accuracy: 0.8475\n",
            "Epoch 449/1000\n",
            "668/668 [==============================] - 0s 96us/step - loss: 0.2872 - accuracy: 0.8862 - val_loss: 0.5833 - val_accuracy: 0.8386\n",
            "Epoch 450/1000\n",
            "668/668 [==============================] - 0s 95us/step - loss: 0.2843 - accuracy: 0.8743 - val_loss: 0.6642 - val_accuracy: 0.8520\n",
            "Epoch 451/1000\n",
            "668/668 [==============================] - 0s 91us/step - loss: 0.2719 - accuracy: 0.8922 - val_loss: 0.6035 - val_accuracy: 0.8475\n",
            "Epoch 452/1000\n",
            "668/668 [==============================] - 0s 87us/step - loss: 0.2809 - accuracy: 0.8757 - val_loss: 0.5882 - val_accuracy: 0.8296\n",
            "Epoch 453/1000\n",
            "668/668 [==============================] - 0s 100us/step - loss: 0.2970 - accuracy: 0.8907 - val_loss: 0.6212 - val_accuracy: 0.8341\n",
            "Epoch 454/1000\n",
            "668/668 [==============================] - 0s 102us/step - loss: 0.2867 - accuracy: 0.8832 - val_loss: 0.6314 - val_accuracy: 0.8565\n",
            "Epoch 455/1000\n",
            "668/668 [==============================] - 0s 93us/step - loss: 0.2736 - accuracy: 0.8877 - val_loss: 0.6814 - val_accuracy: 0.8610\n",
            "Epoch 456/1000\n",
            "668/668 [==============================] - 0s 93us/step - loss: 0.2826 - accuracy: 0.8922 - val_loss: 0.6584 - val_accuracy: 0.8565\n",
            "Epoch 457/1000\n",
            "668/668 [==============================] - 0s 94us/step - loss: 0.2661 - accuracy: 0.8967 - val_loss: 0.6696 - val_accuracy: 0.8565\n",
            "Epoch 458/1000\n",
            "668/668 [==============================] - 0s 91us/step - loss: 0.2740 - accuracy: 0.8847 - val_loss: 0.7033 - val_accuracy: 0.8520\n",
            "Epoch 459/1000\n",
            "668/668 [==============================] - 0s 93us/step - loss: 0.2536 - accuracy: 0.8952 - val_loss: 0.7694 - val_accuracy: 0.8475\n",
            "Epoch 460/1000\n",
            "668/668 [==============================] - 0s 108us/step - loss: 0.2737 - accuracy: 0.8877 - val_loss: 0.6453 - val_accuracy: 0.8475\n",
            "Epoch 461/1000\n",
            "668/668 [==============================] - 0s 105us/step - loss: 0.2790 - accuracy: 0.8802 - val_loss: 0.6451 - val_accuracy: 0.8610\n",
            "Epoch 462/1000\n",
            "668/668 [==============================] - 0s 88us/step - loss: 0.2843 - accuracy: 0.8847 - val_loss: 0.6268 - val_accuracy: 0.8520\n",
            "Epoch 463/1000\n",
            "668/668 [==============================] - 0s 88us/step - loss: 0.2552 - accuracy: 0.8862 - val_loss: 0.6830 - val_accuracy: 0.8565\n",
            "Epoch 464/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.2749 - accuracy: 0.8892 - val_loss: 0.6123 - val_accuracy: 0.8610\n",
            "Epoch 465/1000\n",
            "668/668 [==============================] - 0s 88us/step - loss: 0.2740 - accuracy: 0.8907 - val_loss: 0.6157 - val_accuracy: 0.8610\n",
            "Epoch 466/1000\n",
            "668/668 [==============================] - 0s 96us/step - loss: 0.2657 - accuracy: 0.8877 - val_loss: 0.7127 - val_accuracy: 0.8520\n",
            "Epoch 467/1000\n",
            "668/668 [==============================] - 0s 88us/step - loss: 0.2752 - accuracy: 0.8757 - val_loss: 0.6713 - val_accuracy: 0.8386\n",
            "Epoch 468/1000\n",
            "668/668 [==============================] - 0s 103us/step - loss: 0.2789 - accuracy: 0.8952 - val_loss: 0.7132 - val_accuracy: 0.8341\n",
            "Epoch 469/1000\n",
            "668/668 [==============================] - 0s 98us/step - loss: 0.2847 - accuracy: 0.8787 - val_loss: 0.6428 - val_accuracy: 0.8386\n",
            "Epoch 470/1000\n",
            "668/668 [==============================] - 0s 94us/step - loss: 0.2931 - accuracy: 0.8787 - val_loss: 0.5683 - val_accuracy: 0.8430\n",
            "Epoch 471/1000\n",
            "668/668 [==============================] - 0s 91us/step - loss: 0.2514 - accuracy: 0.9102 - val_loss: 0.6364 - val_accuracy: 0.8565\n",
            "Epoch 472/1000\n",
            "668/668 [==============================] - 0s 94us/step - loss: 0.2875 - accuracy: 0.8817 - val_loss: 0.5845 - val_accuracy: 0.8565\n",
            "Epoch 473/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.2481 - accuracy: 0.9042 - val_loss: 0.6502 - val_accuracy: 0.8520\n",
            "Epoch 474/1000\n",
            "668/668 [==============================] - 0s 92us/step - loss: 0.2631 - accuracy: 0.8907 - val_loss: 0.7125 - val_accuracy: 0.8520\n",
            "Epoch 475/1000\n",
            "668/668 [==============================] - 0s 94us/step - loss: 0.2481 - accuracy: 0.8967 - val_loss: 0.8012 - val_accuracy: 0.8520\n",
            "Epoch 476/1000\n",
            "668/668 [==============================] - 0s 95us/step - loss: 0.2860 - accuracy: 0.8847 - val_loss: 0.6866 - val_accuracy: 0.8610\n",
            "Epoch 477/1000\n",
            "668/668 [==============================] - 0s 105us/step - loss: 0.2754 - accuracy: 0.8907 - val_loss: 0.6550 - val_accuracy: 0.8386\n",
            "Epoch 478/1000\n",
            "668/668 [==============================] - 0s 92us/step - loss: 0.2570 - accuracy: 0.8847 - val_loss: 0.7089 - val_accuracy: 0.8341\n",
            "Epoch 479/1000\n",
            "668/668 [==============================] - 0s 94us/step - loss: 0.2682 - accuracy: 0.8847 - val_loss: 0.6993 - val_accuracy: 0.8430\n",
            "Epoch 480/1000\n",
            "668/668 [==============================] - 0s 97us/step - loss: 0.2919 - accuracy: 0.8802 - val_loss: 0.6789 - val_accuracy: 0.8475\n",
            "Epoch 481/1000\n",
            "668/668 [==============================] - 0s 97us/step - loss: 0.2747 - accuracy: 0.8787 - val_loss: 0.6672 - val_accuracy: 0.8520\n",
            "Epoch 482/1000\n",
            "668/668 [==============================] - 0s 96us/step - loss: 0.2568 - accuracy: 0.9027 - val_loss: 0.6694 - val_accuracy: 0.8565\n",
            "Epoch 483/1000\n",
            "668/668 [==============================] - 0s 108us/step - loss: 0.2551 - accuracy: 0.8922 - val_loss: 0.7581 - val_accuracy: 0.8430\n",
            "Epoch 484/1000\n",
            "668/668 [==============================] - 0s 108us/step - loss: 0.2712 - accuracy: 0.8907 - val_loss: 0.6905 - val_accuracy: 0.8475\n",
            "Epoch 485/1000\n",
            "668/668 [==============================] - 0s 91us/step - loss: 0.2753 - accuracy: 0.8892 - val_loss: 0.7220 - val_accuracy: 0.8610\n",
            "Epoch 486/1000\n",
            "668/668 [==============================] - 0s 92us/step - loss: 0.2731 - accuracy: 0.8892 - val_loss: 0.7076 - val_accuracy: 0.8655\n",
            "Epoch 487/1000\n",
            "668/668 [==============================] - 0s 98us/step - loss: 0.2522 - accuracy: 0.8952 - val_loss: 0.7773 - val_accuracy: 0.8655\n",
            "Epoch 488/1000\n",
            "668/668 [==============================] - 0s 95us/step - loss: 0.2510 - accuracy: 0.8967 - val_loss: 0.8229 - val_accuracy: 0.8610\n",
            "Epoch 489/1000\n",
            "668/668 [==============================] - 0s 98us/step - loss: 0.2685 - accuracy: 0.8937 - val_loss: 0.7806 - val_accuracy: 0.8386\n",
            "Epoch 490/1000\n",
            "668/668 [==============================] - 0s 91us/step - loss: 0.2704 - accuracy: 0.8877 - val_loss: 0.8032 - val_accuracy: 0.8430\n",
            "Epoch 491/1000\n",
            "668/668 [==============================] - 0s 92us/step - loss: 0.2589 - accuracy: 0.8907 - val_loss: 0.7255 - val_accuracy: 0.8430\n",
            "Epoch 492/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.2713 - accuracy: 0.8907 - val_loss: 0.6417 - val_accuracy: 0.8341\n",
            "Epoch 493/1000\n",
            "668/668 [==============================] - 0s 88us/step - loss: 0.2766 - accuracy: 0.8892 - val_loss: 0.6415 - val_accuracy: 0.8430\n",
            "Epoch 494/1000\n",
            "668/668 [==============================] - 0s 92us/step - loss: 0.2738 - accuracy: 0.9012 - val_loss: 0.6617 - val_accuracy: 0.8565\n",
            "Epoch 495/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.2710 - accuracy: 0.8787 - val_loss: 0.6214 - val_accuracy: 0.8520\n",
            "Epoch 496/1000\n",
            "668/668 [==============================] - 0s 92us/step - loss: 0.2640 - accuracy: 0.8967 - val_loss: 0.6354 - val_accuracy: 0.8475\n",
            "Epoch 497/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.2656 - accuracy: 0.8922 - val_loss: 0.6701 - val_accuracy: 0.8520\n",
            "Epoch 498/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.2600 - accuracy: 0.8922 - val_loss: 0.7058 - val_accuracy: 0.8430\n",
            "Epoch 499/1000\n",
            "668/668 [==============================] - 0s 91us/step - loss: 0.2640 - accuracy: 0.8937 - val_loss: 0.6628 - val_accuracy: 0.8610\n",
            "Epoch 500/1000\n",
            "668/668 [==============================] - 0s 123us/step - loss: 0.2601 - accuracy: 0.8907 - val_loss: 0.6623 - val_accuracy: 0.8475\n",
            "Epoch 501/1000\n",
            "668/668 [==============================] - 0s 94us/step - loss: 0.2626 - accuracy: 0.8892 - val_loss: 0.6455 - val_accuracy: 0.8520\n",
            "Epoch 502/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.2605 - accuracy: 0.8952 - val_loss: 0.7598 - val_accuracy: 0.8520\n",
            "Epoch 503/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.2939 - accuracy: 0.8892 - val_loss: 0.6843 - val_accuracy: 0.8520\n",
            "Epoch 504/1000\n",
            "668/668 [==============================] - 0s 88us/step - loss: 0.2613 - accuracy: 0.8907 - val_loss: 0.6817 - val_accuracy: 0.8520\n",
            "Epoch 505/1000\n",
            "668/668 [==============================] - 0s 91us/step - loss: 0.2847 - accuracy: 0.8862 - val_loss: 0.6731 - val_accuracy: 0.8520\n",
            "Epoch 506/1000\n",
            "668/668 [==============================] - 0s 99us/step - loss: 0.2595 - accuracy: 0.9027 - val_loss: 0.6967 - val_accuracy: 0.8520\n",
            "Epoch 507/1000\n",
            "668/668 [==============================] - 0s 91us/step - loss: 0.2597 - accuracy: 0.8907 - val_loss: 0.7414 - val_accuracy: 0.8475\n",
            "Epoch 508/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.2863 - accuracy: 0.8817 - val_loss: 0.5427 - val_accuracy: 0.8341\n",
            "Epoch 509/1000\n",
            "668/668 [==============================] - 0s 91us/step - loss: 0.2671 - accuracy: 0.8937 - val_loss: 0.5400 - val_accuracy: 0.8475\n",
            "Epoch 510/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.2590 - accuracy: 0.8922 - val_loss: 0.6088 - val_accuracy: 0.8475\n",
            "Epoch 511/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.2587 - accuracy: 0.8892 - val_loss: 0.6380 - val_accuracy: 0.8565\n",
            "Epoch 512/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.2899 - accuracy: 0.8922 - val_loss: 0.5498 - val_accuracy: 0.8520\n",
            "Epoch 513/1000\n",
            "668/668 [==============================] - 0s 94us/step - loss: 0.2798 - accuracy: 0.8862 - val_loss: 0.5346 - val_accuracy: 0.8520\n",
            "Epoch 514/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.2645 - accuracy: 0.8922 - val_loss: 0.5950 - val_accuracy: 0.8520\n",
            "Epoch 515/1000\n",
            "668/668 [==============================] - 0s 91us/step - loss: 0.2716 - accuracy: 0.8907 - val_loss: 0.6079 - val_accuracy: 0.8475\n",
            "Epoch 516/1000\n",
            "668/668 [==============================] - 0s 96us/step - loss: 0.2871 - accuracy: 0.8892 - val_loss: 0.5716 - val_accuracy: 0.8475\n",
            "Epoch 517/1000\n",
            "668/668 [==============================] - 0s 87us/step - loss: 0.2720 - accuracy: 0.8937 - val_loss: 0.6307 - val_accuracy: 0.8430\n",
            "Epoch 518/1000\n",
            "668/668 [==============================] - 0s 92us/step - loss: 0.2634 - accuracy: 0.8907 - val_loss: 0.7055 - val_accuracy: 0.8296\n",
            "Epoch 519/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.2621 - accuracy: 0.8967 - val_loss: 0.6487 - val_accuracy: 0.8296\n",
            "Epoch 520/1000\n",
            "668/668 [==============================] - 0s 92us/step - loss: 0.2599 - accuracy: 0.8982 - val_loss: 0.6341 - val_accuracy: 0.8251\n",
            "Epoch 521/1000\n",
            "668/668 [==============================] - 0s 93us/step - loss: 0.2702 - accuracy: 0.8967 - val_loss: 0.6905 - val_accuracy: 0.8296\n",
            "Epoch 522/1000\n",
            "668/668 [==============================] - 0s 91us/step - loss: 0.2789 - accuracy: 0.8892 - val_loss: 0.6422 - val_accuracy: 0.8296\n",
            "Epoch 523/1000\n",
            "668/668 [==============================] - 0s 94us/step - loss: 0.2709 - accuracy: 0.8937 - val_loss: 0.6515 - val_accuracy: 0.8475\n",
            "Epoch 524/1000\n",
            "668/668 [==============================] - 0s 98us/step - loss: 0.2686 - accuracy: 0.8922 - val_loss: 0.6352 - val_accuracy: 0.8520\n",
            "Epoch 525/1000\n",
            "668/668 [==============================] - 0s 93us/step - loss: 0.2588 - accuracy: 0.8952 - val_loss: 0.7177 - val_accuracy: 0.8386\n",
            "Epoch 526/1000\n",
            "668/668 [==============================] - 0s 92us/step - loss: 0.2519 - accuracy: 0.8952 - val_loss: 0.7933 - val_accuracy: 0.8430\n",
            "Epoch 527/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.3012 - accuracy: 0.8892 - val_loss: 0.7052 - val_accuracy: 0.8475\n",
            "Epoch 528/1000\n",
            "668/668 [==============================] - 0s 92us/step - loss: 0.2852 - accuracy: 0.8832 - val_loss: 0.7119 - val_accuracy: 0.8296\n",
            "Epoch 529/1000\n",
            "668/668 [==============================] - 0s 91us/step - loss: 0.2739 - accuracy: 0.8817 - val_loss: 0.7270 - val_accuracy: 0.8520\n",
            "Epoch 530/1000\n",
            "668/668 [==============================] - 0s 95us/step - loss: 0.2658 - accuracy: 0.8937 - val_loss: 0.6976 - val_accuracy: 0.8475\n",
            "Epoch 531/1000\n",
            "668/668 [==============================] - 0s 98us/step - loss: 0.2690 - accuracy: 0.8892 - val_loss: 0.7403 - val_accuracy: 0.8296\n",
            "Epoch 532/1000\n",
            "668/668 [==============================] - 0s 106us/step - loss: 0.2673 - accuracy: 0.8997 - val_loss: 0.7857 - val_accuracy: 0.8386\n",
            "Epoch 533/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.2563 - accuracy: 0.8937 - val_loss: 0.7474 - val_accuracy: 0.8520\n",
            "Epoch 534/1000\n",
            "668/668 [==============================] - 0s 93us/step - loss: 0.2725 - accuracy: 0.8832 - val_loss: 0.7730 - val_accuracy: 0.8430\n",
            "Epoch 535/1000\n",
            "668/668 [==============================] - 0s 92us/step - loss: 0.2660 - accuracy: 0.8892 - val_loss: 0.7620 - val_accuracy: 0.8430\n",
            "Epoch 536/1000\n",
            "668/668 [==============================] - 0s 109us/step - loss: 0.2550 - accuracy: 0.8967 - val_loss: 0.7289 - val_accuracy: 0.8475\n",
            "Epoch 537/1000\n",
            "668/668 [==============================] - 0s 112us/step - loss: 0.3188 - accuracy: 0.8862 - val_loss: 0.7149 - val_accuracy: 0.8251\n",
            "Epoch 538/1000\n",
            "668/668 [==============================] - 0s 94us/step - loss: 0.2768 - accuracy: 0.8802 - val_loss: 0.6335 - val_accuracy: 0.8206\n",
            "Epoch 539/1000\n",
            "668/668 [==============================] - 0s 95us/step - loss: 0.2756 - accuracy: 0.8967 - val_loss: 0.6214 - val_accuracy: 0.8475\n",
            "Epoch 540/1000\n",
            "668/668 [==============================] - 0s 95us/step - loss: 0.2525 - accuracy: 0.8997 - val_loss: 0.6695 - val_accuracy: 0.8341\n",
            "Epoch 541/1000\n",
            "668/668 [==============================] - 0s 98us/step - loss: 0.2606 - accuracy: 0.8907 - val_loss: 0.6798 - val_accuracy: 0.8430\n",
            "Epoch 542/1000\n",
            "668/668 [==============================] - 0s 92us/step - loss: 0.2666 - accuracy: 0.8997 - val_loss: 0.6826 - val_accuracy: 0.8386\n",
            "Epoch 543/1000\n",
            "668/668 [==============================] - 0s 91us/step - loss: 0.2607 - accuracy: 0.8907 - val_loss: 0.7037 - val_accuracy: 0.8341\n",
            "Epoch 544/1000\n",
            "668/668 [==============================] - 0s 103us/step - loss: 0.2609 - accuracy: 0.8922 - val_loss: 0.7200 - val_accuracy: 0.8341\n",
            "Epoch 545/1000\n",
            "668/668 [==============================] - 0s 96us/step - loss: 0.2469 - accuracy: 0.9012 - val_loss: 0.8086 - val_accuracy: 0.8386\n",
            "Epoch 546/1000\n",
            "668/668 [==============================] - 0s 103us/step - loss: 0.2692 - accuracy: 0.8967 - val_loss: 0.7798 - val_accuracy: 0.8341\n",
            "Epoch 547/1000\n",
            "668/668 [==============================] - 0s 104us/step - loss: 0.2801 - accuracy: 0.8952 - val_loss: 0.6661 - val_accuracy: 0.8251\n",
            "Epoch 548/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.2978 - accuracy: 0.8847 - val_loss: 0.6690 - val_accuracy: 0.8117\n",
            "Epoch 549/1000\n",
            "668/668 [==============================] - 0s 91us/step - loss: 0.2642 - accuracy: 0.8937 - val_loss: 0.6623 - val_accuracy: 0.8206\n",
            "Epoch 550/1000\n",
            "668/668 [==============================] - 0s 92us/step - loss: 0.2743 - accuracy: 0.8892 - val_loss: 0.6068 - val_accuracy: 0.8251\n",
            "Epoch 551/1000\n",
            "668/668 [==============================] - 0s 92us/step - loss: 0.2485 - accuracy: 0.8982 - val_loss: 0.6902 - val_accuracy: 0.8251\n",
            "Epoch 552/1000\n",
            "668/668 [==============================] - 0s 94us/step - loss: 0.2466 - accuracy: 0.9042 - val_loss: 0.7304 - val_accuracy: 0.8206\n",
            "Epoch 553/1000\n",
            "668/668 [==============================] - 0s 94us/step - loss: 0.2699 - accuracy: 0.8982 - val_loss: 0.6289 - val_accuracy: 0.8072\n",
            "Epoch 554/1000\n",
            "668/668 [==============================] - 0s 107us/step - loss: 0.2475 - accuracy: 0.9072 - val_loss: 0.6686 - val_accuracy: 0.8341\n",
            "Epoch 555/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.2405 - accuracy: 0.9042 - val_loss: 0.6954 - val_accuracy: 0.8341\n",
            "Epoch 556/1000\n",
            "668/668 [==============================] - 0s 98us/step - loss: 0.2571 - accuracy: 0.8967 - val_loss: 0.6913 - val_accuracy: 0.8386\n",
            "Epoch 557/1000\n",
            "668/668 [==============================] - 0s 92us/step - loss: 0.2525 - accuracy: 0.8997 - val_loss: 0.6757 - val_accuracy: 0.8341\n",
            "Epoch 558/1000\n",
            "668/668 [==============================] - 0s 92us/step - loss: 0.2481 - accuracy: 0.9027 - val_loss: 0.7213 - val_accuracy: 0.8341\n",
            "Epoch 559/1000\n",
            "668/668 [==============================] - 0s 93us/step - loss: 0.2508 - accuracy: 0.8937 - val_loss: 0.7383 - val_accuracy: 0.8206\n",
            "Epoch 560/1000\n",
            "668/668 [==============================] - 0s 97us/step - loss: 0.2616 - accuracy: 0.8817 - val_loss: 0.7590 - val_accuracy: 0.8251\n",
            "Epoch 561/1000\n",
            "668/668 [==============================] - 0s 97us/step - loss: 0.2490 - accuracy: 0.8967 - val_loss: 0.8119 - val_accuracy: 0.8251\n",
            "Epoch 562/1000\n",
            "668/668 [==============================] - 0s 100us/step - loss: 0.2429 - accuracy: 0.8967 - val_loss: 0.8570 - val_accuracy: 0.8296\n",
            "Epoch 563/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.2554 - accuracy: 0.8997 - val_loss: 0.7274 - val_accuracy: 0.8117\n",
            "Epoch 564/1000\n",
            "668/668 [==============================] - 0s 96us/step - loss: 0.2608 - accuracy: 0.8982 - val_loss: 0.7591 - val_accuracy: 0.8296\n",
            "Epoch 565/1000\n",
            "668/668 [==============================] - 0s 92us/step - loss: 0.2597 - accuracy: 0.8967 - val_loss: 0.7378 - val_accuracy: 0.8251\n",
            "Epoch 566/1000\n",
            "668/668 [==============================] - 0s 102us/step - loss: 0.2679 - accuracy: 0.8952 - val_loss: 0.8077 - val_accuracy: 0.8296\n",
            "Epoch 567/1000\n",
            "668/668 [==============================] - 0s 94us/step - loss: 0.2617 - accuracy: 0.8952 - val_loss: 0.7442 - val_accuracy: 0.8296\n",
            "Epoch 568/1000\n",
            "668/668 [==============================] - 0s 130us/step - loss: 0.2780 - accuracy: 0.8967 - val_loss: 0.6704 - val_accuracy: 0.8296\n",
            "Epoch 569/1000\n",
            "668/668 [==============================] - 0s 95us/step - loss: 0.2556 - accuracy: 0.8937 - val_loss: 0.7049 - val_accuracy: 0.8251\n",
            "Epoch 570/1000\n",
            "668/668 [==============================] - 0s 95us/step - loss: 0.2478 - accuracy: 0.9102 - val_loss: 0.6790 - val_accuracy: 0.8296\n",
            "Epoch 571/1000\n",
            "668/668 [==============================] - 0s 105us/step - loss: 0.2497 - accuracy: 0.9012 - val_loss: 0.7430 - val_accuracy: 0.8251\n",
            "Epoch 572/1000\n",
            "668/668 [==============================] - 0s 106us/step - loss: 0.2476 - accuracy: 0.9027 - val_loss: 0.7669 - val_accuracy: 0.8296\n",
            "Epoch 573/1000\n",
            "668/668 [==============================] - 0s 94us/step - loss: 0.2545 - accuracy: 0.8922 - val_loss: 0.8454 - val_accuracy: 0.8251\n",
            "Epoch 574/1000\n",
            "668/668 [==============================] - 0s 95us/step - loss: 0.2447 - accuracy: 0.8982 - val_loss: 0.8492 - val_accuracy: 0.8341\n",
            "Epoch 575/1000\n",
            "668/668 [==============================] - 0s 102us/step - loss: 0.2467 - accuracy: 0.9012 - val_loss: 0.8746 - val_accuracy: 0.8341\n",
            "Epoch 576/1000\n",
            "668/668 [==============================] - 0s 96us/step - loss: 0.2692 - accuracy: 0.9012 - val_loss: 0.7267 - val_accuracy: 0.8341\n",
            "Epoch 577/1000\n",
            "668/668 [==============================] - 0s 119us/step - loss: 0.2644 - accuracy: 0.8907 - val_loss: 0.6817 - val_accuracy: 0.8206\n",
            "Epoch 578/1000\n",
            "668/668 [==============================] - 0s 115us/step - loss: 0.2787 - accuracy: 0.8877 - val_loss: 0.6426 - val_accuracy: 0.8430\n",
            "Epoch 579/1000\n",
            "668/668 [==============================] - 0s 97us/step - loss: 0.2451 - accuracy: 0.9027 - val_loss: 0.7685 - val_accuracy: 0.8341\n",
            "Epoch 580/1000\n",
            "668/668 [==============================] - 0s 96us/step - loss: 0.2456 - accuracy: 0.9087 - val_loss: 0.7812 - val_accuracy: 0.8251\n",
            "Epoch 581/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.2413 - accuracy: 0.8967 - val_loss: 0.7933 - val_accuracy: 0.8206\n",
            "Epoch 582/1000\n",
            "668/668 [==============================] - 0s 94us/step - loss: 0.2555 - accuracy: 0.8967 - val_loss: 0.8536 - val_accuracy: 0.8206\n",
            "Epoch 583/1000\n",
            "668/668 [==============================] - 0s 94us/step - loss: 0.2471 - accuracy: 0.9027 - val_loss: 0.8875 - val_accuracy: 0.8430\n",
            "Epoch 584/1000\n",
            "668/668 [==============================] - 0s 96us/step - loss: 0.2472 - accuracy: 0.9087 - val_loss: 0.8579 - val_accuracy: 0.8341\n",
            "Epoch 585/1000\n",
            "668/668 [==============================] - 0s 95us/step - loss: 0.2617 - accuracy: 0.8862 - val_loss: 0.7573 - val_accuracy: 0.8117\n",
            "Epoch 586/1000\n",
            "668/668 [==============================] - 0s 91us/step - loss: 0.2307 - accuracy: 0.9087 - val_loss: 0.8644 - val_accuracy: 0.8296\n",
            "Epoch 587/1000\n",
            "668/668 [==============================] - 0s 93us/step - loss: 0.2557 - accuracy: 0.8967 - val_loss: 0.8379 - val_accuracy: 0.8296\n",
            "Epoch 588/1000\n",
            "668/668 [==============================] - 0s 93us/step - loss: 0.2480 - accuracy: 0.9132 - val_loss: 0.8429 - val_accuracy: 0.8206\n",
            "Epoch 589/1000\n",
            "668/668 [==============================] - 0s 95us/step - loss: 0.2218 - accuracy: 0.9102 - val_loss: 0.9047 - val_accuracy: 0.8430\n",
            "Epoch 590/1000\n",
            "668/668 [==============================] - 0s 97us/step - loss: 0.2457 - accuracy: 0.9027 - val_loss: 0.8438 - val_accuracy: 0.8296\n",
            "Epoch 591/1000\n",
            "668/668 [==============================] - 0s 94us/step - loss: 0.2475 - accuracy: 0.8997 - val_loss: 0.8734 - val_accuracy: 0.8386\n",
            "Epoch 592/1000\n",
            "668/668 [==============================] - 0s 113us/step - loss: 0.2336 - accuracy: 0.9132 - val_loss: 0.8353 - val_accuracy: 0.8430\n",
            "Epoch 593/1000\n",
            "668/668 [==============================] - 0s 91us/step - loss: 0.2627 - accuracy: 0.9057 - val_loss: 0.7487 - val_accuracy: 0.8430\n",
            "Epoch 594/1000\n",
            "668/668 [==============================] - 0s 95us/step - loss: 0.2468 - accuracy: 0.8922 - val_loss: 0.8037 - val_accuracy: 0.8520\n",
            "Epoch 595/1000\n",
            "668/668 [==============================] - 0s 95us/step - loss: 0.2528 - accuracy: 0.8922 - val_loss: 0.8177 - val_accuracy: 0.8430\n",
            "Epoch 596/1000\n",
            "668/668 [==============================] - 0s 95us/step - loss: 0.2432 - accuracy: 0.9012 - val_loss: 0.7986 - val_accuracy: 0.8475\n",
            "Epoch 597/1000\n",
            "668/668 [==============================] - 0s 101us/step - loss: 0.2342 - accuracy: 0.8967 - val_loss: 0.7889 - val_accuracy: 0.8430\n",
            "Epoch 598/1000\n",
            "668/668 [==============================] - 0s 102us/step - loss: 0.2659 - accuracy: 0.8922 - val_loss: 0.7976 - val_accuracy: 0.8430\n",
            "Epoch 599/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.2495 - accuracy: 0.9087 - val_loss: 0.7671 - val_accuracy: 0.8430\n",
            "Epoch 600/1000\n",
            "668/668 [==============================] - 0s 99us/step - loss: 0.2721 - accuracy: 0.8967 - val_loss: 0.7807 - val_accuracy: 0.8430\n",
            "Epoch 601/1000\n",
            "668/668 [==============================] - 0s 95us/step - loss: 0.2636 - accuracy: 0.8937 - val_loss: 0.7019 - val_accuracy: 0.8386\n",
            "Epoch 602/1000\n",
            "668/668 [==============================] - 0s 92us/step - loss: 0.2559 - accuracy: 0.8922 - val_loss: 0.7744 - val_accuracy: 0.8475\n",
            "Epoch 603/1000\n",
            "668/668 [==============================] - 0s 95us/step - loss: 0.2623 - accuracy: 0.9057 - val_loss: 0.7174 - val_accuracy: 0.8430\n",
            "Epoch 604/1000\n",
            "668/668 [==============================] - 0s 88us/step - loss: 0.2428 - accuracy: 0.9027 - val_loss: 0.8009 - val_accuracy: 0.8430\n",
            "Epoch 605/1000\n",
            "668/668 [==============================] - 0s 98us/step - loss: 0.2578 - accuracy: 0.8952 - val_loss: 0.7829 - val_accuracy: 0.8475\n",
            "Epoch 606/1000\n",
            "668/668 [==============================] - 0s 93us/step - loss: 0.2672 - accuracy: 0.8907 - val_loss: 0.7804 - val_accuracy: 0.8251\n",
            "Epoch 607/1000\n",
            "668/668 [==============================] - 0s 101us/step - loss: 0.2380 - accuracy: 0.9042 - val_loss: 0.8583 - val_accuracy: 0.8251\n",
            "Epoch 608/1000\n",
            "668/668 [==============================] - 0s 93us/step - loss: 0.2621 - accuracy: 0.8922 - val_loss: 0.9272 - val_accuracy: 0.8430\n",
            "Epoch 609/1000\n",
            "668/668 [==============================] - 0s 92us/step - loss: 0.2350 - accuracy: 0.9042 - val_loss: 0.7962 - val_accuracy: 0.8386\n",
            "Epoch 610/1000\n",
            "668/668 [==============================] - 0s 96us/step - loss: 0.2659 - accuracy: 0.8952 - val_loss: 0.7962 - val_accuracy: 0.8341\n",
            "Epoch 611/1000\n",
            "668/668 [==============================] - 0s 100us/step - loss: 0.2626 - accuracy: 0.8952 - val_loss: 0.8115 - val_accuracy: 0.8386\n",
            "Epoch 612/1000\n",
            "668/668 [==============================] - 0s 95us/step - loss: 0.2704 - accuracy: 0.8952 - val_loss: 0.7951 - val_accuracy: 0.8251\n",
            "Epoch 613/1000\n",
            "668/668 [==============================] - 0s 102us/step - loss: 0.2448 - accuracy: 0.9012 - val_loss: 0.8455 - val_accuracy: 0.8341\n",
            "Epoch 614/1000\n",
            "668/668 [==============================] - 0s 104us/step - loss: 0.2657 - accuracy: 0.8862 - val_loss: 0.7175 - val_accuracy: 0.8251\n",
            "Epoch 615/1000\n",
            "668/668 [==============================] - 0s 115us/step - loss: 0.2398 - accuracy: 0.8967 - val_loss: 0.7571 - val_accuracy: 0.8430\n",
            "Epoch 616/1000\n",
            "668/668 [==============================] - 0s 96us/step - loss: 0.2514 - accuracy: 0.9012 - val_loss: 0.7310 - val_accuracy: 0.8430\n",
            "Epoch 617/1000\n",
            "668/668 [==============================] - 0s 93us/step - loss: 0.2613 - accuracy: 0.9012 - val_loss: 0.6338 - val_accuracy: 0.8430\n",
            "Epoch 618/1000\n",
            "668/668 [==============================] - 0s 93us/step - loss: 0.2633 - accuracy: 0.8937 - val_loss: 0.6302 - val_accuracy: 0.8341\n",
            "Epoch 619/1000\n",
            "668/668 [==============================] - 0s 95us/step - loss: 0.2385 - accuracy: 0.9012 - val_loss: 0.7227 - val_accuracy: 0.8251\n",
            "Epoch 620/1000\n",
            "668/668 [==============================] - 0s 94us/step - loss: 0.2570 - accuracy: 0.8877 - val_loss: 0.8416 - val_accuracy: 0.8161\n",
            "Epoch 621/1000\n",
            "668/668 [==============================] - 0s 96us/step - loss: 0.2556 - accuracy: 0.8892 - val_loss: 0.7804 - val_accuracy: 0.8161\n",
            "Epoch 622/1000\n",
            "668/668 [==============================] - 0s 100us/step - loss: 0.2426 - accuracy: 0.8952 - val_loss: 0.7137 - val_accuracy: 0.8475\n",
            "Epoch 623/1000\n",
            "668/668 [==============================] - 0s 98us/step - loss: 0.2508 - accuracy: 0.8967 - val_loss: 0.8253 - val_accuracy: 0.8430\n",
            "Epoch 624/1000\n",
            "668/668 [==============================] - 0s 93us/step - loss: 0.2520 - accuracy: 0.9057 - val_loss: 0.7381 - val_accuracy: 0.8386\n",
            "Epoch 625/1000\n",
            "668/668 [==============================] - 0s 96us/step - loss: 0.2555 - accuracy: 0.8952 - val_loss: 0.7791 - val_accuracy: 0.8251\n",
            "Epoch 626/1000\n",
            "668/668 [==============================] - 0s 102us/step - loss: 0.2487 - accuracy: 0.8937 - val_loss: 0.8339 - val_accuracy: 0.8520\n",
            "Epoch 627/1000\n",
            "668/668 [==============================] - 0s 108us/step - loss: 0.2536 - accuracy: 0.8997 - val_loss: 0.8553 - val_accuracy: 0.8430\n",
            "Epoch 628/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.2378 - accuracy: 0.8997 - val_loss: 0.7995 - val_accuracy: 0.8565\n",
            "Epoch 629/1000\n",
            "668/668 [==============================] - 0s 94us/step - loss: 0.2408 - accuracy: 0.9117 - val_loss: 0.7906 - val_accuracy: 0.8296\n",
            "Epoch 630/1000\n",
            "668/668 [==============================] - 0s 95us/step - loss: 0.2548 - accuracy: 0.8967 - val_loss: 0.8446 - val_accuracy: 0.8206\n",
            "Epoch 631/1000\n",
            "668/668 [==============================] - 0s 95us/step - loss: 0.2362 - accuracy: 0.9102 - val_loss: 0.8927 - val_accuracy: 0.8386\n",
            "Epoch 632/1000\n",
            "668/668 [==============================] - 0s 112us/step - loss: 0.2440 - accuracy: 0.9012 - val_loss: 0.8696 - val_accuracy: 0.8386\n",
            "Epoch 633/1000\n",
            "668/668 [==============================] - 0s 107us/step - loss: 0.2576 - accuracy: 0.9012 - val_loss: 0.8783 - val_accuracy: 0.8206\n",
            "Epoch 634/1000\n",
            "668/668 [==============================] - 0s 92us/step - loss: 0.2654 - accuracy: 0.9012 - val_loss: 0.7110 - val_accuracy: 0.8386\n",
            "Epoch 635/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.2577 - accuracy: 0.8892 - val_loss: 0.7098 - val_accuracy: 0.8475\n",
            "Epoch 636/1000\n",
            "668/668 [==============================] - 0s 109us/step - loss: 0.2422 - accuracy: 0.9027 - val_loss: 0.7897 - val_accuracy: 0.8610\n",
            "Epoch 637/1000\n",
            "668/668 [==============================] - 0s 124us/step - loss: 0.2579 - accuracy: 0.8922 - val_loss: 0.7324 - val_accuracy: 0.8386\n",
            "Epoch 638/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.2515 - accuracy: 0.9057 - val_loss: 0.7813 - val_accuracy: 0.8520\n",
            "Epoch 639/1000\n",
            "668/668 [==============================] - 0s 91us/step - loss: 0.2432 - accuracy: 0.9057 - val_loss: 0.7837 - val_accuracy: 0.8430\n",
            "Epoch 640/1000\n",
            "668/668 [==============================] - 0s 94us/step - loss: 0.2576 - accuracy: 0.9072 - val_loss: 0.7768 - val_accuracy: 0.8430\n",
            "Epoch 641/1000\n",
            "668/668 [==============================] - 0s 93us/step - loss: 0.2484 - accuracy: 0.8997 - val_loss: 0.8300 - val_accuracy: 0.8475\n",
            "Epoch 642/1000\n",
            "668/668 [==============================] - 0s 96us/step - loss: 0.2573 - accuracy: 0.8937 - val_loss: 0.9066 - val_accuracy: 0.8610\n",
            "Epoch 643/1000\n",
            "668/668 [==============================] - 0s 99us/step - loss: 0.2510 - accuracy: 0.8982 - val_loss: 0.7727 - val_accuracy: 0.8341\n",
            "Epoch 644/1000\n",
            "668/668 [==============================] - 0s 100us/step - loss: 0.2482 - accuracy: 0.8982 - val_loss: 0.8690 - val_accuracy: 0.8386\n",
            "Epoch 645/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.2532 - accuracy: 0.8952 - val_loss: 0.7580 - val_accuracy: 0.8386\n",
            "Epoch 646/1000\n",
            "668/668 [==============================] - 0s 101us/step - loss: 0.2555 - accuracy: 0.8967 - val_loss: 0.7864 - val_accuracy: 0.8610\n",
            "Epoch 647/1000\n",
            "668/668 [==============================] - 0s 96us/step - loss: 0.2510 - accuracy: 0.8997 - val_loss: 0.7746 - val_accuracy: 0.8565\n",
            "Epoch 648/1000\n",
            "668/668 [==============================] - 0s 94us/step - loss: 0.2397 - accuracy: 0.9072 - val_loss: 0.8233 - val_accuracy: 0.8520\n",
            "Epoch 649/1000\n",
            "668/668 [==============================] - 0s 110us/step - loss: 0.2431 - accuracy: 0.8982 - val_loss: 0.8916 - val_accuracy: 0.8430\n",
            "Epoch 650/1000\n",
            "668/668 [==============================] - 0s 96us/step - loss: 0.2283 - accuracy: 0.9012 - val_loss: 0.9950 - val_accuracy: 0.8341\n",
            "Epoch 651/1000\n",
            "668/668 [==============================] - 0s 93us/step - loss: 0.2515 - accuracy: 0.9042 - val_loss: 0.8984 - val_accuracy: 0.8386\n",
            "Epoch 652/1000\n",
            "668/668 [==============================] - 0s 98us/step - loss: 0.2624 - accuracy: 0.8952 - val_loss: 0.7498 - val_accuracy: 0.8565\n",
            "Epoch 653/1000\n",
            "668/668 [==============================] - 0s 105us/step - loss: 0.2609 - accuracy: 0.8877 - val_loss: 0.6708 - val_accuracy: 0.8565\n",
            "Epoch 654/1000\n",
            "668/668 [==============================] - 0s 101us/step - loss: 0.2689 - accuracy: 0.8892 - val_loss: 0.6886 - val_accuracy: 0.8520\n",
            "Epoch 655/1000\n",
            "668/668 [==============================] - 0s 111us/step - loss: 0.2489 - accuracy: 0.8982 - val_loss: 0.7686 - val_accuracy: 0.8296\n",
            "Epoch 656/1000\n",
            "668/668 [==============================] - 0s 105us/step - loss: 0.2505 - accuracy: 0.9027 - val_loss: 0.7036 - val_accuracy: 0.8341\n",
            "Epoch 657/1000\n",
            "668/668 [==============================] - 0s 93us/step - loss: 0.2420 - accuracy: 0.8982 - val_loss: 0.7455 - val_accuracy: 0.8565\n",
            "Epoch 658/1000\n",
            "668/668 [==============================] - 0s 91us/step - loss: 0.2342 - accuracy: 0.9072 - val_loss: 0.8817 - val_accuracy: 0.8610\n",
            "Epoch 659/1000\n",
            "668/668 [==============================] - 0s 92us/step - loss: 0.2569 - accuracy: 0.8862 - val_loss: 0.7675 - val_accuracy: 0.8475\n",
            "Epoch 660/1000\n",
            "668/668 [==============================] - 0s 91us/step - loss: 0.2554 - accuracy: 0.9117 - val_loss: 0.8379 - val_accuracy: 0.8430\n",
            "Epoch 661/1000\n",
            "668/668 [==============================] - 0s 103us/step - loss: 0.2513 - accuracy: 0.8892 - val_loss: 0.8245 - val_accuracy: 0.8386\n",
            "Epoch 662/1000\n",
            "668/668 [==============================] - 0s 92us/step - loss: 0.2518 - accuracy: 0.8922 - val_loss: 0.8015 - val_accuracy: 0.8475\n",
            "Epoch 663/1000\n",
            "668/668 [==============================] - 0s 93us/step - loss: 0.2405 - accuracy: 0.9072 - val_loss: 0.8273 - val_accuracy: 0.8430\n",
            "Epoch 664/1000\n",
            "668/668 [==============================] - 0s 95us/step - loss: 0.2834 - accuracy: 0.8862 - val_loss: 0.7993 - val_accuracy: 0.8430\n",
            "Epoch 665/1000\n",
            "668/668 [==============================] - 0s 92us/step - loss: 0.2549 - accuracy: 0.8907 - val_loss: 0.7455 - val_accuracy: 0.8430\n",
            "Epoch 666/1000\n",
            "668/668 [==============================] - 0s 91us/step - loss: 0.2441 - accuracy: 0.9042 - val_loss: 0.7784 - val_accuracy: 0.8475\n",
            "Epoch 667/1000\n",
            "668/668 [==============================] - 0s 95us/step - loss: 0.2468 - accuracy: 0.8997 - val_loss: 0.8870 - val_accuracy: 0.8565\n",
            "Epoch 668/1000\n",
            "668/668 [==============================] - 0s 107us/step - loss: 0.2419 - accuracy: 0.9162 - val_loss: 0.8723 - val_accuracy: 0.8475\n",
            "Epoch 669/1000\n",
            "668/668 [==============================] - 0s 98us/step - loss: 0.2660 - accuracy: 0.8997 - val_loss: 0.7482 - val_accuracy: 0.8475\n",
            "Epoch 670/1000\n",
            "668/668 [==============================] - 0s 93us/step - loss: 0.2680 - accuracy: 0.8862 - val_loss: 0.7339 - val_accuracy: 0.8475\n",
            "Epoch 671/1000\n",
            "668/668 [==============================] - 0s 97us/step - loss: 0.2587 - accuracy: 0.8922 - val_loss: 0.7024 - val_accuracy: 0.8386\n",
            "Epoch 672/1000\n",
            "668/668 [==============================] - 0s 97us/step - loss: 0.2334 - accuracy: 0.9057 - val_loss: 0.8605 - val_accuracy: 0.8430\n",
            "Epoch 673/1000\n",
            "668/668 [==============================] - 0s 106us/step - loss: 0.2609 - accuracy: 0.8967 - val_loss: 0.7734 - val_accuracy: 0.8565\n",
            "Epoch 674/1000\n",
            "668/668 [==============================] - 0s 95us/step - loss: 0.2388 - accuracy: 0.9012 - val_loss: 0.8121 - val_accuracy: 0.8430\n",
            "Epoch 675/1000\n",
            "668/668 [==============================] - 0s 105us/step - loss: 0.2739 - accuracy: 0.8967 - val_loss: 0.7502 - val_accuracy: 0.8430\n",
            "Epoch 676/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.2581 - accuracy: 0.8997 - val_loss: 0.6140 - val_accuracy: 0.8475\n",
            "Epoch 677/1000\n",
            "668/668 [==============================] - 0s 92us/step - loss: 0.2542 - accuracy: 0.8952 - val_loss: 0.6375 - val_accuracy: 0.8475\n",
            "Epoch 678/1000\n",
            "668/668 [==============================] - 0s 97us/step - loss: 0.2524 - accuracy: 0.8997 - val_loss: 0.6524 - val_accuracy: 0.8341\n",
            "Epoch 679/1000\n",
            "668/668 [==============================] - 0s 96us/step - loss: 0.2565 - accuracy: 0.8982 - val_loss: 0.6293 - val_accuracy: 0.8430\n",
            "Epoch 680/1000\n",
            "668/668 [==============================] - 0s 95us/step - loss: 0.2514 - accuracy: 0.9012 - val_loss: 0.6791 - val_accuracy: 0.8430\n",
            "Epoch 681/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.2463 - accuracy: 0.9027 - val_loss: 0.6923 - val_accuracy: 0.8475\n",
            "Epoch 682/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.2621 - accuracy: 0.8952 - val_loss: 0.7064 - val_accuracy: 0.8386\n",
            "Epoch 683/1000\n",
            "668/668 [==============================] - 0s 104us/step - loss: 0.2499 - accuracy: 0.9012 - val_loss: 0.7307 - val_accuracy: 0.8386\n",
            "Epoch 684/1000\n",
            "668/668 [==============================] - 0s 92us/step - loss: 0.2415 - accuracy: 0.9027 - val_loss: 0.8036 - val_accuracy: 0.8341\n",
            "Epoch 685/1000\n",
            "668/668 [==============================] - 0s 105us/step - loss: 0.2382 - accuracy: 0.9087 - val_loss: 0.8360 - val_accuracy: 0.8386\n",
            "Epoch 686/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.2363 - accuracy: 0.9087 - val_loss: 0.7714 - val_accuracy: 0.8341\n",
            "Epoch 687/1000\n",
            "668/668 [==============================] - 0s 94us/step - loss: 0.2514 - accuracy: 0.8892 - val_loss: 0.7996 - val_accuracy: 0.8520\n",
            "Epoch 688/1000\n",
            "668/668 [==============================] - 0s 93us/step - loss: 0.2616 - accuracy: 0.8967 - val_loss: 0.7916 - val_accuracy: 0.8430\n",
            "Epoch 689/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.2361 - accuracy: 0.9027 - val_loss: 0.8812 - val_accuracy: 0.8565\n",
            "Epoch 690/1000\n",
            "668/668 [==============================] - 0s 88us/step - loss: 0.2684 - accuracy: 0.8997 - val_loss: 0.7499 - val_accuracy: 0.8341\n",
            "Epoch 691/1000\n",
            "668/668 [==============================] - 0s 94us/step - loss: 0.2445 - accuracy: 0.9042 - val_loss: 0.7874 - val_accuracy: 0.8430\n",
            "Epoch 692/1000\n",
            "668/668 [==============================] - 0s 95us/step - loss: 0.2459 - accuracy: 0.8922 - val_loss: 0.7743 - val_accuracy: 0.8430\n",
            "Epoch 693/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.2350 - accuracy: 0.9012 - val_loss: 0.8428 - val_accuracy: 0.8430\n",
            "Epoch 694/1000\n",
            "668/668 [==============================] - 0s 91us/step - loss: 0.2479 - accuracy: 0.9072 - val_loss: 0.8739 - val_accuracy: 0.8520\n",
            "Epoch 695/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.2439 - accuracy: 0.9042 - val_loss: 0.8707 - val_accuracy: 0.8520\n",
            "Epoch 696/1000\n",
            "668/668 [==============================] - 0s 92us/step - loss: 0.2415 - accuracy: 0.8937 - val_loss: 0.8411 - val_accuracy: 0.8475\n",
            "Epoch 697/1000\n",
            "668/668 [==============================] - 0s 109us/step - loss: 0.2434 - accuracy: 0.9012 - val_loss: 0.8347 - val_accuracy: 0.8296\n",
            "Epoch 698/1000\n",
            "668/668 [==============================] - 0s 108us/step - loss: 0.2504 - accuracy: 0.8997 - val_loss: 0.8829 - val_accuracy: 0.8296\n",
            "Epoch 699/1000\n",
            "668/668 [==============================] - 0s 105us/step - loss: 0.2458 - accuracy: 0.8997 - val_loss: 0.8535 - val_accuracy: 0.8341\n",
            "Epoch 700/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.2360 - accuracy: 0.9042 - val_loss: 0.8930 - val_accuracy: 0.8475\n",
            "Epoch 701/1000\n",
            "668/668 [==============================] - 0s 93us/step - loss: 0.2290 - accuracy: 0.9072 - val_loss: 0.9279 - val_accuracy: 0.8520\n",
            "Epoch 702/1000\n",
            "668/668 [==============================] - 0s 96us/step - loss: 0.2449 - accuracy: 0.9132 - val_loss: 0.8256 - val_accuracy: 0.8475\n",
            "Epoch 703/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.2404 - accuracy: 0.9102 - val_loss: 0.7185 - val_accuracy: 0.8475\n",
            "Epoch 704/1000\n",
            "668/668 [==============================] - 0s 88us/step - loss: 0.2412 - accuracy: 0.9087 - val_loss: 0.8633 - val_accuracy: 0.8475\n",
            "Epoch 705/1000\n",
            "668/668 [==============================] - 0s 95us/step - loss: 0.2573 - accuracy: 0.8982 - val_loss: 0.8265 - val_accuracy: 0.8475\n",
            "Epoch 706/1000\n",
            "668/668 [==============================] - 0s 87us/step - loss: 0.2543 - accuracy: 0.8922 - val_loss: 0.9032 - val_accuracy: 0.8520\n",
            "Epoch 707/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.2478 - accuracy: 0.9042 - val_loss: 0.9950 - val_accuracy: 0.8475\n",
            "Epoch 708/1000\n",
            "668/668 [==============================] - 0s 88us/step - loss: 0.2748 - accuracy: 0.9042 - val_loss: 0.7476 - val_accuracy: 0.8341\n",
            "Epoch 709/1000\n",
            "668/668 [==============================] - 0s 86us/step - loss: 0.2555 - accuracy: 0.9132 - val_loss: 0.6546 - val_accuracy: 0.8430\n",
            "Epoch 710/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.2567 - accuracy: 0.8952 - val_loss: 0.6833 - val_accuracy: 0.8341\n",
            "Epoch 711/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.2357 - accuracy: 0.9057 - val_loss: 0.6866 - val_accuracy: 0.8430\n",
            "Epoch 712/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.2521 - accuracy: 0.8997 - val_loss: 0.7495 - val_accuracy: 0.8430\n",
            "Epoch 713/1000\n",
            "668/668 [==============================] - 0s 92us/step - loss: 0.2482 - accuracy: 0.9012 - val_loss: 0.7138 - val_accuracy: 0.8565\n",
            "Epoch 714/1000\n",
            "668/668 [==============================] - 0s 92us/step - loss: 0.2489 - accuracy: 0.8967 - val_loss: 0.6798 - val_accuracy: 0.8520\n",
            "Epoch 715/1000\n",
            "668/668 [==============================] - 0s 105us/step - loss: 0.2453 - accuracy: 0.9102 - val_loss: 0.6767 - val_accuracy: 0.8520\n",
            "Epoch 716/1000\n",
            "668/668 [==============================] - 0s 92us/step - loss: 0.2349 - accuracy: 0.9057 - val_loss: 0.7503 - val_accuracy: 0.8520\n",
            "Epoch 717/1000\n",
            "668/668 [==============================] - 0s 91us/step - loss: 0.2320 - accuracy: 0.9057 - val_loss: 0.7596 - val_accuracy: 0.8520\n",
            "Epoch 718/1000\n",
            "668/668 [==============================] - 0s 95us/step - loss: 0.2608 - accuracy: 0.9012 - val_loss: 0.6882 - val_accuracy: 0.8475\n",
            "Epoch 719/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.2243 - accuracy: 0.9147 - val_loss: 0.7509 - val_accuracy: 0.8296\n",
            "Epoch 720/1000\n",
            "668/668 [==============================] - 0s 91us/step - loss: 0.2376 - accuracy: 0.9042 - val_loss: 0.8128 - val_accuracy: 0.8386\n",
            "Epoch 721/1000\n",
            "668/668 [==============================] - 0s 92us/step - loss: 0.2273 - accuracy: 0.9117 - val_loss: 0.8768 - val_accuracy: 0.8430\n",
            "Epoch 722/1000\n",
            "668/668 [==============================] - 0s 88us/step - loss: 0.2434 - accuracy: 0.9042 - val_loss: 0.8325 - val_accuracy: 0.8251\n",
            "Epoch 723/1000\n",
            "668/668 [==============================] - 0s 102us/step - loss: 0.2317 - accuracy: 0.9057 - val_loss: 0.9358 - val_accuracy: 0.8430\n",
            "Epoch 724/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.2222 - accuracy: 0.9132 - val_loss: 0.8951 - val_accuracy: 0.8386\n",
            "Epoch 725/1000\n",
            "668/668 [==============================] - 0s 91us/step - loss: 0.2571 - accuracy: 0.9057 - val_loss: 0.8176 - val_accuracy: 0.8386\n",
            "Epoch 726/1000\n",
            "668/668 [==============================] - 0s 91us/step - loss: 0.2665 - accuracy: 0.9012 - val_loss: 0.6852 - val_accuracy: 0.8296\n",
            "Epoch 727/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.2481 - accuracy: 0.9072 - val_loss: 0.7188 - val_accuracy: 0.8386\n",
            "Epoch 728/1000\n",
            "668/668 [==============================] - 0s 119us/step - loss: 0.2541 - accuracy: 0.8967 - val_loss: 0.7047 - val_accuracy: 0.8386\n",
            "Epoch 729/1000\n",
            "668/668 [==============================] - 0s 104us/step - loss: 0.2587 - accuracy: 0.9027 - val_loss: 0.6998 - val_accuracy: 0.8341\n",
            "Epoch 730/1000\n",
            "668/668 [==============================] - 0s 102us/step - loss: 0.2438 - accuracy: 0.8967 - val_loss: 0.8047 - val_accuracy: 0.8251\n",
            "Epoch 731/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.2709 - accuracy: 0.8922 - val_loss: 0.7224 - val_accuracy: 0.8386\n",
            "Epoch 732/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.2324 - accuracy: 0.9087 - val_loss: 0.8478 - val_accuracy: 0.8386\n",
            "Epoch 733/1000\n",
            "668/668 [==============================] - 0s 93us/step - loss: 0.2553 - accuracy: 0.8997 - val_loss: 0.8331 - val_accuracy: 0.8386\n",
            "Epoch 734/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.2588 - accuracy: 0.8967 - val_loss: 0.7871 - val_accuracy: 0.8430\n",
            "Epoch 735/1000\n",
            "668/668 [==============================] - 0s 97us/step - loss: 0.2432 - accuracy: 0.8997 - val_loss: 0.8200 - val_accuracy: 0.8475\n",
            "Epoch 736/1000\n",
            "668/668 [==============================] - 0s 94us/step - loss: 0.2394 - accuracy: 0.9027 - val_loss: 0.8308 - val_accuracy: 0.8475\n",
            "Epoch 737/1000\n",
            "668/668 [==============================] - 0s 91us/step - loss: 0.2269 - accuracy: 0.8997 - val_loss: 0.8263 - val_accuracy: 0.8520\n",
            "Epoch 738/1000\n",
            "668/668 [==============================] - 0s 91us/step - loss: 0.2404 - accuracy: 0.9042 - val_loss: 0.8445 - val_accuracy: 0.8520\n",
            "Epoch 739/1000\n",
            "668/668 [==============================] - 0s 88us/step - loss: 0.2153 - accuracy: 0.9072 - val_loss: 0.9338 - val_accuracy: 0.8341\n",
            "Epoch 740/1000\n",
            "668/668 [==============================] - 0s 91us/step - loss: 0.2357 - accuracy: 0.9072 - val_loss: 0.9070 - val_accuracy: 0.8251\n",
            "Epoch 741/1000\n",
            "668/668 [==============================] - 0s 95us/step - loss: 0.2340 - accuracy: 0.9042 - val_loss: 0.9935 - val_accuracy: 0.8296\n",
            "Epoch 742/1000\n",
            "668/668 [==============================] - 0s 91us/step - loss: 0.2480 - accuracy: 0.8937 - val_loss: 1.0192 - val_accuracy: 0.8386\n",
            "Epoch 743/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.2588 - accuracy: 0.9087 - val_loss: 0.7998 - val_accuracy: 0.8296\n",
            "Epoch 744/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.2415 - accuracy: 0.8982 - val_loss: 0.7562 - val_accuracy: 0.8430\n",
            "Epoch 745/1000\n",
            "668/668 [==============================] - 0s 91us/step - loss: 0.2533 - accuracy: 0.8967 - val_loss: 0.8181 - val_accuracy: 0.8520\n",
            "Epoch 746/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.2451 - accuracy: 0.9042 - val_loss: 0.8818 - val_accuracy: 0.8430\n",
            "Epoch 747/1000\n",
            "668/668 [==============================] - 0s 96us/step - loss: 0.2405 - accuracy: 0.9027 - val_loss: 0.8825 - val_accuracy: 0.8520\n",
            "Epoch 748/1000\n",
            "668/668 [==============================] - 0s 100us/step - loss: 0.2435 - accuracy: 0.8982 - val_loss: 0.8417 - val_accuracy: 0.8341\n",
            "Epoch 749/1000\n",
            "668/668 [==============================] - 0s 91us/step - loss: 0.2202 - accuracy: 0.9132 - val_loss: 0.8462 - val_accuracy: 0.8430\n",
            "Epoch 750/1000\n",
            "668/668 [==============================] - 0s 95us/step - loss: 0.2336 - accuracy: 0.9102 - val_loss: 0.7292 - val_accuracy: 0.8475\n",
            "Epoch 751/1000\n",
            "668/668 [==============================] - 0s 94us/step - loss: 0.2288 - accuracy: 0.9057 - val_loss: 0.8839 - val_accuracy: 0.8251\n",
            "Epoch 752/1000\n",
            "668/668 [==============================] - 0s 93us/step - loss: 0.2461 - accuracy: 0.9027 - val_loss: 0.8424 - val_accuracy: 0.8430\n",
            "Epoch 753/1000\n",
            "668/668 [==============================] - 0s 93us/step - loss: 0.2575 - accuracy: 0.9027 - val_loss: 0.8333 - val_accuracy: 0.8341\n",
            "Epoch 754/1000\n",
            "668/668 [==============================] - 0s 107us/step - loss: 0.2256 - accuracy: 0.9087 - val_loss: 0.9539 - val_accuracy: 0.8430\n",
            "Epoch 755/1000\n",
            "668/668 [==============================] - 0s 92us/step - loss: 0.2548 - accuracy: 0.8967 - val_loss: 0.8285 - val_accuracy: 0.8386\n",
            "Epoch 756/1000\n",
            "668/668 [==============================] - 0s 93us/step - loss: 0.2244 - accuracy: 0.9027 - val_loss: 0.8930 - val_accuracy: 0.8475\n",
            "Epoch 757/1000\n",
            "668/668 [==============================] - 0s 93us/step - loss: 0.2295 - accuracy: 0.9102 - val_loss: 0.9508 - val_accuracy: 0.8475\n",
            "Epoch 758/1000\n",
            "668/668 [==============================] - 0s 92us/step - loss: 0.2382 - accuracy: 0.8997 - val_loss: 0.9616 - val_accuracy: 0.8475\n",
            "Epoch 759/1000\n",
            "668/668 [==============================] - 0s 92us/step - loss: 0.2204 - accuracy: 0.9102 - val_loss: 1.0174 - val_accuracy: 0.8520\n",
            "Epoch 760/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.2327 - accuracy: 0.9162 - val_loss: 0.9178 - val_accuracy: 0.8475\n",
            "Epoch 761/1000\n",
            "668/668 [==============================] - 0s 94us/step - loss: 0.2217 - accuracy: 0.9117 - val_loss: 0.8663 - val_accuracy: 0.8475\n",
            "Epoch 762/1000\n",
            "668/668 [==============================] - 0s 106us/step - loss: 0.2321 - accuracy: 0.9027 - val_loss: 0.8412 - val_accuracy: 0.8475\n",
            "Epoch 763/1000\n",
            "668/668 [==============================] - 0s 94us/step - loss: 0.2153 - accuracy: 0.9192 - val_loss: 1.0462 - val_accuracy: 0.8296\n",
            "Epoch 764/1000\n",
            "668/668 [==============================] - 0s 88us/step - loss: 0.2360 - accuracy: 0.9162 - val_loss: 0.9682 - val_accuracy: 0.8475\n",
            "Epoch 765/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.2372 - accuracy: 0.9117 - val_loss: 0.9487 - val_accuracy: 0.8475\n",
            "Epoch 766/1000\n",
            "668/668 [==============================] - 0s 91us/step - loss: 0.2450 - accuracy: 0.9072 - val_loss: 0.9557 - val_accuracy: 0.8475\n",
            "Epoch 767/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.2334 - accuracy: 0.9117 - val_loss: 0.9101 - val_accuracy: 0.8386\n",
            "Epoch 768/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.2334 - accuracy: 0.9042 - val_loss: 0.9032 - val_accuracy: 0.8520\n",
            "Epoch 769/1000\n",
            "668/668 [==============================] - 0s 91us/step - loss: 0.2363 - accuracy: 0.9072 - val_loss: 0.8958 - val_accuracy: 0.8520\n",
            "Epoch 770/1000\n",
            "668/668 [==============================] - 0s 92us/step - loss: 0.2114 - accuracy: 0.9162 - val_loss: 1.0164 - val_accuracy: 0.8520\n",
            "Epoch 771/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.2168 - accuracy: 0.9132 - val_loss: 1.0984 - val_accuracy: 0.8430\n",
            "Epoch 772/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.2299 - accuracy: 0.9012 - val_loss: 0.9527 - val_accuracy: 0.8430\n",
            "Epoch 773/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.2532 - accuracy: 0.9042 - val_loss: 0.8956 - val_accuracy: 0.8430\n",
            "Epoch 774/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.2432 - accuracy: 0.9072 - val_loss: 0.9767 - val_accuracy: 0.8430\n",
            "Epoch 775/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.2414 - accuracy: 0.9072 - val_loss: 1.0019 - val_accuracy: 0.8296\n",
            "Epoch 776/1000\n",
            "668/668 [==============================] - 0s 92us/step - loss: 0.2593 - accuracy: 0.9012 - val_loss: 0.8891 - val_accuracy: 0.8251\n",
            "Epoch 777/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.2416 - accuracy: 0.9072 - val_loss: 0.9529 - val_accuracy: 0.8430\n",
            "Epoch 778/1000\n",
            "668/668 [==============================] - 0s 109us/step - loss: 0.2585 - accuracy: 0.8982 - val_loss: 1.0059 - val_accuracy: 0.8430\n",
            "Epoch 779/1000\n",
            "668/668 [==============================] - 0s 94us/step - loss: 0.2364 - accuracy: 0.9102 - val_loss: 0.9407 - val_accuracy: 0.8475\n",
            "Epoch 780/1000\n",
            "668/668 [==============================] - 0s 86us/step - loss: 0.2379 - accuracy: 0.8982 - val_loss: 0.9350 - val_accuracy: 0.8430\n",
            "Epoch 781/1000\n",
            "668/668 [==============================] - 0s 91us/step - loss: 0.2454 - accuracy: 0.9102 - val_loss: 0.8057 - val_accuracy: 0.8475\n",
            "Epoch 782/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.2397 - accuracy: 0.9057 - val_loss: 0.8622 - val_accuracy: 0.8475\n",
            "Epoch 783/1000\n",
            "668/668 [==============================] - 0s 92us/step - loss: 0.2378 - accuracy: 0.9087 - val_loss: 0.9568 - val_accuracy: 0.8430\n",
            "Epoch 784/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.2251 - accuracy: 0.9102 - val_loss: 1.0216 - val_accuracy: 0.8341\n",
            "Epoch 785/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.2248 - accuracy: 0.9117 - val_loss: 0.9895 - val_accuracy: 0.8475\n",
            "Epoch 786/1000\n",
            "668/668 [==============================] - 0s 88us/step - loss: 0.2427 - accuracy: 0.9042 - val_loss: 0.9288 - val_accuracy: 0.8475\n",
            "Epoch 787/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.2189 - accuracy: 0.9147 - val_loss: 0.9087 - val_accuracy: 0.8386\n",
            "Epoch 788/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.2477 - accuracy: 0.9102 - val_loss: 0.9754 - val_accuracy: 0.8565\n",
            "Epoch 789/1000\n",
            "668/668 [==============================] - 0s 103us/step - loss: 0.2528 - accuracy: 0.8922 - val_loss: 0.9209 - val_accuracy: 0.8520\n",
            "Epoch 790/1000\n",
            "668/668 [==============================] - 0s 105us/step - loss: 0.2351 - accuracy: 0.9072 - val_loss: 0.8866 - val_accuracy: 0.8565\n",
            "Epoch 791/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.2551 - accuracy: 0.9042 - val_loss: 0.7923 - val_accuracy: 0.8475\n",
            "Epoch 792/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.2506 - accuracy: 0.9042 - val_loss: 0.7863 - val_accuracy: 0.8520\n",
            "Epoch 793/1000\n",
            "668/668 [==============================] - 0s 93us/step - loss: 0.2672 - accuracy: 0.8982 - val_loss: 0.8089 - val_accuracy: 0.8520\n",
            "Epoch 794/1000\n",
            "668/668 [==============================] - 0s 106us/step - loss: 0.2360 - accuracy: 0.9042 - val_loss: 0.8478 - val_accuracy: 0.8475\n",
            "Epoch 795/1000\n",
            "668/668 [==============================] - 0s 95us/step - loss: 0.2283 - accuracy: 0.9057 - val_loss: 0.9415 - val_accuracy: 0.8520\n",
            "Epoch 796/1000\n",
            "668/668 [==============================] - 0s 92us/step - loss: 0.2367 - accuracy: 0.9012 - val_loss: 0.9182 - val_accuracy: 0.8386\n",
            "Epoch 797/1000\n",
            "668/668 [==============================] - 0s 95us/step - loss: 0.2378 - accuracy: 0.9012 - val_loss: 0.8730 - val_accuracy: 0.8475\n",
            "Epoch 798/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.2374 - accuracy: 0.9057 - val_loss: 0.8641 - val_accuracy: 0.8341\n",
            "Epoch 799/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.2327 - accuracy: 0.9117 - val_loss: 0.7826 - val_accuracy: 0.8475\n",
            "Epoch 800/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.2479 - accuracy: 0.9027 - val_loss: 0.8406 - val_accuracy: 0.8430\n",
            "Epoch 801/1000\n",
            "668/668 [==============================] - 0s 88us/step - loss: 0.2387 - accuracy: 0.9057 - val_loss: 0.8394 - val_accuracy: 0.8341\n",
            "Epoch 802/1000\n",
            "668/668 [==============================] - 0s 91us/step - loss: 0.2327 - accuracy: 0.9042 - val_loss: 0.8395 - val_accuracy: 0.8520\n",
            "Epoch 803/1000\n",
            "668/668 [==============================] - 0s 91us/step - loss: 0.2338 - accuracy: 0.9132 - val_loss: 0.8610 - val_accuracy: 0.8386\n",
            "Epoch 804/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.2301 - accuracy: 0.9042 - val_loss: 0.8470 - val_accuracy: 0.8475\n",
            "Epoch 805/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.2307 - accuracy: 0.9057 - val_loss: 0.8712 - val_accuracy: 0.8386\n",
            "Epoch 806/1000\n",
            "668/668 [==============================] - 0s 92us/step - loss: 0.2409 - accuracy: 0.9057 - val_loss: 0.8777 - val_accuracy: 0.8341\n",
            "Epoch 807/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.2358 - accuracy: 0.9027 - val_loss: 0.8820 - val_accuracy: 0.8251\n",
            "Epoch 808/1000\n",
            "668/668 [==============================] - 0s 92us/step - loss: 0.2502 - accuracy: 0.9072 - val_loss: 0.9051 - val_accuracy: 0.8341\n",
            "Epoch 809/1000\n",
            "668/668 [==============================] - 0s 92us/step - loss: 0.2298 - accuracy: 0.9072 - val_loss: 0.8717 - val_accuracy: 0.8296\n",
            "Epoch 810/1000\n",
            "668/668 [==============================] - 0s 97us/step - loss: 0.2455 - accuracy: 0.9072 - val_loss: 0.8899 - val_accuracy: 0.8386\n",
            "Epoch 811/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.2408 - accuracy: 0.9012 - val_loss: 0.9291 - val_accuracy: 0.8386\n",
            "Epoch 812/1000\n",
            "668/668 [==============================] - 0s 104us/step - loss: 0.2537 - accuracy: 0.9072 - val_loss: 0.9528 - val_accuracy: 0.8206\n",
            "Epoch 813/1000\n",
            "668/668 [==============================] - 0s 91us/step - loss: 0.2296 - accuracy: 0.9012 - val_loss: 0.9006 - val_accuracy: 0.8251\n",
            "Epoch 814/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.2348 - accuracy: 0.9027 - val_loss: 0.8408 - val_accuracy: 0.8296\n",
            "Epoch 815/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.2290 - accuracy: 0.9012 - val_loss: 0.9208 - val_accuracy: 0.8475\n",
            "Epoch 816/1000\n",
            "668/668 [==============================] - 0s 93us/step - loss: 0.2665 - accuracy: 0.8967 - val_loss: 0.7523 - val_accuracy: 0.8430\n",
            "Epoch 817/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.2466 - accuracy: 0.8997 - val_loss: 0.9026 - val_accuracy: 0.8386\n",
            "Epoch 818/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.2674 - accuracy: 0.8967 - val_loss: 0.7973 - val_accuracy: 0.8386\n",
            "Epoch 819/1000\n",
            "668/668 [==============================] - 0s 95us/step - loss: 0.2319 - accuracy: 0.9042 - val_loss: 0.9261 - val_accuracy: 0.8430\n",
            "Epoch 820/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.2323 - accuracy: 0.9042 - val_loss: 0.9232 - val_accuracy: 0.8386\n",
            "Epoch 821/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.2470 - accuracy: 0.9072 - val_loss: 1.0772 - val_accuracy: 0.8341\n",
            "Epoch 822/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.2146 - accuracy: 0.9102 - val_loss: 1.0351 - val_accuracy: 0.8430\n",
            "Epoch 823/1000\n",
            "668/668 [==============================] - 0s 92us/step - loss: 0.2255 - accuracy: 0.8967 - val_loss: 1.0185 - val_accuracy: 0.8430\n",
            "Epoch 824/1000\n",
            "668/668 [==============================] - 0s 91us/step - loss: 0.2544 - accuracy: 0.8982 - val_loss: 1.1300 - val_accuracy: 0.8430\n",
            "Epoch 825/1000\n",
            "668/668 [==============================] - 0s 108us/step - loss: 0.2185 - accuracy: 0.9117 - val_loss: 1.0073 - val_accuracy: 0.8475\n",
            "Epoch 826/1000\n",
            "668/668 [==============================] - 0s 109us/step - loss: 0.2378 - accuracy: 0.9012 - val_loss: 1.0259 - val_accuracy: 0.8386\n",
            "Epoch 827/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.2300 - accuracy: 0.9087 - val_loss: 0.9786 - val_accuracy: 0.8430\n",
            "Epoch 828/1000\n",
            "668/668 [==============================] - 0s 91us/step - loss: 0.2500 - accuracy: 0.8967 - val_loss: 0.9616 - val_accuracy: 0.8430\n",
            "Epoch 829/1000\n",
            "668/668 [==============================] - 0s 107us/step - loss: 0.2402 - accuracy: 0.9117 - val_loss: 0.9681 - val_accuracy: 0.8565\n",
            "Epoch 830/1000\n",
            "668/668 [==============================] - 0s 94us/step - loss: 0.2348 - accuracy: 0.9027 - val_loss: 0.9351 - val_accuracy: 0.8475\n",
            "Epoch 831/1000\n",
            "668/668 [==============================] - 0s 91us/step - loss: 0.2378 - accuracy: 0.8952 - val_loss: 1.0089 - val_accuracy: 0.8520\n",
            "Epoch 832/1000\n",
            "668/668 [==============================] - 0s 94us/step - loss: 0.2288 - accuracy: 0.9102 - val_loss: 1.0545 - val_accuracy: 0.8296\n",
            "Epoch 833/1000\n",
            "668/668 [==============================] - 0s 93us/step - loss: 0.2286 - accuracy: 0.9027 - val_loss: 1.0075 - val_accuracy: 0.8430\n",
            "Epoch 834/1000\n",
            "668/668 [==============================] - 0s 92us/step - loss: 0.2336 - accuracy: 0.9057 - val_loss: 1.0491 - val_accuracy: 0.8430\n",
            "Epoch 835/1000\n",
            "668/668 [==============================] - 0s 91us/step - loss: 0.2417 - accuracy: 0.9012 - val_loss: 1.0345 - val_accuracy: 0.8386\n",
            "Epoch 836/1000\n",
            "668/668 [==============================] - 0s 91us/step - loss: 0.2306 - accuracy: 0.9042 - val_loss: 0.9966 - val_accuracy: 0.8296\n",
            "Epoch 837/1000\n",
            "668/668 [==============================] - 0s 93us/step - loss: 0.2267 - accuracy: 0.9087 - val_loss: 1.1312 - val_accuracy: 0.8296\n",
            "Epoch 838/1000\n",
            "668/668 [==============================] - 0s 94us/step - loss: 0.2265 - accuracy: 0.9012 - val_loss: 1.2039 - val_accuracy: 0.8386\n",
            "Epoch 839/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.2108 - accuracy: 0.9057 - val_loss: 1.2688 - val_accuracy: 0.8386\n",
            "Epoch 840/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.2117 - accuracy: 0.9057 - val_loss: 1.1444 - val_accuracy: 0.8430\n",
            "Epoch 841/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.2443 - accuracy: 0.9012 - val_loss: 0.9231 - val_accuracy: 0.8520\n",
            "Epoch 842/1000\n",
            "668/668 [==============================] - 0s 107us/step - loss: 0.2674 - accuracy: 0.8982 - val_loss: 0.8419 - val_accuracy: 0.8520\n",
            "Epoch 843/1000\n",
            "668/668 [==============================] - 0s 93us/step - loss: 0.2333 - accuracy: 0.9012 - val_loss: 0.8227 - val_accuracy: 0.8475\n",
            "Epoch 844/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.2476 - accuracy: 0.9027 - val_loss: 0.8891 - val_accuracy: 0.8430\n",
            "Epoch 845/1000\n",
            "668/668 [==============================] - 0s 88us/step - loss: 0.2581 - accuracy: 0.8967 - val_loss: 0.9683 - val_accuracy: 0.8475\n",
            "Epoch 846/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.2587 - accuracy: 0.8922 - val_loss: 0.7837 - val_accuracy: 0.8430\n",
            "Epoch 847/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.2614 - accuracy: 0.8772 - val_loss: 0.7869 - val_accuracy: 0.8386\n",
            "Epoch 848/1000\n",
            "668/668 [==============================] - 0s 99us/step - loss: 0.2448 - accuracy: 0.9057 - val_loss: 0.8206 - val_accuracy: 0.8341\n",
            "Epoch 849/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.2692 - accuracy: 0.8937 - val_loss: 0.7121 - val_accuracy: 0.8341\n",
            "Epoch 850/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.2377 - accuracy: 0.9117 - val_loss: 0.7686 - val_accuracy: 0.8341\n",
            "Epoch 851/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.2381 - accuracy: 0.9102 - val_loss: 0.8774 - val_accuracy: 0.8430\n",
            "Epoch 852/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.2211 - accuracy: 0.9207 - val_loss: 0.9996 - val_accuracy: 0.8430\n",
            "Epoch 853/1000\n",
            "668/668 [==============================] - 0s 91us/step - loss: 0.2183 - accuracy: 0.9147 - val_loss: 1.0688 - val_accuracy: 0.8430\n",
            "Epoch 854/1000\n",
            "668/668 [==============================] - 0s 92us/step - loss: 0.2216 - accuracy: 0.9087 - val_loss: 0.9640 - val_accuracy: 0.8386\n",
            "Epoch 855/1000\n",
            "668/668 [==============================] - 0s 91us/step - loss: 0.2099 - accuracy: 0.9177 - val_loss: 1.0495 - val_accuracy: 0.8430\n",
            "Epoch 856/1000\n",
            "668/668 [==============================] - 0s 93us/step - loss: 0.2164 - accuracy: 0.9147 - val_loss: 1.1243 - val_accuracy: 0.8341\n",
            "Epoch 857/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.2435 - accuracy: 0.9027 - val_loss: 0.8393 - val_accuracy: 0.8341\n",
            "Epoch 858/1000\n",
            "668/668 [==============================] - 0s 101us/step - loss: 0.2376 - accuracy: 0.8922 - val_loss: 0.9174 - val_accuracy: 0.8430\n",
            "Epoch 859/1000\n",
            "668/668 [==============================] - 0s 105us/step - loss: 0.2268 - accuracy: 0.9117 - val_loss: 0.9385 - val_accuracy: 0.8251\n",
            "Epoch 860/1000\n",
            "668/668 [==============================] - 0s 94us/step - loss: 0.2388 - accuracy: 0.9072 - val_loss: 0.7828 - val_accuracy: 0.8251\n",
            "Epoch 861/1000\n",
            "668/668 [==============================] - 0s 86us/step - loss: 0.2473 - accuracy: 0.8967 - val_loss: 0.7588 - val_accuracy: 0.8341\n",
            "Epoch 862/1000\n",
            "668/668 [==============================] - 0s 93us/step - loss: 0.2597 - accuracy: 0.8922 - val_loss: 0.6858 - val_accuracy: 0.8565\n",
            "Epoch 863/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.2336 - accuracy: 0.9042 - val_loss: 0.7562 - val_accuracy: 0.8386\n",
            "Epoch 864/1000\n",
            "668/668 [==============================] - 0s 93us/step - loss: 0.2318 - accuracy: 0.9102 - val_loss: 0.9176 - val_accuracy: 0.8430\n",
            "Epoch 865/1000\n",
            "668/668 [==============================] - 0s 92us/step - loss: 0.2319 - accuracy: 0.9132 - val_loss: 0.8415 - val_accuracy: 0.8475\n",
            "Epoch 866/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.2367 - accuracy: 0.9117 - val_loss: 0.9190 - val_accuracy: 0.8430\n",
            "Epoch 867/1000\n",
            "668/668 [==============================] - 0s 91us/step - loss: 0.2252 - accuracy: 0.9087 - val_loss: 0.9039 - val_accuracy: 0.8386\n",
            "Epoch 868/1000\n",
            "668/668 [==============================] - 0s 92us/step - loss: 0.2064 - accuracy: 0.9162 - val_loss: 1.0910 - val_accuracy: 0.8341\n",
            "Epoch 869/1000\n",
            "668/668 [==============================] - 0s 93us/step - loss: 0.2497 - accuracy: 0.9072 - val_loss: 0.9712 - val_accuracy: 0.8386\n",
            "Epoch 870/1000\n",
            "668/668 [==============================] - 0s 93us/step - loss: 0.2239 - accuracy: 0.9147 - val_loss: 1.0103 - val_accuracy: 0.8475\n",
            "Epoch 871/1000\n",
            "668/668 [==============================] - 0s 91us/step - loss: 0.2213 - accuracy: 0.9057 - val_loss: 1.0534 - val_accuracy: 0.8475\n",
            "Epoch 872/1000\n",
            "668/668 [==============================] - 0s 92us/step - loss: 0.2405 - accuracy: 0.9027 - val_loss: 0.9978 - val_accuracy: 0.8341\n",
            "Epoch 873/1000\n",
            "668/668 [==============================] - 0s 92us/step - loss: 0.2207 - accuracy: 0.9087 - val_loss: 1.0089 - val_accuracy: 0.8251\n",
            "Epoch 874/1000\n",
            "668/668 [==============================] - 0s 105us/step - loss: 0.2186 - accuracy: 0.9042 - val_loss: 1.0982 - val_accuracy: 0.8341\n",
            "Epoch 875/1000\n",
            "668/668 [==============================] - 0s 95us/step - loss: 0.2291 - accuracy: 0.9012 - val_loss: 1.2653 - val_accuracy: 0.8386\n",
            "Epoch 876/1000\n",
            "668/668 [==============================] - 0s 107us/step - loss: 0.2220 - accuracy: 0.9117 - val_loss: 1.0468 - val_accuracy: 0.8430\n",
            "Epoch 877/1000\n",
            "668/668 [==============================] - 0s 88us/step - loss: 0.2184 - accuracy: 0.9087 - val_loss: 1.1235 - val_accuracy: 0.8475\n",
            "Epoch 878/1000\n",
            "668/668 [==============================] - 0s 92us/step - loss: 0.2184 - accuracy: 0.9072 - val_loss: 1.2940 - val_accuracy: 0.8430\n",
            "Epoch 879/1000\n",
            "668/668 [==============================] - 0s 93us/step - loss: 0.2264 - accuracy: 0.9102 - val_loss: 1.1619 - val_accuracy: 0.8430\n",
            "Epoch 880/1000\n",
            "668/668 [==============================] - 0s 87us/step - loss: 0.2463 - accuracy: 0.9027 - val_loss: 0.8878 - val_accuracy: 0.8386\n",
            "Epoch 881/1000\n",
            "668/668 [==============================] - 0s 88us/step - loss: 0.2296 - accuracy: 0.8982 - val_loss: 0.9852 - val_accuracy: 0.8520\n",
            "Epoch 882/1000\n",
            "668/668 [==============================] - 0s 91us/step - loss: 0.2410 - accuracy: 0.9072 - val_loss: 0.9914 - val_accuracy: 0.8386\n",
            "Epoch 883/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.2713 - accuracy: 0.8967 - val_loss: 0.9668 - val_accuracy: 0.8341\n",
            "Epoch 884/1000\n",
            "668/668 [==============================] - 0s 87us/step - loss: 0.2600 - accuracy: 0.8982 - val_loss: 0.6867 - val_accuracy: 0.8565\n",
            "Epoch 885/1000\n",
            "668/668 [==============================] - 0s 93us/step - loss: 0.2477 - accuracy: 0.8967 - val_loss: 0.8959 - val_accuracy: 0.8520\n",
            "Epoch 886/1000\n",
            "668/668 [==============================] - 0s 91us/step - loss: 0.2290 - accuracy: 0.9027 - val_loss: 1.0452 - val_accuracy: 0.8430\n",
            "Epoch 887/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.2370 - accuracy: 0.9087 - val_loss: 1.0288 - val_accuracy: 0.8520\n",
            "Epoch 888/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.2341 - accuracy: 0.9012 - val_loss: 0.9093 - val_accuracy: 0.8386\n",
            "Epoch 889/1000\n",
            "668/668 [==============================] - 0s 91us/step - loss: 0.2362 - accuracy: 0.8997 - val_loss: 1.1288 - val_accuracy: 0.8296\n",
            "Epoch 890/1000\n",
            "668/668 [==============================] - 0s 124us/step - loss: 0.2420 - accuracy: 0.8937 - val_loss: 0.9859 - val_accuracy: 0.8251\n",
            "Epoch 891/1000\n",
            "668/668 [==============================] - 0s 94us/step - loss: 0.2153 - accuracy: 0.9147 - val_loss: 1.1140 - val_accuracy: 0.8251\n",
            "Epoch 892/1000\n",
            "668/668 [==============================] - 0s 104us/step - loss: 0.2220 - accuracy: 0.9192 - val_loss: 1.1303 - val_accuracy: 0.8341\n",
            "Epoch 893/1000\n",
            "668/668 [==============================] - 0s 108us/step - loss: 0.2327 - accuracy: 0.9117 - val_loss: 1.0050 - val_accuracy: 0.8296\n",
            "Epoch 894/1000\n",
            "668/668 [==============================] - 0s 93us/step - loss: 0.2436 - accuracy: 0.8967 - val_loss: 1.0406 - val_accuracy: 0.8251\n",
            "Epoch 895/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.2459 - accuracy: 0.8982 - val_loss: 0.9775 - val_accuracy: 0.8251\n",
            "Epoch 896/1000\n",
            "668/668 [==============================] - 0s 95us/step - loss: 0.2420 - accuracy: 0.9132 - val_loss: 0.9413 - val_accuracy: 0.8341\n",
            "Epoch 897/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.2217 - accuracy: 0.9117 - val_loss: 1.0228 - val_accuracy: 0.8430\n",
            "Epoch 898/1000\n",
            "668/668 [==============================] - 0s 96us/step - loss: 0.2361 - accuracy: 0.9027 - val_loss: 1.0649 - val_accuracy: 0.8251\n",
            "Epoch 899/1000\n",
            "668/668 [==============================] - 0s 92us/step - loss: 0.2461 - accuracy: 0.8922 - val_loss: 1.0010 - val_accuracy: 0.8341\n",
            "Epoch 900/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.2420 - accuracy: 0.9042 - val_loss: 0.9070 - val_accuracy: 0.8296\n",
            "Epoch 901/1000\n",
            "668/668 [==============================] - 0s 92us/step - loss: 0.2301 - accuracy: 0.8922 - val_loss: 1.0446 - val_accuracy: 0.8296\n",
            "Epoch 902/1000\n",
            "668/668 [==============================] - 0s 91us/step - loss: 0.2249 - accuracy: 0.9042 - val_loss: 1.0982 - val_accuracy: 0.8386\n",
            "Epoch 903/1000\n",
            "668/668 [==============================] - 0s 93us/step - loss: 0.2308 - accuracy: 0.9042 - val_loss: 1.1864 - val_accuracy: 0.8296\n",
            "Epoch 904/1000\n",
            "668/668 [==============================] - 0s 96us/step - loss: 0.2326 - accuracy: 0.9072 - val_loss: 1.2281 - val_accuracy: 0.8386\n",
            "Epoch 905/1000\n",
            "668/668 [==============================] - 0s 107us/step - loss: 0.2130 - accuracy: 0.9102 - val_loss: 1.4345 - val_accuracy: 0.8341\n",
            "Epoch 906/1000\n",
            "668/668 [==============================] - 0s 98us/step - loss: 0.2301 - accuracy: 0.9012 - val_loss: 1.3019 - val_accuracy: 0.8341\n",
            "Epoch 907/1000\n",
            "668/668 [==============================] - 0s 94us/step - loss: 0.2704 - accuracy: 0.8892 - val_loss: 0.8968 - val_accuracy: 0.8341\n",
            "Epoch 908/1000\n",
            "668/668 [==============================] - 0s 97us/step - loss: 0.2400 - accuracy: 0.8982 - val_loss: 0.8844 - val_accuracy: 0.8386\n",
            "Epoch 909/1000\n",
            "668/668 [==============================] - 0s 109us/step - loss: 0.2452 - accuracy: 0.8997 - val_loss: 0.9831 - val_accuracy: 0.8475\n",
            "Epoch 910/1000\n",
            "668/668 [==============================] - 0s 103us/step - loss: 0.2256 - accuracy: 0.9072 - val_loss: 1.1143 - val_accuracy: 0.8475\n",
            "Epoch 911/1000\n",
            "668/668 [==============================] - 0s 94us/step - loss: 0.2396 - accuracy: 0.8997 - val_loss: 1.0568 - val_accuracy: 0.8430\n",
            "Epoch 912/1000\n",
            "668/668 [==============================] - 0s 93us/step - loss: 0.2188 - accuracy: 0.9072 - val_loss: 1.2034 - val_accuracy: 0.8475\n",
            "Epoch 913/1000\n",
            "668/668 [==============================] - 0s 92us/step - loss: 0.2343 - accuracy: 0.9042 - val_loss: 1.0518 - val_accuracy: 0.8430\n",
            "Epoch 914/1000\n",
            "668/668 [==============================] - 0s 86us/step - loss: 0.2193 - accuracy: 0.9192 - val_loss: 1.0277 - val_accuracy: 0.8430\n",
            "Epoch 915/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.2652 - accuracy: 0.8997 - val_loss: 1.0084 - val_accuracy: 0.8341\n",
            "Epoch 916/1000\n",
            "668/668 [==============================] - 0s 92us/step - loss: 0.2291 - accuracy: 0.9012 - val_loss: 0.8764 - val_accuracy: 0.8341\n",
            "Epoch 917/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.2437 - accuracy: 0.8937 - val_loss: 0.9313 - val_accuracy: 0.8341\n",
            "Epoch 918/1000\n",
            "668/668 [==============================] - 0s 94us/step - loss: 0.2242 - accuracy: 0.9057 - val_loss: 1.1049 - val_accuracy: 0.8296\n",
            "Epoch 919/1000\n",
            "668/668 [==============================] - 0s 94us/step - loss: 0.2430 - accuracy: 0.9027 - val_loss: 0.9906 - val_accuracy: 0.8386\n",
            "Epoch 920/1000\n",
            "668/668 [==============================] - 0s 99us/step - loss: 0.2449 - accuracy: 0.8967 - val_loss: 1.0867 - val_accuracy: 0.8386\n",
            "Epoch 921/1000\n",
            "668/668 [==============================] - 0s 100us/step - loss: 0.2204 - accuracy: 0.9087 - val_loss: 1.0478 - val_accuracy: 0.8341\n",
            "Epoch 922/1000\n",
            "668/668 [==============================] - 0s 94us/step - loss: 0.2019 - accuracy: 0.9132 - val_loss: 1.1301 - val_accuracy: 0.8251\n",
            "Epoch 923/1000\n",
            "668/668 [==============================] - 0s 91us/step - loss: 0.2408 - accuracy: 0.9057 - val_loss: 1.0391 - val_accuracy: 0.8386\n",
            "Epoch 924/1000\n",
            "668/668 [==============================] - 0s 91us/step - loss: 0.2297 - accuracy: 0.9102 - val_loss: 0.8973 - val_accuracy: 0.8341\n",
            "Epoch 925/1000\n",
            "668/668 [==============================] - 0s 92us/step - loss: 0.2572 - accuracy: 0.8922 - val_loss: 0.8409 - val_accuracy: 0.8475\n",
            "Epoch 926/1000\n",
            "668/668 [==============================] - 0s 91us/step - loss: 0.2651 - accuracy: 0.8952 - val_loss: 0.8138 - val_accuracy: 0.8475\n",
            "Epoch 927/1000\n",
            "668/668 [==============================] - 0s 91us/step - loss: 0.2365 - accuracy: 0.9102 - val_loss: 0.9174 - val_accuracy: 0.8475\n",
            "Epoch 928/1000\n",
            "668/668 [==============================] - 0s 92us/step - loss: 0.2059 - accuracy: 0.9177 - val_loss: 1.0917 - val_accuracy: 0.8475\n",
            "Epoch 929/1000\n",
            "668/668 [==============================] - 0s 92us/step - loss: 0.2407 - accuracy: 0.8982 - val_loss: 1.0651 - val_accuracy: 0.8520\n",
            "Epoch 930/1000\n",
            "668/668 [==============================] - 0s 107us/step - loss: 0.2332 - accuracy: 0.9027 - val_loss: 1.0014 - val_accuracy: 0.8341\n",
            "Epoch 931/1000\n",
            "668/668 [==============================] - 0s 100us/step - loss: 0.2065 - accuracy: 0.9117 - val_loss: 1.0131 - val_accuracy: 0.8475\n",
            "Epoch 932/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.2172 - accuracy: 0.9117 - val_loss: 1.1347 - val_accuracy: 0.8520\n",
            "Epoch 933/1000\n",
            "668/668 [==============================] - 0s 91us/step - loss: 0.2113 - accuracy: 0.9207 - val_loss: 1.2080 - val_accuracy: 0.8565\n",
            "Epoch 934/1000\n",
            "668/668 [==============================] - 0s 91us/step - loss: 0.2187 - accuracy: 0.9117 - val_loss: 1.1956 - val_accuracy: 0.8520\n",
            "Epoch 935/1000\n",
            "668/668 [==============================] - 0s 96us/step - loss: 0.2424 - accuracy: 0.9087 - val_loss: 1.0675 - val_accuracy: 0.8475\n",
            "Epoch 936/1000\n",
            "668/668 [==============================] - 0s 110us/step - loss: 0.2347 - accuracy: 0.9057 - val_loss: 1.0084 - val_accuracy: 0.8430\n",
            "Epoch 937/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.2337 - accuracy: 0.9027 - val_loss: 1.1149 - val_accuracy: 0.8475\n",
            "Epoch 938/1000\n",
            "668/668 [==============================] - 0s 87us/step - loss: 0.2318 - accuracy: 0.9042 - val_loss: 1.1195 - val_accuracy: 0.8475\n",
            "Epoch 939/1000\n",
            "668/668 [==============================] - 0s 86us/step - loss: 0.2057 - accuracy: 0.9222 - val_loss: 1.1504 - val_accuracy: 0.8475\n",
            "Epoch 940/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.1944 - accuracy: 0.9192 - val_loss: 1.1270 - val_accuracy: 0.8475\n",
            "Epoch 941/1000\n",
            "668/668 [==============================] - 0s 92us/step - loss: 0.2330 - accuracy: 0.9102 - val_loss: 1.2190 - val_accuracy: 0.8430\n",
            "Epoch 942/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.2349 - accuracy: 0.8952 - val_loss: 0.9718 - val_accuracy: 0.8386\n",
            "Epoch 943/1000\n",
            "668/668 [==============================] - 0s 86us/step - loss: 0.2272 - accuracy: 0.9027 - val_loss: 0.9785 - val_accuracy: 0.8565\n",
            "Epoch 944/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.2103 - accuracy: 0.9102 - val_loss: 1.1040 - val_accuracy: 0.8341\n",
            "Epoch 945/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.2528 - accuracy: 0.8997 - val_loss: 0.9235 - val_accuracy: 0.8520\n",
            "Epoch 946/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.2453 - accuracy: 0.8892 - val_loss: 0.9251 - val_accuracy: 0.8475\n",
            "Epoch 947/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.2310 - accuracy: 0.9072 - val_loss: 0.9903 - val_accuracy: 0.8386\n",
            "Epoch 948/1000\n",
            "668/668 [==============================] - 0s 91us/step - loss: 0.2263 - accuracy: 0.9072 - val_loss: 1.1734 - val_accuracy: 0.8341\n",
            "Epoch 949/1000\n",
            "668/668 [==============================] - 0s 91us/step - loss: 0.2311 - accuracy: 0.8997 - val_loss: 1.2329 - val_accuracy: 0.8520\n",
            "Epoch 950/1000\n",
            "668/668 [==============================] - 0s 92us/step - loss: 0.2089 - accuracy: 0.9162 - val_loss: 1.1620 - val_accuracy: 0.8520\n",
            "Epoch 951/1000\n",
            "668/668 [==============================] - 0s 99us/step - loss: 0.2093 - accuracy: 0.9207 - val_loss: 1.3763 - val_accuracy: 0.8341\n",
            "Epoch 952/1000\n",
            "668/668 [==============================] - 0s 93us/step - loss: 0.2230 - accuracy: 0.8997 - val_loss: 1.3404 - val_accuracy: 0.8475\n",
            "Epoch 953/1000\n",
            "668/668 [==============================] - 0s 105us/step - loss: 0.2294 - accuracy: 0.9012 - val_loss: 1.0352 - val_accuracy: 0.8520\n",
            "Epoch 954/1000\n",
            "668/668 [==============================] - 0s 91us/step - loss: 0.2195 - accuracy: 0.9042 - val_loss: 1.1290 - val_accuracy: 0.8475\n",
            "Epoch 955/1000\n",
            "668/668 [==============================] - 0s 95us/step - loss: 0.2159 - accuracy: 0.9162 - val_loss: 1.1940 - val_accuracy: 0.8565\n",
            "Epoch 956/1000\n",
            "668/668 [==============================] - 0s 102us/step - loss: 0.2288 - accuracy: 0.9012 - val_loss: 1.1685 - val_accuracy: 0.8520\n",
            "Epoch 957/1000\n",
            "668/668 [==============================] - 0s 93us/step - loss: 0.2238 - accuracy: 0.9027 - val_loss: 1.2874 - val_accuracy: 0.8520\n",
            "Epoch 958/1000\n",
            "668/668 [==============================] - 0s 96us/step - loss: 0.2068 - accuracy: 0.9177 - val_loss: 1.4650 - val_accuracy: 0.8475\n",
            "Epoch 959/1000\n",
            "668/668 [==============================] - 0s 92us/step - loss: 0.2128 - accuracy: 0.9132 - val_loss: 1.4004 - val_accuracy: 0.8475\n",
            "Epoch 960/1000\n",
            "668/668 [==============================] - 0s 91us/step - loss: 0.2244 - accuracy: 0.9132 - val_loss: 1.3432 - val_accuracy: 0.8475\n",
            "Epoch 961/1000\n",
            "668/668 [==============================] - 0s 91us/step - loss: 0.2101 - accuracy: 0.9102 - val_loss: 1.5800 - val_accuracy: 0.8430\n",
            "Epoch 962/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.2594 - accuracy: 0.8967 - val_loss: 0.9404 - val_accuracy: 0.8341\n",
            "Epoch 963/1000\n",
            "668/668 [==============================] - 0s 94us/step - loss: 0.2331 - accuracy: 0.9087 - val_loss: 1.1165 - val_accuracy: 0.8475\n",
            "Epoch 964/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.2299 - accuracy: 0.9042 - val_loss: 1.1938 - val_accuracy: 0.8520\n",
            "Epoch 965/1000\n",
            "668/668 [==============================] - 0s 88us/step - loss: 0.2252 - accuracy: 0.9132 - val_loss: 0.9930 - val_accuracy: 0.8475\n",
            "Epoch 966/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.2371 - accuracy: 0.9072 - val_loss: 1.0639 - val_accuracy: 0.8430\n",
            "Epoch 967/1000\n",
            "668/668 [==============================] - 0s 92us/step - loss: 0.2412 - accuracy: 0.8892 - val_loss: 0.9871 - val_accuracy: 0.8475\n",
            "Epoch 968/1000\n",
            "668/668 [==============================] - 0s 108us/step - loss: 0.2468 - accuracy: 0.8997 - val_loss: 0.9519 - val_accuracy: 0.8430\n",
            "Epoch 969/1000\n",
            "668/668 [==============================] - 0s 99us/step - loss: 0.2291 - accuracy: 0.9027 - val_loss: 1.0966 - val_accuracy: 0.8475\n",
            "Epoch 970/1000\n",
            "668/668 [==============================] - 0s 88us/step - loss: 0.2280 - accuracy: 0.9087 - val_loss: 1.0563 - val_accuracy: 0.8341\n",
            "Epoch 971/1000\n",
            "668/668 [==============================] - 0s 88us/step - loss: 0.2154 - accuracy: 0.9057 - val_loss: 1.2208 - val_accuracy: 0.8475\n",
            "Epoch 972/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.2185 - accuracy: 0.9147 - val_loss: 1.2707 - val_accuracy: 0.8430\n",
            "Epoch 973/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.2296 - accuracy: 0.8997 - val_loss: 1.2744 - val_accuracy: 0.8341\n",
            "Epoch 974/1000\n",
            "668/668 [==============================] - 0s 109us/step - loss: 0.2339 - accuracy: 0.9057 - val_loss: 0.9737 - val_accuracy: 0.8296\n",
            "Epoch 975/1000\n",
            "668/668 [==============================] - 0s 94us/step - loss: 0.2322 - accuracy: 0.9042 - val_loss: 1.0108 - val_accuracy: 0.8251\n",
            "Epoch 976/1000\n",
            "668/668 [==============================] - 0s 91us/step - loss: 0.2347 - accuracy: 0.9072 - val_loss: 1.0248 - val_accuracy: 0.8386\n",
            "Epoch 977/1000\n",
            "668/668 [==============================] - 0s 91us/step - loss: 0.2271 - accuracy: 0.9162 - val_loss: 1.1707 - val_accuracy: 0.8430\n",
            "Epoch 978/1000\n",
            "668/668 [==============================] - 0s 88us/step - loss: 0.2326 - accuracy: 0.9072 - val_loss: 1.1368 - val_accuracy: 0.8475\n",
            "Epoch 979/1000\n",
            "668/668 [==============================] - 0s 93us/step - loss: 0.2168 - accuracy: 0.9147 - val_loss: 1.1644 - val_accuracy: 0.8430\n",
            "Epoch 980/1000\n",
            "668/668 [==============================] - 0s 92us/step - loss: 0.2231 - accuracy: 0.9042 - val_loss: 1.2539 - val_accuracy: 0.8520\n",
            "Epoch 981/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.2102 - accuracy: 0.9192 - val_loss: 1.3234 - val_accuracy: 0.8430\n",
            "Epoch 982/1000\n",
            "668/668 [==============================] - 0s 92us/step - loss: 0.2141 - accuracy: 0.9147 - val_loss: 1.2511 - val_accuracy: 0.8386\n",
            "Epoch 983/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.2369 - accuracy: 0.9087 - val_loss: 1.1682 - val_accuracy: 0.8386\n",
            "Epoch 984/1000\n",
            "668/668 [==============================] - 0s 103us/step - loss: 0.2070 - accuracy: 0.9117 - val_loss: 1.1404 - val_accuracy: 0.8341\n",
            "Epoch 985/1000\n",
            "668/668 [==============================] - 0s 94us/step - loss: 0.2253 - accuracy: 0.9087 - val_loss: 1.2154 - val_accuracy: 0.8475\n",
            "Epoch 986/1000\n",
            "668/668 [==============================] - 0s 96us/step - loss: 0.2228 - accuracy: 0.9072 - val_loss: 1.2037 - val_accuracy: 0.8430\n",
            "Epoch 987/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.2357 - accuracy: 0.8982 - val_loss: 1.3073 - val_accuracy: 0.8251\n",
            "Epoch 988/1000\n",
            "668/668 [==============================] - 0s 86us/step - loss: 0.2120 - accuracy: 0.9132 - val_loss: 1.3599 - val_accuracy: 0.8341\n",
            "Epoch 989/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.2075 - accuracy: 0.9057 - val_loss: 1.3279 - val_accuracy: 0.8386\n",
            "Epoch 990/1000\n",
            "668/668 [==============================] - 0s 86us/step - loss: 0.2182 - accuracy: 0.9027 - val_loss: 1.2283 - val_accuracy: 0.8386\n",
            "Epoch 991/1000\n",
            "668/668 [==============================] - 0s 95us/step - loss: 0.2233 - accuracy: 0.9072 - val_loss: 1.3997 - val_accuracy: 0.8520\n",
            "Epoch 992/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.2248 - accuracy: 0.9072 - val_loss: 1.2826 - val_accuracy: 0.8296\n",
            "Epoch 993/1000\n",
            "668/668 [==============================] - 0s 91us/step - loss: 0.2210 - accuracy: 0.9057 - val_loss: 1.2851 - val_accuracy: 0.8341\n",
            "Epoch 994/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.2234 - accuracy: 0.8982 - val_loss: 1.3191 - val_accuracy: 0.8386\n",
            "Epoch 995/1000\n",
            "668/668 [==============================] - 0s 88us/step - loss: 0.2264 - accuracy: 0.9057 - val_loss: 1.4811 - val_accuracy: 0.8386\n",
            "Epoch 996/1000\n",
            "668/668 [==============================] - 0s 89us/step - loss: 0.2015 - accuracy: 0.9162 - val_loss: 1.5210 - val_accuracy: 0.8341\n",
            "Epoch 997/1000\n",
            "668/668 [==============================] - 0s 92us/step - loss: 0.2295 - accuracy: 0.9012 - val_loss: 1.3737 - val_accuracy: 0.8251\n",
            "Epoch 998/1000\n",
            "668/668 [==============================] - 0s 93us/step - loss: 0.2204 - accuracy: 0.9132 - val_loss: 1.4698 - val_accuracy: 0.8117\n",
            "Epoch 999/1000\n",
            "668/668 [==============================] - 0s 90us/step - loss: 0.2115 - accuracy: 0.9117 - val_loss: 1.7381 - val_accuracy: 0.8296\n",
            "Epoch 1000/1000\n",
            "668/668 [==============================] - 0s 100us/step - loss: 0.2525 - accuracy: 0.9027 - val_loss: 1.3440 - val_accuracy: 0.8341\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fOq32ByzFdlY",
        "outputId": "8557962b-d208-4552-9ad8-45ab6c4e2cb7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 609
        }
      },
      "source": [
        "def graph_draw(model) :\n",
        "    # Accuracy 시각화\n",
        "    plt.plot(model.history['accuracy'])\n",
        "    plt.plot(model.history['val_accuracy'])\n",
        "    plt.title('Model Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend(['train', 'test'], loc = 'upper left')\n",
        "    plt.show()\n",
        "    # Loss 시각화\n",
        "    plt.plot(model.history['loss'])\n",
        "    plt.plot(model.history['val_loss'])\n",
        "    plt.title('Model Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend(['train', 'test'], loc = 'upper left')\n",
        "    plt.show()\n",
        "\n",
        "result = DNN.evaluate(x_testing, y_testing)\n",
        "print(result[1])\n",
        "graph_draw(log_DNN)     # StandardScaler 적용 시각화"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "223/223 [==============================] - 0s 43us/step\n",
            "0.834080696105957\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "652L9zlS6eva",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a7377e5c-1b5d-4f63-b096-0547d86e7534"
      },
      "source": [
        ""
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1],\n",
              "       [0],\n",
              "       [1],\n",
              "       [1],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [1],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [1],\n",
              "       [1],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [1],\n",
              "       [1],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [1],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [1],\n",
              "       [0],\n",
              "       [0],\n",
              "       [1],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [1],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [1],\n",
              "       [0],\n",
              "       [0],\n",
              "       [1],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [1],\n",
              "       [0],\n",
              "       [1],\n",
              "       [0],\n",
              "       [0],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [1],\n",
              "       [1],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [1],\n",
              "       [0],\n",
              "       [1],\n",
              "       [1],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [1],\n",
              "       [0],\n",
              "       [1],\n",
              "       [0],\n",
              "       [0],\n",
              "       [1],\n",
              "       [0],\n",
              "       [0],\n",
              "       [1],\n",
              "       [1],\n",
              "       [0],\n",
              "       [1],\n",
              "       [1],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [1],\n",
              "       [1],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [1],\n",
              "       [0],\n",
              "       [0],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [1],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [1],\n",
              "       [0],\n",
              "       [0],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [0],\n",
              "       [0],\n",
              "       [1],\n",
              "       [1],\n",
              "       [0],\n",
              "       [1],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [1],\n",
              "       [1],\n",
              "       [0],\n",
              "       [1],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [1],\n",
              "       [0],\n",
              "       [0],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [1],\n",
              "       [1],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [1],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [1],\n",
              "       [1],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [1],\n",
              "       [0],\n",
              "       [0],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [0],\n",
              "       [1],\n",
              "       [0],\n",
              "       [0],\n",
              "       [1],\n",
              "       [0],\n",
              "       [0],\n",
              "       [1],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [1],\n",
              "       [1],\n",
              "       [0],\n",
              "       [1],\n",
              "       [0],\n",
              "       [1],\n",
              "       [0],\n",
              "       [1],\n",
              "       [1],\n",
              "       [0],\n",
              "       [0],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [1],\n",
              "       [0],\n",
              "       [0],\n",
              "       [1],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NUGDHdDu-zT7",
        "outputId": "40469234-8871-411e-9fa5-710c0f1814bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# 4. Random Forest model\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "def RF_create_prepare(x, y) :\n",
        "    Forest = RandomForestClassifier(n_estimators = 650, min_samples_split = 10, min_samples_leaf = 5)\n",
        "    Forest.fit(x, y)\n",
        "    return Forest\n",
        "\n",
        "RF_Model = RF_create_prepare(x_training, y_training)\n",
        "RF_test = RF_Model.predict(x_testing)\n",
        "print(\"Random Forest score\\t: \", (RF_test == y_testing).sum() / len(y_testing))"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Random Forest score\t:  0.8385650224215246\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IsOUg4meVwP1",
        "outputId": "dce90861-c6e5-48e1-b2ac-034fa9011d68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Logistic Regression 예측\n",
        "LogReg = LogisticRegression().fit(x_train_scale, y_train)\n",
        "Log_predict = LogReg.predict(x_test_scale)\n",
        "\n",
        "# SVM 예측\n",
        "SVM_Linear = SVC(kernel = 'linear', C = 1.0)\n",
        "SVM_Linear.fit(x_train_scale, y_train)\n",
        "SVM_L_predict = SVM_Linear.predict(x_test_scale)\n",
        "\n",
        "SVM_Nonlinear = SVC(kernel = 'rbf', C = 1.0, gamma = 10.0)\n",
        "SVM_Nonlinear.fit(x_train_scale, y_train)\n",
        "SVM_NL_predict = SVM_Nonlinear.predict(x_test_scale)\n",
        "\n",
        "# DNN 예측\n",
        "DNN = DNN_create()\n",
        "DNN.fit(x_train_scale, y_train, epochs = 1000, batch_size = 128)\n",
        "DNN_predict = DNN.predict_classes(x_test_scale)\n",
        "\n",
        "# Random Forest 예측\n",
        "RF_Model = RF_create_prepare(x_train_scale, y_train)\n",
        "Forest_predict = RF_Model.predict(x_test_scale)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "891/891 [==============================] - 0s 479us/step - loss: 0.6925 - accuracy: 0.5376\n",
            "Epoch 2/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.6900 - accuracy: 0.5825\n",
            "Epoch 3/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.6821 - accuracy: 0.6263\n",
            "Epoch 4/1000\n",
            "891/891 [==============================] - 0s 34us/step - loss: 0.6773 - accuracy: 0.6150\n",
            "Epoch 5/1000\n",
            "891/891 [==============================] - 0s 36us/step - loss: 0.6687 - accuracy: 0.6352\n",
            "Epoch 6/1000\n",
            "891/891 [==============================] - 0s 36us/step - loss: 0.6612 - accuracy: 0.6285\n",
            "Epoch 7/1000\n",
            "891/891 [==============================] - 0s 38us/step - loss: 0.6630 - accuracy: 0.6296\n",
            "Epoch 8/1000\n",
            "891/891 [==============================] - 0s 34us/step - loss: 0.6438 - accuracy: 0.6453\n",
            "Epoch 9/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.6243 - accuracy: 0.6745\n",
            "Epoch 10/1000\n",
            "891/891 [==============================] - 0s 34us/step - loss: 0.6225 - accuracy: 0.6790\n",
            "Epoch 11/1000\n",
            "891/891 [==============================] - 0s 36us/step - loss: 0.5940 - accuracy: 0.6970\n",
            "Epoch 12/1000\n",
            "891/891 [==============================] - 0s 34us/step - loss: 0.5864 - accuracy: 0.7071\n",
            "Epoch 13/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.5608 - accuracy: 0.7385\n",
            "Epoch 14/1000\n",
            "891/891 [==============================] - 0s 39us/step - loss: 0.5623 - accuracy: 0.7385\n",
            "Epoch 15/1000\n",
            "891/891 [==============================] - 0s 35us/step - loss: 0.5379 - accuracy: 0.7508\n",
            "Epoch 16/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.5441 - accuracy: 0.7340\n",
            "Epoch 17/1000\n",
            "891/891 [==============================] - 0s 35us/step - loss: 0.5008 - accuracy: 0.7677\n",
            "Epoch 18/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.5042 - accuracy: 0.7565\n",
            "Epoch 19/1000\n",
            "891/891 [==============================] - 0s 55us/step - loss: 0.5023 - accuracy: 0.7666\n",
            "Epoch 20/1000\n",
            "891/891 [==============================] - 0s 35us/step - loss: 0.5172 - accuracy: 0.7677\n",
            "Epoch 21/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.5016 - accuracy: 0.7778\n",
            "Epoch 22/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.5012 - accuracy: 0.7710\n",
            "Epoch 23/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.4776 - accuracy: 0.7912\n",
            "Epoch 24/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.4911 - accuracy: 0.7767\n",
            "Epoch 25/1000\n",
            "891/891 [==============================] - 0s 41us/step - loss: 0.4859 - accuracy: 0.8047\n",
            "Epoch 26/1000\n",
            "891/891 [==============================] - 0s 43us/step - loss: 0.4801 - accuracy: 0.7980\n",
            "Epoch 27/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.4798 - accuracy: 0.8081\n",
            "Epoch 28/1000\n",
            "891/891 [==============================] - 0s 35us/step - loss: 0.4750 - accuracy: 0.7935\n",
            "Epoch 29/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.4789 - accuracy: 0.7946\n",
            "Epoch 30/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.4649 - accuracy: 0.7901\n",
            "Epoch 31/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.4461 - accuracy: 0.7856\n",
            "Epoch 32/1000\n",
            "891/891 [==============================] - 0s 36us/step - loss: 0.4619 - accuracy: 0.7901\n",
            "Epoch 33/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.4716 - accuracy: 0.7991\n",
            "Epoch 34/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.4656 - accuracy: 0.7969\n",
            "Epoch 35/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.4589 - accuracy: 0.8126\n",
            "Epoch 36/1000\n",
            "891/891 [==============================] - 0s 34us/step - loss: 0.4541 - accuracy: 0.8092\n",
            "Epoch 37/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.4442 - accuracy: 0.8092\n",
            "Epoch 38/1000\n",
            "891/891 [==============================] - 0s 30us/step - loss: 0.4348 - accuracy: 0.8193\n",
            "Epoch 39/1000\n",
            "891/891 [==============================] - 0s 30us/step - loss: 0.4233 - accuracy: 0.8238\n",
            "Epoch 40/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.4563 - accuracy: 0.8171\n",
            "Epoch 41/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.4477 - accuracy: 0.8215\n",
            "Epoch 42/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.4495 - accuracy: 0.8159\n",
            "Epoch 43/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.4506 - accuracy: 0.8070\n",
            "Epoch 44/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.4379 - accuracy: 0.8272\n",
            "Epoch 45/1000\n",
            "891/891 [==============================] - 0s 39us/step - loss: 0.4386 - accuracy: 0.8294\n",
            "Epoch 46/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.4385 - accuracy: 0.8159\n",
            "Epoch 47/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.4263 - accuracy: 0.8171\n",
            "Epoch 48/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.4346 - accuracy: 0.8215\n",
            "Epoch 49/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.4284 - accuracy: 0.8204\n",
            "Epoch 50/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.4260 - accuracy: 0.8215\n",
            "Epoch 51/1000\n",
            "891/891 [==============================] - 0s 38us/step - loss: 0.4256 - accuracy: 0.8227\n",
            "Epoch 52/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.4284 - accuracy: 0.8238\n",
            "Epoch 53/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.4259 - accuracy: 0.8238\n",
            "Epoch 54/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.4259 - accuracy: 0.8204\n",
            "Epoch 55/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.4148 - accuracy: 0.8418\n",
            "Epoch 56/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.4094 - accuracy: 0.8339\n",
            "Epoch 57/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.4273 - accuracy: 0.8227\n",
            "Epoch 58/1000\n",
            "891/891 [==============================] - 0s 35us/step - loss: 0.4152 - accuracy: 0.8373\n",
            "Epoch 59/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.4257 - accuracy: 0.8316\n",
            "Epoch 60/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.4215 - accuracy: 0.8305\n",
            "Epoch 61/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.4033 - accuracy: 0.8350\n",
            "Epoch 62/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.4109 - accuracy: 0.8339\n",
            "Epoch 63/1000\n",
            "891/891 [==============================] - 0s 30us/step - loss: 0.4125 - accuracy: 0.8305\n",
            "Epoch 64/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.4208 - accuracy: 0.8294\n",
            "Epoch 65/1000\n",
            "891/891 [==============================] - 0s 30us/step - loss: 0.4114 - accuracy: 0.8328\n",
            "Epoch 66/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.4052 - accuracy: 0.8339\n",
            "Epoch 67/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.3862 - accuracy: 0.8418\n",
            "Epoch 68/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.4041 - accuracy: 0.8451\n",
            "Epoch 69/1000\n",
            "891/891 [==============================] - 0s 34us/step - loss: 0.4192 - accuracy: 0.8339\n",
            "Epoch 70/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.4165 - accuracy: 0.8272\n",
            "Epoch 71/1000\n",
            "891/891 [==============================] - 0s 34us/step - loss: 0.4041 - accuracy: 0.8283\n",
            "Epoch 72/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.3874 - accuracy: 0.8440\n",
            "Epoch 73/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.4087 - accuracy: 0.8339\n",
            "Epoch 74/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.4057 - accuracy: 0.8406\n",
            "Epoch 75/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.3972 - accuracy: 0.8328\n",
            "Epoch 76/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.3951 - accuracy: 0.8406\n",
            "Epoch 77/1000\n",
            "891/891 [==============================] - 0s 30us/step - loss: 0.3940 - accuracy: 0.8384\n",
            "Epoch 78/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.3936 - accuracy: 0.8474\n",
            "Epoch 79/1000\n",
            "891/891 [==============================] - 0s 35us/step - loss: 0.3971 - accuracy: 0.8440\n",
            "Epoch 80/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.3998 - accuracy: 0.8350\n",
            "Epoch 81/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.4015 - accuracy: 0.8373\n",
            "Epoch 82/1000\n",
            "891/891 [==============================] - 0s 34us/step - loss: 0.3913 - accuracy: 0.8361\n",
            "Epoch 83/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.3995 - accuracy: 0.8429\n",
            "Epoch 84/1000\n",
            "891/891 [==============================] - 0s 45us/step - loss: 0.3965 - accuracy: 0.8384\n",
            "Epoch 85/1000\n",
            "891/891 [==============================] - 0s 34us/step - loss: 0.4115 - accuracy: 0.8316\n",
            "Epoch 86/1000\n",
            "891/891 [==============================] - 0s 35us/step - loss: 0.3914 - accuracy: 0.8395\n",
            "Epoch 87/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.3860 - accuracy: 0.8451\n",
            "Epoch 88/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.3918 - accuracy: 0.8440\n",
            "Epoch 89/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.3836 - accuracy: 0.8507\n",
            "Epoch 90/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.3860 - accuracy: 0.8418\n",
            "Epoch 91/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.3929 - accuracy: 0.8418\n",
            "Epoch 92/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.3971 - accuracy: 0.8429\n",
            "Epoch 93/1000\n",
            "891/891 [==============================] - 0s 36us/step - loss: 0.3970 - accuracy: 0.8429\n",
            "Epoch 94/1000\n",
            "891/891 [==============================] - 0s 34us/step - loss: 0.3883 - accuracy: 0.8474\n",
            "Epoch 95/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.3716 - accuracy: 0.8462\n",
            "Epoch 96/1000\n",
            "891/891 [==============================] - 0s 30us/step - loss: 0.3774 - accuracy: 0.8608\n",
            "Epoch 97/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.3821 - accuracy: 0.8429\n",
            "Epoch 98/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.3882 - accuracy: 0.8440\n",
            "Epoch 99/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.3813 - accuracy: 0.8552\n",
            "Epoch 100/1000\n",
            "891/891 [==============================] - 0s 30us/step - loss: 0.3856 - accuracy: 0.8485\n",
            "Epoch 101/1000\n",
            "891/891 [==============================] - 0s 30us/step - loss: 0.3767 - accuracy: 0.8429\n",
            "Epoch 102/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.3883 - accuracy: 0.8462\n",
            "Epoch 103/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.3741 - accuracy: 0.8440\n",
            "Epoch 104/1000\n",
            "891/891 [==============================] - 0s 29us/step - loss: 0.3779 - accuracy: 0.8474\n",
            "Epoch 105/1000\n",
            "891/891 [==============================] - 0s 30us/step - loss: 0.3752 - accuracy: 0.8474\n",
            "Epoch 106/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.3810 - accuracy: 0.8507\n",
            "Epoch 107/1000\n",
            "891/891 [==============================] - 0s 36us/step - loss: 0.3702 - accuracy: 0.8541\n",
            "Epoch 108/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.3860 - accuracy: 0.8350\n",
            "Epoch 109/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.3769 - accuracy: 0.8496\n",
            "Epoch 110/1000\n",
            "891/891 [==============================] - 0s 30us/step - loss: 0.3683 - accuracy: 0.8608\n",
            "Epoch 111/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.3722 - accuracy: 0.8507\n",
            "Epoch 112/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.3846 - accuracy: 0.8474\n",
            "Epoch 113/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.3873 - accuracy: 0.8530\n",
            "Epoch 114/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.3714 - accuracy: 0.8552\n",
            "Epoch 115/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.3728 - accuracy: 0.8541\n",
            "Epoch 116/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.3889 - accuracy: 0.8507\n",
            "Epoch 117/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.3682 - accuracy: 0.8541\n",
            "Epoch 118/1000\n",
            "891/891 [==============================] - 0s 38us/step - loss: 0.3722 - accuracy: 0.8451\n",
            "Epoch 119/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.3756 - accuracy: 0.8496\n",
            "Epoch 120/1000\n",
            "891/891 [==============================] - 0s 37us/step - loss: 0.3643 - accuracy: 0.8552\n",
            "Epoch 121/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.3777 - accuracy: 0.8496\n",
            "Epoch 122/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.3676 - accuracy: 0.8485\n",
            "Epoch 123/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.3710 - accuracy: 0.8597\n",
            "Epoch 124/1000\n",
            "891/891 [==============================] - 0s 30us/step - loss: 0.3568 - accuracy: 0.8563\n",
            "Epoch 125/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.3619 - accuracy: 0.8462\n",
            "Epoch 126/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.3926 - accuracy: 0.8418\n",
            "Epoch 127/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.3696 - accuracy: 0.8653\n",
            "Epoch 128/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.3836 - accuracy: 0.8395\n",
            "Epoch 129/1000\n",
            "891/891 [==============================] - 0s 30us/step - loss: 0.3882 - accuracy: 0.8507\n",
            "Epoch 130/1000\n",
            "891/891 [==============================] - 0s 30us/step - loss: 0.3633 - accuracy: 0.8519\n",
            "Epoch 131/1000\n",
            "891/891 [==============================] - 0s 30us/step - loss: 0.3676 - accuracy: 0.8507\n",
            "Epoch 132/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.3752 - accuracy: 0.8451\n",
            "Epoch 133/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.3702 - accuracy: 0.8586\n",
            "Epoch 134/1000\n",
            "891/891 [==============================] - 0s 35us/step - loss: 0.3699 - accuracy: 0.8586\n",
            "Epoch 135/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.3646 - accuracy: 0.8507\n",
            "Epoch 136/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.3828 - accuracy: 0.8530\n",
            "Epoch 137/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.3677 - accuracy: 0.8530\n",
            "Epoch 138/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.3599 - accuracy: 0.8519\n",
            "Epoch 139/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.3611 - accuracy: 0.8451\n",
            "Epoch 140/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.3726 - accuracy: 0.8642\n",
            "Epoch 141/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.3691 - accuracy: 0.8519\n",
            "Epoch 142/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.3553 - accuracy: 0.8541\n",
            "Epoch 143/1000\n",
            "891/891 [==============================] - 0s 34us/step - loss: 0.3681 - accuracy: 0.8597\n",
            "Epoch 144/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.3665 - accuracy: 0.8474\n",
            "Epoch 145/1000\n",
            "891/891 [==============================] - 0s 34us/step - loss: 0.3638 - accuracy: 0.8519\n",
            "Epoch 146/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.3683 - accuracy: 0.8530\n",
            "Epoch 147/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.3649 - accuracy: 0.8597\n",
            "Epoch 148/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.3580 - accuracy: 0.8586\n",
            "Epoch 149/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.3614 - accuracy: 0.8552\n",
            "Epoch 150/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.3526 - accuracy: 0.8519\n",
            "Epoch 151/1000\n",
            "891/891 [==============================] - 0s 38us/step - loss: 0.3569 - accuracy: 0.8507\n",
            "Epoch 152/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.3646 - accuracy: 0.8575\n",
            "Epoch 153/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.3507 - accuracy: 0.8530\n",
            "Epoch 154/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.3604 - accuracy: 0.8519\n",
            "Epoch 155/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.3518 - accuracy: 0.8496\n",
            "Epoch 156/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.3610 - accuracy: 0.8597\n",
            "Epoch 157/1000\n",
            "891/891 [==============================] - 0s 30us/step - loss: 0.3628 - accuracy: 0.8496\n",
            "Epoch 158/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.3556 - accuracy: 0.8563\n",
            "Epoch 159/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.3620 - accuracy: 0.8586\n",
            "Epoch 160/1000\n",
            "891/891 [==============================] - 0s 35us/step - loss: 0.3681 - accuracy: 0.8620\n",
            "Epoch 161/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.3384 - accuracy: 0.8664\n",
            "Epoch 162/1000\n",
            "891/891 [==============================] - 0s 35us/step - loss: 0.3393 - accuracy: 0.8732\n",
            "Epoch 163/1000\n",
            "891/891 [==============================] - 0s 46us/step - loss: 0.3532 - accuracy: 0.8631\n",
            "Epoch 164/1000\n",
            "891/891 [==============================] - 0s 42us/step - loss: 0.3445 - accuracy: 0.8519\n",
            "Epoch 165/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.3614 - accuracy: 0.8552\n",
            "Epoch 166/1000\n",
            "891/891 [==============================] - 0s 35us/step - loss: 0.3630 - accuracy: 0.8530\n",
            "Epoch 167/1000\n",
            "891/891 [==============================] - 0s 30us/step - loss: 0.3493 - accuracy: 0.8631\n",
            "Epoch 168/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.3429 - accuracy: 0.8698\n",
            "Epoch 169/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.3536 - accuracy: 0.8597\n",
            "Epoch 170/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.3503 - accuracy: 0.8642\n",
            "Epoch 171/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.3523 - accuracy: 0.8653\n",
            "Epoch 172/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.3598 - accuracy: 0.8462\n",
            "Epoch 173/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.3528 - accuracy: 0.8552\n",
            "Epoch 174/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.3391 - accuracy: 0.8676\n",
            "Epoch 175/1000\n",
            "891/891 [==============================] - 0s 35us/step - loss: 0.3338 - accuracy: 0.8664\n",
            "Epoch 176/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.3420 - accuracy: 0.8732\n",
            "Epoch 177/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.3433 - accuracy: 0.8608\n",
            "Epoch 178/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.3511 - accuracy: 0.8586\n",
            "Epoch 179/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.3374 - accuracy: 0.8687\n",
            "Epoch 180/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.3421 - accuracy: 0.8709\n",
            "Epoch 181/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.3655 - accuracy: 0.8608\n",
            "Epoch 182/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.3463 - accuracy: 0.8575\n",
            "Epoch 183/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.3603 - accuracy: 0.8631\n",
            "Epoch 184/1000\n",
            "891/891 [==============================] - 0s 38us/step - loss: 0.3388 - accuracy: 0.8631\n",
            "Epoch 185/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.3509 - accuracy: 0.8530\n",
            "Epoch 186/1000\n",
            "891/891 [==============================] - 0s 30us/step - loss: 0.3429 - accuracy: 0.8586\n",
            "Epoch 187/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.3597 - accuracy: 0.8552\n",
            "Epoch 188/1000\n",
            "891/891 [==============================] - 0s 30us/step - loss: 0.3373 - accuracy: 0.8631\n",
            "Epoch 189/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.3643 - accuracy: 0.8608\n",
            "Epoch 190/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.3408 - accuracy: 0.8743\n",
            "Epoch 191/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.3514 - accuracy: 0.8653\n",
            "Epoch 192/1000\n",
            "891/891 [==============================] - 0s 35us/step - loss: 0.3445 - accuracy: 0.8631\n",
            "Epoch 193/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.3491 - accuracy: 0.8608\n",
            "Epoch 194/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.3562 - accuracy: 0.8575\n",
            "Epoch 195/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.3532 - accuracy: 0.8507\n",
            "Epoch 196/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.3359 - accuracy: 0.8709\n",
            "Epoch 197/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.3480 - accuracy: 0.8642\n",
            "Epoch 198/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.3329 - accuracy: 0.8788\n",
            "Epoch 199/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.3520 - accuracy: 0.8620\n",
            "Epoch 200/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.3577 - accuracy: 0.8597\n",
            "Epoch 201/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.3418 - accuracy: 0.8698\n",
            "Epoch 202/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.3255 - accuracy: 0.8754\n",
            "Epoch 203/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.3350 - accuracy: 0.8709\n",
            "Epoch 204/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.3466 - accuracy: 0.8687\n",
            "Epoch 205/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.3538 - accuracy: 0.8620\n",
            "Epoch 206/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.3359 - accuracy: 0.8664\n",
            "Epoch 207/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.3310 - accuracy: 0.8721\n",
            "Epoch 208/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.3392 - accuracy: 0.8732\n",
            "Epoch 209/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.3522 - accuracy: 0.8597\n",
            "Epoch 210/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.3405 - accuracy: 0.8631\n",
            "Epoch 211/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.3443 - accuracy: 0.8687\n",
            "Epoch 212/1000\n",
            "891/891 [==============================] - 0s 38us/step - loss: 0.3379 - accuracy: 0.8631\n",
            "Epoch 213/1000\n",
            "891/891 [==============================] - 0s 34us/step - loss: 0.3445 - accuracy: 0.8687\n",
            "Epoch 214/1000\n",
            "891/891 [==============================] - 0s 34us/step - loss: 0.3275 - accuracy: 0.8732\n",
            "Epoch 215/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.3458 - accuracy: 0.8597\n",
            "Epoch 216/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.3344 - accuracy: 0.8709\n",
            "Epoch 217/1000\n",
            "891/891 [==============================] - 0s 40us/step - loss: 0.3209 - accuracy: 0.8698\n",
            "Epoch 218/1000\n",
            "891/891 [==============================] - 0s 36us/step - loss: 0.3424 - accuracy: 0.8653\n",
            "Epoch 219/1000\n",
            "891/891 [==============================] - 0s 34us/step - loss: 0.3506 - accuracy: 0.8575\n",
            "Epoch 220/1000\n",
            "891/891 [==============================] - 0s 34us/step - loss: 0.3398 - accuracy: 0.8653\n",
            "Epoch 221/1000\n",
            "891/891 [==============================] - 0s 35us/step - loss: 0.3438 - accuracy: 0.8631\n",
            "Epoch 222/1000\n",
            "891/891 [==============================] - 0s 35us/step - loss: 0.3371 - accuracy: 0.8642\n",
            "Epoch 223/1000\n",
            "891/891 [==============================] - 0s 34us/step - loss: 0.3257 - accuracy: 0.8687\n",
            "Epoch 224/1000\n",
            "891/891 [==============================] - 0s 35us/step - loss: 0.3399 - accuracy: 0.8721\n",
            "Epoch 225/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.3308 - accuracy: 0.8653\n",
            "Epoch 226/1000\n",
            "891/891 [==============================] - 0s 35us/step - loss: 0.3277 - accuracy: 0.8754\n",
            "Epoch 227/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.3491 - accuracy: 0.8709\n",
            "Epoch 228/1000\n",
            "891/891 [==============================] - 0s 38us/step - loss: 0.3227 - accuracy: 0.8799\n",
            "Epoch 229/1000\n",
            "891/891 [==============================] - 0s 34us/step - loss: 0.3312 - accuracy: 0.8698\n",
            "Epoch 230/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.3396 - accuracy: 0.8631\n",
            "Epoch 231/1000\n",
            "891/891 [==============================] - 0s 37us/step - loss: 0.3469 - accuracy: 0.8687\n",
            "Epoch 232/1000\n",
            "891/891 [==============================] - 0s 34us/step - loss: 0.3196 - accuracy: 0.8788\n",
            "Epoch 233/1000\n",
            "891/891 [==============================] - 0s 34us/step - loss: 0.3319 - accuracy: 0.8698\n",
            "Epoch 234/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.3082 - accuracy: 0.8765\n",
            "Epoch 235/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.3269 - accuracy: 0.8653\n",
            "Epoch 236/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.3216 - accuracy: 0.8754\n",
            "Epoch 237/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.3276 - accuracy: 0.8743\n",
            "Epoch 238/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.3293 - accuracy: 0.8788\n",
            "Epoch 239/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.3360 - accuracy: 0.8687\n",
            "Epoch 240/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.3319 - accuracy: 0.8709\n",
            "Epoch 241/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.3319 - accuracy: 0.8653\n",
            "Epoch 242/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.3267 - accuracy: 0.8676\n",
            "Epoch 243/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.3313 - accuracy: 0.8732\n",
            "Epoch 244/1000\n",
            "891/891 [==============================] - 0s 38us/step - loss: 0.3392 - accuracy: 0.8676\n",
            "Epoch 245/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.3396 - accuracy: 0.8698\n",
            "Epoch 246/1000\n",
            "891/891 [==============================] - 0s 34us/step - loss: 0.3173 - accuracy: 0.8799\n",
            "Epoch 247/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.3371 - accuracy: 0.8687\n",
            "Epoch 248/1000\n",
            "891/891 [==============================] - 0s 30us/step - loss: 0.3155 - accuracy: 0.8788\n",
            "Epoch 249/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.3421 - accuracy: 0.8653\n",
            "Epoch 250/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.3301 - accuracy: 0.8653\n",
            "Epoch 251/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.3231 - accuracy: 0.8732\n",
            "Epoch 252/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.3156 - accuracy: 0.8709\n",
            "Epoch 253/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.3129 - accuracy: 0.8687\n",
            "Epoch 254/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.3185 - accuracy: 0.8721\n",
            "Epoch 255/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.3265 - accuracy: 0.8586\n",
            "Epoch 256/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.3311 - accuracy: 0.8676\n",
            "Epoch 257/1000\n",
            "891/891 [==============================] - 0s 35us/step - loss: 0.3271 - accuracy: 0.8676\n",
            "Epoch 258/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.3121 - accuracy: 0.8676\n",
            "Epoch 259/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.3180 - accuracy: 0.8676\n",
            "Epoch 260/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.3297 - accuracy: 0.8743\n",
            "Epoch 261/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.3340 - accuracy: 0.8754\n",
            "Epoch 262/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.3183 - accuracy: 0.8777\n",
            "Epoch 263/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.3201 - accuracy: 0.8676\n",
            "Epoch 264/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.3461 - accuracy: 0.8642\n",
            "Epoch 265/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.3205 - accuracy: 0.8743\n",
            "Epoch 266/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.3231 - accuracy: 0.8676\n",
            "Epoch 267/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.3265 - accuracy: 0.8754\n",
            "Epoch 268/1000\n",
            "891/891 [==============================] - 0s 30us/step - loss: 0.3341 - accuracy: 0.8687\n",
            "Epoch 269/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.3320 - accuracy: 0.8653\n",
            "Epoch 270/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.3267 - accuracy: 0.8597\n",
            "Epoch 271/1000\n",
            "891/891 [==============================] - 0s 30us/step - loss: 0.3063 - accuracy: 0.8822\n",
            "Epoch 272/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.3347 - accuracy: 0.8709\n",
            "Epoch 273/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.3199 - accuracy: 0.8799\n",
            "Epoch 274/1000\n",
            "891/891 [==============================] - 0s 34us/step - loss: 0.3212 - accuracy: 0.8653\n",
            "Epoch 275/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.3194 - accuracy: 0.8810\n",
            "Epoch 276/1000\n",
            "891/891 [==============================] - 0s 30us/step - loss: 0.3099 - accuracy: 0.8822\n",
            "Epoch 277/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.3075 - accuracy: 0.8810\n",
            "Epoch 278/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.3208 - accuracy: 0.8721\n",
            "Epoch 279/1000\n",
            "891/891 [==============================] - 0s 34us/step - loss: 0.3134 - accuracy: 0.8743\n",
            "Epoch 280/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.3218 - accuracy: 0.8653\n",
            "Epoch 281/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.3114 - accuracy: 0.8799\n",
            "Epoch 282/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.3182 - accuracy: 0.8799\n",
            "Epoch 283/1000\n",
            "891/891 [==============================] - 0s 36us/step - loss: 0.3137 - accuracy: 0.8664\n",
            "Epoch 284/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.3090 - accuracy: 0.8754\n",
            "Epoch 285/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.3116 - accuracy: 0.8732\n",
            "Epoch 286/1000\n",
            "891/891 [==============================] - 0s 30us/step - loss: 0.3090 - accuracy: 0.8810\n",
            "Epoch 287/1000\n",
            "891/891 [==============================] - 0s 30us/step - loss: 0.3207 - accuracy: 0.8788\n",
            "Epoch 288/1000\n",
            "891/891 [==============================] - 0s 30us/step - loss: 0.3260 - accuracy: 0.8721\n",
            "Epoch 289/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.3237 - accuracy: 0.8664\n",
            "Epoch 290/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.3434 - accuracy: 0.8631\n",
            "Epoch 291/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.3216 - accuracy: 0.8732\n",
            "Epoch 292/1000\n",
            "891/891 [==============================] - 0s 34us/step - loss: 0.3239 - accuracy: 0.8676\n",
            "Epoch 293/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.3158 - accuracy: 0.8664\n",
            "Epoch 294/1000\n",
            "891/891 [==============================] - 0s 34us/step - loss: 0.3125 - accuracy: 0.8788\n",
            "Epoch 295/1000\n",
            "891/891 [==============================] - 0s 34us/step - loss: 0.3275 - accuracy: 0.8810\n",
            "Epoch 296/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.3092 - accuracy: 0.8833\n",
            "Epoch 297/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.3277 - accuracy: 0.8743\n",
            "Epoch 298/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.3099 - accuracy: 0.8844\n",
            "Epoch 299/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.3281 - accuracy: 0.8676\n",
            "Epoch 300/1000\n",
            "891/891 [==============================] - 0s 30us/step - loss: 0.3138 - accuracy: 0.8687\n",
            "Epoch 301/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.3219 - accuracy: 0.8687\n",
            "Epoch 302/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.3112 - accuracy: 0.8765\n",
            "Epoch 303/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.3073 - accuracy: 0.8754\n",
            "Epoch 304/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.3132 - accuracy: 0.8698\n",
            "Epoch 305/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.3140 - accuracy: 0.8777\n",
            "Epoch 306/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.3015 - accuracy: 0.8799\n",
            "Epoch 307/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.3080 - accuracy: 0.8866\n",
            "Epoch 308/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.3128 - accuracy: 0.8844\n",
            "Epoch 309/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.3172 - accuracy: 0.8743\n",
            "Epoch 310/1000\n",
            "891/891 [==============================] - 0s 30us/step - loss: 0.3026 - accuracy: 0.8698\n",
            "Epoch 311/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.3061 - accuracy: 0.8833\n",
            "Epoch 312/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.3125 - accuracy: 0.8833\n",
            "Epoch 313/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.3098 - accuracy: 0.8822\n",
            "Epoch 314/1000\n",
            "891/891 [==============================] - 0s 30us/step - loss: 0.3000 - accuracy: 0.8721\n",
            "Epoch 315/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2957 - accuracy: 0.8765\n",
            "Epoch 316/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.3073 - accuracy: 0.8822\n",
            "Epoch 317/1000\n",
            "891/891 [==============================] - 0s 41us/step - loss: 0.3148 - accuracy: 0.8743\n",
            "Epoch 318/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.3112 - accuracy: 0.8788\n",
            "Epoch 319/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.3133 - accuracy: 0.8777\n",
            "Epoch 320/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.3001 - accuracy: 0.8810\n",
            "Epoch 321/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.3187 - accuracy: 0.8822\n",
            "Epoch 322/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.3104 - accuracy: 0.8687\n",
            "Epoch 323/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.3246 - accuracy: 0.8743\n",
            "Epoch 324/1000\n",
            "891/891 [==============================] - 0s 34us/step - loss: 0.3288 - accuracy: 0.8631\n",
            "Epoch 325/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.3203 - accuracy: 0.8900\n",
            "Epoch 326/1000\n",
            "891/891 [==============================] - 0s 34us/step - loss: 0.3003 - accuracy: 0.8844\n",
            "Epoch 327/1000\n",
            "891/891 [==============================] - 0s 35us/step - loss: 0.3050 - accuracy: 0.8743\n",
            "Epoch 328/1000\n",
            "891/891 [==============================] - 0s 40us/step - loss: 0.3042 - accuracy: 0.8698\n",
            "Epoch 329/1000\n",
            "891/891 [==============================] - 0s 37us/step - loss: 0.3213 - accuracy: 0.8687\n",
            "Epoch 330/1000\n",
            "891/891 [==============================] - 0s 37us/step - loss: 0.3054 - accuracy: 0.8765\n",
            "Epoch 331/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.3206 - accuracy: 0.8743\n",
            "Epoch 332/1000\n",
            "891/891 [==============================] - 0s 30us/step - loss: 0.2987 - accuracy: 0.8810\n",
            "Epoch 333/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2953 - accuracy: 0.8810\n",
            "Epoch 334/1000\n",
            "891/891 [==============================] - 0s 34us/step - loss: 0.3079 - accuracy: 0.8799\n",
            "Epoch 335/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.3013 - accuracy: 0.8788\n",
            "Epoch 336/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.3208 - accuracy: 0.8653\n",
            "Epoch 337/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.3201 - accuracy: 0.8721\n",
            "Epoch 338/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.3026 - accuracy: 0.8777\n",
            "Epoch 339/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.3148 - accuracy: 0.8833\n",
            "Epoch 340/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.3149 - accuracy: 0.8866\n",
            "Epoch 341/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.3133 - accuracy: 0.8743\n",
            "Epoch 342/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.3081 - accuracy: 0.8855\n",
            "Epoch 343/1000\n",
            "891/891 [==============================] - 0s 36us/step - loss: 0.3058 - accuracy: 0.8788\n",
            "Epoch 344/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.2971 - accuracy: 0.8833\n",
            "Epoch 345/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2978 - accuracy: 0.8833\n",
            "Epoch 346/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.3189 - accuracy: 0.8810\n",
            "Epoch 347/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.3226 - accuracy: 0.8709\n",
            "Epoch 348/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.3112 - accuracy: 0.8799\n",
            "Epoch 349/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.3074 - accuracy: 0.8765\n",
            "Epoch 350/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.3018 - accuracy: 0.8799\n",
            "Epoch 351/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.3118 - accuracy: 0.8788\n",
            "Epoch 352/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.3142 - accuracy: 0.8732\n",
            "Epoch 353/1000\n",
            "891/891 [==============================] - 0s 34us/step - loss: 0.3063 - accuracy: 0.8743\n",
            "Epoch 354/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.3083 - accuracy: 0.8788\n",
            "Epoch 355/1000\n",
            "891/891 [==============================] - 0s 34us/step - loss: 0.2999 - accuracy: 0.8810\n",
            "Epoch 356/1000\n",
            "891/891 [==============================] - 0s 34us/step - loss: 0.2953 - accuracy: 0.8833\n",
            "Epoch 357/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2980 - accuracy: 0.8844\n",
            "Epoch 358/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2994 - accuracy: 0.8822\n",
            "Epoch 359/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.3045 - accuracy: 0.8822\n",
            "Epoch 360/1000\n",
            "891/891 [==============================] - 0s 34us/step - loss: 0.3088 - accuracy: 0.8788\n",
            "Epoch 361/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.3178 - accuracy: 0.8698\n",
            "Epoch 362/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2994 - accuracy: 0.8822\n",
            "Epoch 363/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.3067 - accuracy: 0.8777\n",
            "Epoch 364/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.3166 - accuracy: 0.8732\n",
            "Epoch 365/1000\n",
            "891/891 [==============================] - 0s 36us/step - loss: 0.3182 - accuracy: 0.8687\n",
            "Epoch 366/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2940 - accuracy: 0.8844\n",
            "Epoch 367/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.3056 - accuracy: 0.8687\n",
            "Epoch 368/1000\n",
            "891/891 [==============================] - 0s 30us/step - loss: 0.3086 - accuracy: 0.8754\n",
            "Epoch 369/1000\n",
            "891/891 [==============================] - 0s 34us/step - loss: 0.2982 - accuracy: 0.8844\n",
            "Epoch 370/1000\n",
            "891/891 [==============================] - 0s 37us/step - loss: 0.3174 - accuracy: 0.8889\n",
            "Epoch 371/1000\n",
            "891/891 [==============================] - 0s 39us/step - loss: 0.2943 - accuracy: 0.8765\n",
            "Epoch 372/1000\n",
            "891/891 [==============================] - 0s 35us/step - loss: 0.3012 - accuracy: 0.8754\n",
            "Epoch 373/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.3112 - accuracy: 0.8855\n",
            "Epoch 374/1000\n",
            "891/891 [==============================] - 0s 34us/step - loss: 0.3162 - accuracy: 0.8754\n",
            "Epoch 375/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.3121 - accuracy: 0.8721\n",
            "Epoch 376/1000\n",
            "891/891 [==============================] - 0s 34us/step - loss: 0.3005 - accuracy: 0.8788\n",
            "Epoch 377/1000\n",
            "891/891 [==============================] - 0s 36us/step - loss: 0.3026 - accuracy: 0.8788\n",
            "Epoch 378/1000\n",
            "891/891 [==============================] - 0s 35us/step - loss: 0.3114 - accuracy: 0.8698\n",
            "Epoch 379/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.3051 - accuracy: 0.8855\n",
            "Epoch 380/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2978 - accuracy: 0.8765\n",
            "Epoch 381/1000\n",
            "891/891 [==============================] - 0s 30us/step - loss: 0.2980 - accuracy: 0.8833\n",
            "Epoch 382/1000\n",
            "891/891 [==============================] - 0s 42us/step - loss: 0.2980 - accuracy: 0.8855\n",
            "Epoch 383/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.3050 - accuracy: 0.8732\n",
            "Epoch 384/1000\n",
            "891/891 [==============================] - 0s 34us/step - loss: 0.2906 - accuracy: 0.8799\n",
            "Epoch 385/1000\n",
            "891/891 [==============================] - 0s 30us/step - loss: 0.3027 - accuracy: 0.8810\n",
            "Epoch 386/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2944 - accuracy: 0.8765\n",
            "Epoch 387/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2984 - accuracy: 0.8765\n",
            "Epoch 388/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.3033 - accuracy: 0.8721\n",
            "Epoch 389/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.2937 - accuracy: 0.8844\n",
            "Epoch 390/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.3036 - accuracy: 0.8822\n",
            "Epoch 391/1000\n",
            "891/891 [==============================] - 0s 41us/step - loss: 0.2940 - accuracy: 0.8687\n",
            "Epoch 392/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.2969 - accuracy: 0.8866\n",
            "Epoch 393/1000\n",
            "891/891 [==============================] - 0s 34us/step - loss: 0.3029 - accuracy: 0.8765\n",
            "Epoch 394/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.3016 - accuracy: 0.8743\n",
            "Epoch 395/1000\n",
            "891/891 [==============================] - 0s 34us/step - loss: 0.3088 - accuracy: 0.8799\n",
            "Epoch 396/1000\n",
            "891/891 [==============================] - 0s 34us/step - loss: 0.3055 - accuracy: 0.8844\n",
            "Epoch 397/1000\n",
            "891/891 [==============================] - 0s 34us/step - loss: 0.3104 - accuracy: 0.8743\n",
            "Epoch 398/1000\n",
            "891/891 [==============================] - 0s 30us/step - loss: 0.3061 - accuracy: 0.8765\n",
            "Epoch 399/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2898 - accuracy: 0.8822\n",
            "Epoch 400/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2947 - accuracy: 0.8855\n",
            "Epoch 401/1000\n",
            "891/891 [==============================] - 0s 35us/step - loss: 0.3167 - accuracy: 0.8765\n",
            "Epoch 402/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.3013 - accuracy: 0.8721\n",
            "Epoch 403/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.3039 - accuracy: 0.8777\n",
            "Epoch 404/1000\n",
            "891/891 [==============================] - 0s 36us/step - loss: 0.2861 - accuracy: 0.8945\n",
            "Epoch 405/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2906 - accuracy: 0.8900\n",
            "Epoch 406/1000\n",
            "891/891 [==============================] - 0s 35us/step - loss: 0.3066 - accuracy: 0.8911\n",
            "Epoch 407/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.3016 - accuracy: 0.8765\n",
            "Epoch 408/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2924 - accuracy: 0.8788\n",
            "Epoch 409/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2953 - accuracy: 0.8844\n",
            "Epoch 410/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2885 - accuracy: 0.8788\n",
            "Epoch 411/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2742 - accuracy: 0.8923\n",
            "Epoch 412/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2887 - accuracy: 0.8889\n",
            "Epoch 413/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.3058 - accuracy: 0.8822\n",
            "Epoch 414/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2912 - accuracy: 0.8866\n",
            "Epoch 415/1000\n",
            "891/891 [==============================] - 0s 37us/step - loss: 0.2942 - accuracy: 0.8822\n",
            "Epoch 416/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.3135 - accuracy: 0.8810\n",
            "Epoch 417/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2865 - accuracy: 0.8833\n",
            "Epoch 418/1000\n",
            "891/891 [==============================] - 0s 29us/step - loss: 0.2827 - accuracy: 0.8833\n",
            "Epoch 419/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.2997 - accuracy: 0.8754\n",
            "Epoch 420/1000\n",
            "891/891 [==============================] - 0s 35us/step - loss: 0.2865 - accuracy: 0.8878\n",
            "Epoch 421/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2936 - accuracy: 0.8833\n",
            "Epoch 422/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.2979 - accuracy: 0.8709\n",
            "Epoch 423/1000\n",
            "891/891 [==============================] - 0s 34us/step - loss: 0.2991 - accuracy: 0.8765\n",
            "Epoch 424/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.2821 - accuracy: 0.8855\n",
            "Epoch 425/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.2962 - accuracy: 0.8743\n",
            "Epoch 426/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.2849 - accuracy: 0.8911\n",
            "Epoch 427/1000\n",
            "891/891 [==============================] - 0s 30us/step - loss: 0.2856 - accuracy: 0.8844\n",
            "Epoch 428/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2890 - accuracy: 0.8788\n",
            "Epoch 429/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.2663 - accuracy: 0.8967\n",
            "Epoch 430/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.2913 - accuracy: 0.8833\n",
            "Epoch 431/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.2830 - accuracy: 0.8822\n",
            "Epoch 432/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2824 - accuracy: 0.8911\n",
            "Epoch 433/1000\n",
            "891/891 [==============================] - 0s 36us/step - loss: 0.2891 - accuracy: 0.8844\n",
            "Epoch 434/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2865 - accuracy: 0.8878\n",
            "Epoch 435/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2822 - accuracy: 0.8911\n",
            "Epoch 436/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.3016 - accuracy: 0.8822\n",
            "Epoch 437/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2852 - accuracy: 0.8911\n",
            "Epoch 438/1000\n",
            "891/891 [==============================] - 0s 34us/step - loss: 0.2801 - accuracy: 0.8878\n",
            "Epoch 439/1000\n",
            "891/891 [==============================] - 0s 30us/step - loss: 0.2998 - accuracy: 0.8866\n",
            "Epoch 440/1000\n",
            "891/891 [==============================] - 0s 29us/step - loss: 0.2986 - accuracy: 0.8844\n",
            "Epoch 441/1000\n",
            "891/891 [==============================] - 0s 30us/step - loss: 0.2900 - accuracy: 0.8866\n",
            "Epoch 442/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2869 - accuracy: 0.8844\n",
            "Epoch 443/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2744 - accuracy: 0.8866\n",
            "Epoch 444/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.2967 - accuracy: 0.8765\n",
            "Epoch 445/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2959 - accuracy: 0.8833\n",
            "Epoch 446/1000\n",
            "891/891 [==============================] - 0s 35us/step - loss: 0.2952 - accuracy: 0.8788\n",
            "Epoch 447/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2941 - accuracy: 0.8799\n",
            "Epoch 448/1000\n",
            "891/891 [==============================] - 0s 36us/step - loss: 0.2923 - accuracy: 0.8777\n",
            "Epoch 449/1000\n",
            "891/891 [==============================] - 0s 34us/step - loss: 0.3001 - accuracy: 0.8799\n",
            "Epoch 450/1000\n",
            "891/891 [==============================] - 0s 34us/step - loss: 0.3015 - accuracy: 0.8844\n",
            "Epoch 451/1000\n",
            "891/891 [==============================] - 0s 30us/step - loss: 0.3048 - accuracy: 0.8732\n",
            "Epoch 452/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.2962 - accuracy: 0.8833\n",
            "Epoch 453/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2988 - accuracy: 0.8732\n",
            "Epoch 454/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2855 - accuracy: 0.8844\n",
            "Epoch 455/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2921 - accuracy: 0.8788\n",
            "Epoch 456/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.2879 - accuracy: 0.8810\n",
            "Epoch 457/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.2879 - accuracy: 0.8833\n",
            "Epoch 458/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.3063 - accuracy: 0.8799\n",
            "Epoch 459/1000\n",
            "891/891 [==============================] - 0s 35us/step - loss: 0.2772 - accuracy: 0.8900\n",
            "Epoch 460/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.3048 - accuracy: 0.8754\n",
            "Epoch 461/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2848 - accuracy: 0.8844\n",
            "Epoch 462/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2846 - accuracy: 0.8979\n",
            "Epoch 463/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2976 - accuracy: 0.8833\n",
            "Epoch 464/1000\n",
            "891/891 [==============================] - 0s 34us/step - loss: 0.2991 - accuracy: 0.8822\n",
            "Epoch 465/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.2976 - accuracy: 0.8844\n",
            "Epoch 466/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2866 - accuracy: 0.8799\n",
            "Epoch 467/1000\n",
            "891/891 [==============================] - 0s 34us/step - loss: 0.2854 - accuracy: 0.8866\n",
            "Epoch 468/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2805 - accuracy: 0.8934\n",
            "Epoch 469/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2862 - accuracy: 0.8765\n",
            "Epoch 470/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2676 - accuracy: 0.8866\n",
            "Epoch 471/1000\n",
            "891/891 [==============================] - 0s 34us/step - loss: 0.2950 - accuracy: 0.8844\n",
            "Epoch 472/1000\n",
            "891/891 [==============================] - 0s 36us/step - loss: 0.2620 - accuracy: 0.8967\n",
            "Epoch 473/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.2767 - accuracy: 0.8889\n",
            "Epoch 474/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2931 - accuracy: 0.8777\n",
            "Epoch 475/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2876 - accuracy: 0.8810\n",
            "Epoch 476/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.2833 - accuracy: 0.8866\n",
            "Epoch 477/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.2881 - accuracy: 0.8866\n",
            "Epoch 478/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.2771 - accuracy: 0.8945\n",
            "Epoch 479/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.2798 - accuracy: 0.8878\n",
            "Epoch 480/1000\n",
            "891/891 [==============================] - 0s 34us/step - loss: 0.2763 - accuracy: 0.8866\n",
            "Epoch 481/1000\n",
            "891/891 [==============================] - 0s 38us/step - loss: 0.2728 - accuracy: 0.8945\n",
            "Epoch 482/1000\n",
            "891/891 [==============================] - 0s 35us/step - loss: 0.2823 - accuracy: 0.8967\n",
            "Epoch 483/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.2865 - accuracy: 0.8889\n",
            "Epoch 484/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2811 - accuracy: 0.8878\n",
            "Epoch 485/1000\n",
            "891/891 [==============================] - 0s 39us/step - loss: 0.2789 - accuracy: 0.8956\n",
            "Epoch 486/1000\n",
            "891/891 [==============================] - 0s 34us/step - loss: 0.2745 - accuracy: 0.9001\n",
            "Epoch 487/1000\n",
            "891/891 [==============================] - 0s 34us/step - loss: 0.2772 - accuracy: 0.8855\n",
            "Epoch 488/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.2937 - accuracy: 0.8822\n",
            "Epoch 489/1000\n",
            "891/891 [==============================] - 0s 34us/step - loss: 0.2852 - accuracy: 0.8833\n",
            "Epoch 490/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.2893 - accuracy: 0.8911\n",
            "Epoch 491/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.2814 - accuracy: 0.8923\n",
            "Epoch 492/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2733 - accuracy: 0.8956\n",
            "Epoch 493/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.2762 - accuracy: 0.8855\n",
            "Epoch 494/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.2758 - accuracy: 0.8967\n",
            "Epoch 495/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2845 - accuracy: 0.8889\n",
            "Epoch 496/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2861 - accuracy: 0.8855\n",
            "Epoch 497/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2757 - accuracy: 0.8855\n",
            "Epoch 498/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.2772 - accuracy: 0.8923\n",
            "Epoch 499/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2899 - accuracy: 0.8799\n",
            "Epoch 500/1000\n",
            "891/891 [==============================] - 0s 30us/step - loss: 0.2868 - accuracy: 0.8799\n",
            "Epoch 501/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.2991 - accuracy: 0.8788\n",
            "Epoch 502/1000\n",
            "891/891 [==============================] - 0s 30us/step - loss: 0.2863 - accuracy: 0.8878\n",
            "Epoch 503/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2840 - accuracy: 0.8900\n",
            "Epoch 504/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2879 - accuracy: 0.8923\n",
            "Epoch 505/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2856 - accuracy: 0.8878\n",
            "Epoch 506/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.2745 - accuracy: 0.8945\n",
            "Epoch 507/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2858 - accuracy: 0.8878\n",
            "Epoch 508/1000\n",
            "891/891 [==============================] - 0s 30us/step - loss: 0.2702 - accuracy: 0.8979\n",
            "Epoch 509/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2920 - accuracy: 0.8754\n",
            "Epoch 510/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2894 - accuracy: 0.8866\n",
            "Epoch 511/1000\n",
            "891/891 [==============================] - 0s 34us/step - loss: 0.2818 - accuracy: 0.8900\n",
            "Epoch 512/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.2848 - accuracy: 0.8855\n",
            "Epoch 513/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2844 - accuracy: 0.8923\n",
            "Epoch 514/1000\n",
            "891/891 [==============================] - 0s 39us/step - loss: 0.2893 - accuracy: 0.8777\n",
            "Epoch 515/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2811 - accuracy: 0.8833\n",
            "Epoch 516/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.3051 - accuracy: 0.8878\n",
            "Epoch 517/1000\n",
            "891/891 [==============================] - 0s 30us/step - loss: 0.3014 - accuracy: 0.8822\n",
            "Epoch 518/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.3050 - accuracy: 0.8743\n",
            "Epoch 519/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2955 - accuracy: 0.8788\n",
            "Epoch 520/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2740 - accuracy: 0.8878\n",
            "Epoch 521/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2904 - accuracy: 0.8911\n",
            "Epoch 522/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.2880 - accuracy: 0.8788\n",
            "Epoch 523/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2834 - accuracy: 0.8754\n",
            "Epoch 524/1000\n",
            "891/891 [==============================] - 0s 35us/step - loss: 0.2791 - accuracy: 0.8777\n",
            "Epoch 525/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2954 - accuracy: 0.8833\n",
            "Epoch 526/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2942 - accuracy: 0.8799\n",
            "Epoch 527/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2956 - accuracy: 0.8833\n",
            "Epoch 528/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2814 - accuracy: 0.8765\n",
            "Epoch 529/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2727 - accuracy: 0.8777\n",
            "Epoch 530/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2914 - accuracy: 0.8911\n",
            "Epoch 531/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2700 - accuracy: 0.8911\n",
            "Epoch 532/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2774 - accuracy: 0.8911\n",
            "Epoch 533/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.3007 - accuracy: 0.8844\n",
            "Epoch 534/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.2793 - accuracy: 0.8889\n",
            "Epoch 535/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2772 - accuracy: 0.8923\n",
            "Epoch 536/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2724 - accuracy: 0.8956\n",
            "Epoch 537/1000\n",
            "891/891 [==============================] - 0s 36us/step - loss: 0.2861 - accuracy: 0.8844\n",
            "Epoch 538/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.2838 - accuracy: 0.8844\n",
            "Epoch 539/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2686 - accuracy: 0.8855\n",
            "Epoch 540/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.2689 - accuracy: 0.8900\n",
            "Epoch 541/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2822 - accuracy: 0.8822\n",
            "Epoch 542/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2684 - accuracy: 0.8911\n",
            "Epoch 543/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2867 - accuracy: 0.8855\n",
            "Epoch 544/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2737 - accuracy: 0.8923\n",
            "Epoch 545/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2718 - accuracy: 0.8799\n",
            "Epoch 546/1000\n",
            "891/891 [==============================] - 0s 34us/step - loss: 0.2812 - accuracy: 0.8878\n",
            "Epoch 547/1000\n",
            "891/891 [==============================] - 0s 38us/step - loss: 0.2676 - accuracy: 0.8934\n",
            "Epoch 548/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2752 - accuracy: 0.8844\n",
            "Epoch 549/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2844 - accuracy: 0.8866\n",
            "Epoch 550/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2607 - accuracy: 0.8923\n",
            "Epoch 551/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2722 - accuracy: 0.8900\n",
            "Epoch 552/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2775 - accuracy: 0.8822\n",
            "Epoch 553/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.2636 - accuracy: 0.8979\n",
            "Epoch 554/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2760 - accuracy: 0.8923\n",
            "Epoch 555/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2642 - accuracy: 0.8945\n",
            "Epoch 556/1000\n",
            "891/891 [==============================] - 0s 36us/step - loss: 0.2573 - accuracy: 0.8979\n",
            "Epoch 557/1000\n",
            "891/891 [==============================] - 0s 34us/step - loss: 0.2768 - accuracy: 0.8945\n",
            "Epoch 558/1000\n",
            "891/891 [==============================] - 0s 34us/step - loss: 0.2723 - accuracy: 0.8911\n",
            "Epoch 559/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2955 - accuracy: 0.8810\n",
            "Epoch 560/1000\n",
            "891/891 [==============================] - 0s 35us/step - loss: 0.2624 - accuracy: 0.8956\n",
            "Epoch 561/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2734 - accuracy: 0.8844\n",
            "Epoch 562/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2829 - accuracy: 0.8833\n",
            "Epoch 563/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2767 - accuracy: 0.8923\n",
            "Epoch 564/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2809 - accuracy: 0.8844\n",
            "Epoch 565/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.3083 - accuracy: 0.8754\n",
            "Epoch 566/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2963 - accuracy: 0.8866\n",
            "Epoch 567/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2745 - accuracy: 0.8923\n",
            "Epoch 568/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2673 - accuracy: 0.8900\n",
            "Epoch 569/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.2665 - accuracy: 0.8956\n",
            "Epoch 570/1000\n",
            "891/891 [==============================] - 0s 42us/step - loss: 0.2833 - accuracy: 0.8900\n",
            "Epoch 571/1000\n",
            "891/891 [==============================] - 0s 34us/step - loss: 0.2808 - accuracy: 0.8844\n",
            "Epoch 572/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.2697 - accuracy: 0.8923\n",
            "Epoch 573/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.2773 - accuracy: 0.8945\n",
            "Epoch 574/1000\n",
            "891/891 [==============================] - 0s 34us/step - loss: 0.2724 - accuracy: 0.8900\n",
            "Epoch 575/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2618 - accuracy: 0.8889\n",
            "Epoch 576/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.2736 - accuracy: 0.8934\n",
            "Epoch 577/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.3089 - accuracy: 0.8788\n",
            "Epoch 578/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2806 - accuracy: 0.8889\n",
            "Epoch 579/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2788 - accuracy: 0.8844\n",
            "Epoch 580/1000\n",
            "891/891 [==============================] - 0s 42us/step - loss: 0.3130 - accuracy: 0.8844\n",
            "Epoch 581/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.2900 - accuracy: 0.8788\n",
            "Epoch 582/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2659 - accuracy: 0.8934\n",
            "Epoch 583/1000\n",
            "891/891 [==============================] - 0s 34us/step - loss: 0.2600 - accuracy: 0.8911\n",
            "Epoch 584/1000\n",
            "891/891 [==============================] - 0s 30us/step - loss: 0.2654 - accuracy: 0.8945\n",
            "Epoch 585/1000\n",
            "891/891 [==============================] - 0s 36us/step - loss: 0.3003 - accuracy: 0.8866\n",
            "Epoch 586/1000\n",
            "891/891 [==============================] - 0s 35us/step - loss: 0.2807 - accuracy: 0.8721\n",
            "Epoch 587/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.2697 - accuracy: 0.9024\n",
            "Epoch 588/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.2736 - accuracy: 0.8889\n",
            "Epoch 589/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2832 - accuracy: 0.8923\n",
            "Epoch 590/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.2542 - accuracy: 0.9046\n",
            "Epoch 591/1000\n",
            "891/891 [==============================] - 0s 34us/step - loss: 0.2704 - accuracy: 0.8889\n",
            "Epoch 592/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2663 - accuracy: 0.8934\n",
            "Epoch 593/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.2766 - accuracy: 0.8945\n",
            "Epoch 594/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2697 - accuracy: 0.8923\n",
            "Epoch 595/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.2802 - accuracy: 0.8810\n",
            "Epoch 596/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2592 - accuracy: 0.9125\n",
            "Epoch 597/1000\n",
            "891/891 [==============================] - 0s 34us/step - loss: 0.2672 - accuracy: 0.8990\n",
            "Epoch 598/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2709 - accuracy: 0.9001\n",
            "Epoch 599/1000\n",
            "891/891 [==============================] - 0s 30us/step - loss: 0.2744 - accuracy: 0.8923\n",
            "Epoch 600/1000\n",
            "891/891 [==============================] - 0s 30us/step - loss: 0.2676 - accuracy: 0.8889\n",
            "Epoch 601/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.2669 - accuracy: 0.8889\n",
            "Epoch 602/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2702 - accuracy: 0.8979\n",
            "Epoch 603/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2592 - accuracy: 0.8979\n",
            "Epoch 604/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.2865 - accuracy: 0.8866\n",
            "Epoch 605/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2599 - accuracy: 0.8945\n",
            "Epoch 606/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2771 - accuracy: 0.8934\n",
            "Epoch 607/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2682 - accuracy: 0.8889\n",
            "Epoch 608/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2611 - accuracy: 0.8934\n",
            "Epoch 609/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2865 - accuracy: 0.8979\n",
            "Epoch 610/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.2624 - accuracy: 0.8900\n",
            "Epoch 611/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2551 - accuracy: 0.8990\n",
            "Epoch 612/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.2664 - accuracy: 0.8900\n",
            "Epoch 613/1000\n",
            "891/891 [==============================] - 0s 38us/step - loss: 0.2763 - accuracy: 0.9001\n",
            "Epoch 614/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2871 - accuracy: 0.8900\n",
            "Epoch 615/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.2611 - accuracy: 0.8900\n",
            "Epoch 616/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2648 - accuracy: 0.8967\n",
            "Epoch 617/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2792 - accuracy: 0.8979\n",
            "Epoch 618/1000\n",
            "891/891 [==============================] - 0s 34us/step - loss: 0.2665 - accuracy: 0.8878\n",
            "Epoch 619/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2725 - accuracy: 0.8878\n",
            "Epoch 620/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2685 - accuracy: 0.8934\n",
            "Epoch 621/1000\n",
            "891/891 [==============================] - 0s 34us/step - loss: 0.3075 - accuracy: 0.8956\n",
            "Epoch 622/1000\n",
            "891/891 [==============================] - 0s 37us/step - loss: 0.2548 - accuracy: 0.8900\n",
            "Epoch 623/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2611 - accuracy: 0.9046\n",
            "Epoch 624/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2571 - accuracy: 0.9001\n",
            "Epoch 625/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2758 - accuracy: 0.8956\n",
            "Epoch 626/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2547 - accuracy: 0.9012\n",
            "Epoch 627/1000\n",
            "891/891 [==============================] - 0s 35us/step - loss: 0.2697 - accuracy: 0.8934\n",
            "Epoch 628/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2727 - accuracy: 0.8945\n",
            "Epoch 629/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.2596 - accuracy: 0.8911\n",
            "Epoch 630/1000\n",
            "891/891 [==============================] - 0s 34us/step - loss: 0.2768 - accuracy: 0.8934\n",
            "Epoch 631/1000\n",
            "891/891 [==============================] - 0s 35us/step - loss: 0.2579 - accuracy: 0.8990\n",
            "Epoch 632/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2614 - accuracy: 0.8945\n",
            "Epoch 633/1000\n",
            "891/891 [==============================] - 0s 35us/step - loss: 0.2856 - accuracy: 0.8855\n",
            "Epoch 634/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.2654 - accuracy: 0.8956\n",
            "Epoch 635/1000\n",
            "891/891 [==============================] - 0s 43us/step - loss: 0.2659 - accuracy: 0.8923\n",
            "Epoch 636/1000\n",
            "891/891 [==============================] - 0s 40us/step - loss: 0.2699 - accuracy: 0.8923\n",
            "Epoch 637/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.2639 - accuracy: 0.8934\n",
            "Epoch 638/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.2663 - accuracy: 0.8923\n",
            "Epoch 639/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.2706 - accuracy: 0.8833\n",
            "Epoch 640/1000\n",
            "891/891 [==============================] - 0s 36us/step - loss: 0.2725 - accuracy: 0.8923\n",
            "Epoch 641/1000\n",
            "891/891 [==============================] - 0s 34us/step - loss: 0.2799 - accuracy: 0.8900\n",
            "Epoch 642/1000\n",
            "891/891 [==============================] - 0s 35us/step - loss: 0.2697 - accuracy: 0.8945\n",
            "Epoch 643/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2599 - accuracy: 0.9046\n",
            "Epoch 644/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2661 - accuracy: 0.9001\n",
            "Epoch 645/1000\n",
            "891/891 [==============================] - 0s 39us/step - loss: 0.2530 - accuracy: 0.9057\n",
            "Epoch 646/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2843 - accuracy: 0.8934\n",
            "Epoch 647/1000\n",
            "891/891 [==============================] - 0s 35us/step - loss: 0.2723 - accuracy: 0.8911\n",
            "Epoch 648/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.2897 - accuracy: 0.8833\n",
            "Epoch 649/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2796 - accuracy: 0.8810\n",
            "Epoch 650/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2689 - accuracy: 0.8911\n",
            "Epoch 651/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2723 - accuracy: 0.8979\n",
            "Epoch 652/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2513 - accuracy: 0.8923\n",
            "Epoch 653/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2681 - accuracy: 0.8866\n",
            "Epoch 654/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2533 - accuracy: 0.8979\n",
            "Epoch 655/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2715 - accuracy: 0.9012\n",
            "Epoch 656/1000\n",
            "891/891 [==============================] - 0s 30us/step - loss: 0.2724 - accuracy: 0.8956\n",
            "Epoch 657/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.2751 - accuracy: 0.8934\n",
            "Epoch 658/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2628 - accuracy: 0.8967\n",
            "Epoch 659/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2542 - accuracy: 0.8945\n",
            "Epoch 660/1000\n",
            "891/891 [==============================] - 0s 36us/step - loss: 0.2522 - accuracy: 0.9024\n",
            "Epoch 661/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2714 - accuracy: 0.8889\n",
            "Epoch 662/1000\n",
            "891/891 [==============================] - 0s 30us/step - loss: 0.2630 - accuracy: 0.9012\n",
            "Epoch 663/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2692 - accuracy: 0.8923\n",
            "Epoch 664/1000\n",
            "891/891 [==============================] - 0s 30us/step - loss: 0.2601 - accuracy: 0.9001\n",
            "Epoch 665/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2713 - accuracy: 0.8923\n",
            "Epoch 666/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2555 - accuracy: 0.9046\n",
            "Epoch 667/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.2628 - accuracy: 0.8990\n",
            "Epoch 668/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2584 - accuracy: 0.9001\n",
            "Epoch 669/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2407 - accuracy: 0.9035\n",
            "Epoch 670/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2402 - accuracy: 0.9012\n",
            "Epoch 671/1000\n",
            "891/891 [==============================] - 0s 30us/step - loss: 0.2800 - accuracy: 0.9001\n",
            "Epoch 672/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.2631 - accuracy: 0.8911\n",
            "Epoch 673/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2480 - accuracy: 0.9091\n",
            "Epoch 674/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2538 - accuracy: 0.9024\n",
            "Epoch 675/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2555 - accuracy: 0.9046\n",
            "Epoch 676/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2588 - accuracy: 0.8945\n",
            "Epoch 677/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2538 - accuracy: 0.8945\n",
            "Epoch 678/1000\n",
            "891/891 [==============================] - 0s 30us/step - loss: 0.2650 - accuracy: 0.9001\n",
            "Epoch 679/1000\n",
            "891/891 [==============================] - 0s 34us/step - loss: 0.2496 - accuracy: 0.8934\n",
            "Epoch 680/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.2752 - accuracy: 0.8900\n",
            "Epoch 681/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2477 - accuracy: 0.9035\n",
            "Epoch 682/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.2717 - accuracy: 0.8878\n",
            "Epoch 683/1000\n",
            "891/891 [==============================] - 0s 34us/step - loss: 0.2677 - accuracy: 0.9057\n",
            "Epoch 684/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2528 - accuracy: 0.9046\n",
            "Epoch 685/1000\n",
            "891/891 [==============================] - 0s 30us/step - loss: 0.2647 - accuracy: 0.8945\n",
            "Epoch 686/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2702 - accuracy: 0.8934\n",
            "Epoch 687/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2460 - accuracy: 0.9080\n",
            "Epoch 688/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2568 - accuracy: 0.9035\n",
            "Epoch 689/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2718 - accuracy: 0.8911\n",
            "Epoch 690/1000\n",
            "891/891 [==============================] - 0s 30us/step - loss: 0.2543 - accuracy: 0.9046\n",
            "Epoch 691/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2516 - accuracy: 0.9102\n",
            "Epoch 692/1000\n",
            "891/891 [==============================] - 0s 34us/step - loss: 0.2683 - accuracy: 0.8956\n",
            "Epoch 693/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2621 - accuracy: 0.8979\n",
            "Epoch 694/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2648 - accuracy: 0.8889\n",
            "Epoch 695/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2680 - accuracy: 0.8923\n",
            "Epoch 696/1000\n",
            "891/891 [==============================] - 0s 34us/step - loss: 0.2450 - accuracy: 0.9046\n",
            "Epoch 697/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.2438 - accuracy: 0.9046\n",
            "Epoch 698/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2628 - accuracy: 0.8911\n",
            "Epoch 699/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2447 - accuracy: 0.9012\n",
            "Epoch 700/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.2467 - accuracy: 0.9024\n",
            "Epoch 701/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2625 - accuracy: 0.8945\n",
            "Epoch 702/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2739 - accuracy: 0.8934\n",
            "Epoch 703/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2603 - accuracy: 0.9046\n",
            "Epoch 704/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2576 - accuracy: 0.9001\n",
            "Epoch 705/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2466 - accuracy: 0.9024\n",
            "Epoch 706/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2645 - accuracy: 0.9012\n",
            "Epoch 707/1000\n",
            "891/891 [==============================] - 0s 34us/step - loss: 0.2622 - accuracy: 0.8878\n",
            "Epoch 708/1000\n",
            "891/891 [==============================] - 0s 36us/step - loss: 0.2533 - accuracy: 0.9080\n",
            "Epoch 709/1000\n",
            "891/891 [==============================] - 0s 38us/step - loss: 0.2579 - accuracy: 0.9001\n",
            "Epoch 710/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.2491 - accuracy: 0.9046\n",
            "Epoch 711/1000\n",
            "891/891 [==============================] - 0s 40us/step - loss: 0.2417 - accuracy: 0.9035\n",
            "Epoch 712/1000\n",
            "891/891 [==============================] - 0s 30us/step - loss: 0.2606 - accuracy: 0.8979\n",
            "Epoch 713/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2733 - accuracy: 0.8967\n",
            "Epoch 714/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2581 - accuracy: 0.8990\n",
            "Epoch 715/1000\n",
            "891/891 [==============================] - 0s 35us/step - loss: 0.2784 - accuracy: 0.8956\n",
            "Epoch 716/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2750 - accuracy: 0.8889\n",
            "Epoch 717/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2525 - accuracy: 0.8945\n",
            "Epoch 718/1000\n",
            "891/891 [==============================] - 0s 34us/step - loss: 0.2525 - accuracy: 0.8956\n",
            "Epoch 719/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2536 - accuracy: 0.8956\n",
            "Epoch 720/1000\n",
            "891/891 [==============================] - 0s 30us/step - loss: 0.2413 - accuracy: 0.9024\n",
            "Epoch 721/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2580 - accuracy: 0.9035\n",
            "Epoch 722/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2381 - accuracy: 0.9057\n",
            "Epoch 723/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2482 - accuracy: 0.8967\n",
            "Epoch 724/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2322 - accuracy: 0.9068\n",
            "Epoch 725/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2576 - accuracy: 0.8990\n",
            "Epoch 726/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2548 - accuracy: 0.9102\n",
            "Epoch 727/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2579 - accuracy: 0.8878\n",
            "Epoch 728/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2730 - accuracy: 0.9068\n",
            "Epoch 729/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2565 - accuracy: 0.8956\n",
            "Epoch 730/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2444 - accuracy: 0.8934\n",
            "Epoch 731/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.2487 - accuracy: 0.9035\n",
            "Epoch 732/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.2480 - accuracy: 0.9035\n",
            "Epoch 733/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2687 - accuracy: 0.8923\n",
            "Epoch 734/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2651 - accuracy: 0.8945\n",
            "Epoch 735/1000\n",
            "891/891 [==============================] - 0s 34us/step - loss: 0.2621 - accuracy: 0.8979\n",
            "Epoch 736/1000\n",
            "891/891 [==============================] - 0s 34us/step - loss: 0.2577 - accuracy: 0.9125\n",
            "Epoch 737/1000\n",
            "891/891 [==============================] - 0s 38us/step - loss: 0.2644 - accuracy: 0.9113\n",
            "Epoch 738/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2410 - accuracy: 0.9068\n",
            "Epoch 739/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2566 - accuracy: 0.8990\n",
            "Epoch 740/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2656 - accuracy: 0.8878\n",
            "Epoch 741/1000\n",
            "891/891 [==============================] - 0s 30us/step - loss: 0.2616 - accuracy: 0.9012\n",
            "Epoch 742/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2544 - accuracy: 0.9102\n",
            "Epoch 743/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2620 - accuracy: 0.8911\n",
            "Epoch 744/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2414 - accuracy: 0.8956\n",
            "Epoch 745/1000\n",
            "891/891 [==============================] - 0s 44us/step - loss: 0.2533 - accuracy: 0.8956\n",
            "Epoch 746/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2492 - accuracy: 0.9012\n",
            "Epoch 747/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2454 - accuracy: 0.9035\n",
            "Epoch 748/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.2696 - accuracy: 0.9057\n",
            "Epoch 749/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2428 - accuracy: 0.9012\n",
            "Epoch 750/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2580 - accuracy: 0.9046\n",
            "Epoch 751/1000\n",
            "891/891 [==============================] - 0s 36us/step - loss: 0.2658 - accuracy: 0.8911\n",
            "Epoch 752/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2383 - accuracy: 0.9057\n",
            "Epoch 753/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2451 - accuracy: 0.9046\n",
            "Epoch 754/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2424 - accuracy: 0.9158\n",
            "Epoch 755/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2387 - accuracy: 0.9169\n",
            "Epoch 756/1000\n",
            "891/891 [==============================] - 0s 30us/step - loss: 0.2485 - accuracy: 0.8945\n",
            "Epoch 757/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2470 - accuracy: 0.8967\n",
            "Epoch 758/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2348 - accuracy: 0.9035\n",
            "Epoch 759/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2552 - accuracy: 0.8979\n",
            "Epoch 760/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2719 - accuracy: 0.8956\n",
            "Epoch 761/1000\n",
            "891/891 [==============================] - 0s 30us/step - loss: 0.2431 - accuracy: 0.9035\n",
            "Epoch 762/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2608 - accuracy: 0.8990\n",
            "Epoch 763/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2607 - accuracy: 0.9035\n",
            "Epoch 764/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.2438 - accuracy: 0.9046\n",
            "Epoch 765/1000\n",
            "891/891 [==============================] - 0s 34us/step - loss: 0.2453 - accuracy: 0.9001\n",
            "Epoch 766/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2664 - accuracy: 0.8979\n",
            "Epoch 767/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2858 - accuracy: 0.8833\n",
            "Epoch 768/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2366 - accuracy: 0.9091\n",
            "Epoch 769/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2636 - accuracy: 0.8923\n",
            "Epoch 770/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2576 - accuracy: 0.8923\n",
            "Epoch 771/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2585 - accuracy: 0.8979\n",
            "Epoch 772/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2460 - accuracy: 0.9057\n",
            "Epoch 773/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2526 - accuracy: 0.9001\n",
            "Epoch 774/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2532 - accuracy: 0.9035\n",
            "Epoch 775/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2465 - accuracy: 0.8967\n",
            "Epoch 776/1000\n",
            "891/891 [==============================] - 0s 36us/step - loss: 0.2532 - accuracy: 0.8923\n",
            "Epoch 777/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2577 - accuracy: 0.9125\n",
            "Epoch 778/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.2494 - accuracy: 0.9012\n",
            "Epoch 779/1000\n",
            "891/891 [==============================] - 0s 35us/step - loss: 0.2747 - accuracy: 0.8911\n",
            "Epoch 780/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2606 - accuracy: 0.8956\n",
            "Epoch 781/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2565 - accuracy: 0.8889\n",
            "Epoch 782/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2584 - accuracy: 0.8967\n",
            "Epoch 783/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2594 - accuracy: 0.8990\n",
            "Epoch 784/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2603 - accuracy: 0.9024\n",
            "Epoch 785/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2499 - accuracy: 0.8911\n",
            "Epoch 786/1000\n",
            "891/891 [==============================] - 0s 30us/step - loss: 0.2500 - accuracy: 0.9091\n",
            "Epoch 787/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.2515 - accuracy: 0.8967\n",
            "Epoch 788/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2639 - accuracy: 0.8967\n",
            "Epoch 789/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2611 - accuracy: 0.8990\n",
            "Epoch 790/1000\n",
            "891/891 [==============================] - 0s 30us/step - loss: 0.2590 - accuracy: 0.9046\n",
            "Epoch 791/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2471 - accuracy: 0.9001\n",
            "Epoch 792/1000\n",
            "891/891 [==============================] - 0s 34us/step - loss: 0.2451 - accuracy: 0.9080\n",
            "Epoch 793/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2539 - accuracy: 0.9068\n",
            "Epoch 794/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2513 - accuracy: 0.9024\n",
            "Epoch 795/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2586 - accuracy: 0.9001\n",
            "Epoch 796/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2460 - accuracy: 0.9024\n",
            "Epoch 797/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2442 - accuracy: 0.9136\n",
            "Epoch 798/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2510 - accuracy: 0.8967\n",
            "Epoch 799/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2529 - accuracy: 0.8945\n",
            "Epoch 800/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2574 - accuracy: 0.9024\n",
            "Epoch 801/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2474 - accuracy: 0.8945\n",
            "Epoch 802/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2356 - accuracy: 0.9035\n",
            "Epoch 803/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2972 - accuracy: 0.8979\n",
            "Epoch 804/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2564 - accuracy: 0.8979\n",
            "Epoch 805/1000\n",
            "891/891 [==============================] - 0s 34us/step - loss: 0.2516 - accuracy: 0.8990\n",
            "Epoch 806/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2301 - accuracy: 0.9136\n",
            "Epoch 807/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2519 - accuracy: 0.9046\n",
            "Epoch 808/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2640 - accuracy: 0.9046\n",
            "Epoch 809/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2540 - accuracy: 0.8979\n",
            "Epoch 810/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2514 - accuracy: 0.9001\n",
            "Epoch 811/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.2513 - accuracy: 0.9046\n",
            "Epoch 812/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2406 - accuracy: 0.9057\n",
            "Epoch 813/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2320 - accuracy: 0.9158\n",
            "Epoch 814/1000\n",
            "891/891 [==============================] - 0s 35us/step - loss: 0.2573 - accuracy: 0.8945\n",
            "Epoch 815/1000\n",
            "891/891 [==============================] - 0s 36us/step - loss: 0.2459 - accuracy: 0.9080\n",
            "Epoch 816/1000\n",
            "891/891 [==============================] - 0s 35us/step - loss: 0.2575 - accuracy: 0.9057\n",
            "Epoch 817/1000\n",
            "891/891 [==============================] - 0s 34us/step - loss: 0.2638 - accuracy: 0.8934\n",
            "Epoch 818/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.2673 - accuracy: 0.8934\n",
            "Epoch 819/1000\n",
            "891/891 [==============================] - 0s 34us/step - loss: 0.2579 - accuracy: 0.8923\n",
            "Epoch 820/1000\n",
            "891/891 [==============================] - 0s 35us/step - loss: 0.2465 - accuracy: 0.8979\n",
            "Epoch 821/1000\n",
            "891/891 [==============================] - 0s 35us/step - loss: 0.2455 - accuracy: 0.9057\n",
            "Epoch 822/1000\n",
            "891/891 [==============================] - 0s 34us/step - loss: 0.2426 - accuracy: 0.9057\n",
            "Epoch 823/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2484 - accuracy: 0.9035\n",
            "Epoch 824/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2452 - accuracy: 0.8945\n",
            "Epoch 825/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2478 - accuracy: 0.9057\n",
            "Epoch 826/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2444 - accuracy: 0.9035\n",
            "Epoch 827/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2408 - accuracy: 0.9024\n",
            "Epoch 828/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2633 - accuracy: 0.8923\n",
            "Epoch 829/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2604 - accuracy: 0.9035\n",
            "Epoch 830/1000\n",
            "891/891 [==============================] - 0s 34us/step - loss: 0.2258 - accuracy: 0.9192\n",
            "Epoch 831/1000\n",
            "891/891 [==============================] - 0s 35us/step - loss: 0.2557 - accuracy: 0.8979\n",
            "Epoch 832/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2598 - accuracy: 0.8979\n",
            "Epoch 833/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2516 - accuracy: 0.9035\n",
            "Epoch 834/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2212 - accuracy: 0.9113\n",
            "Epoch 835/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2572 - accuracy: 0.9012\n",
            "Epoch 836/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2264 - accuracy: 0.9136\n",
            "Epoch 837/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2484 - accuracy: 0.9024\n",
            "Epoch 838/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2444 - accuracy: 0.8979\n",
            "Epoch 839/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2509 - accuracy: 0.8979\n",
            "Epoch 840/1000\n",
            "891/891 [==============================] - 0s 30us/step - loss: 0.2469 - accuracy: 0.9057\n",
            "Epoch 841/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2504 - accuracy: 0.9012\n",
            "Epoch 842/1000\n",
            "891/891 [==============================] - 0s 30us/step - loss: 0.2224 - accuracy: 0.9125\n",
            "Epoch 843/1000\n",
            "891/891 [==============================] - 0s 43us/step - loss: 0.2331 - accuracy: 0.9057\n",
            "Epoch 844/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.2437 - accuracy: 0.9091\n",
            "Epoch 845/1000\n",
            "891/891 [==============================] - 0s 37us/step - loss: 0.2429 - accuracy: 0.9001\n",
            "Epoch 846/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2345 - accuracy: 0.9102\n",
            "Epoch 847/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2468 - accuracy: 0.9057\n",
            "Epoch 848/1000\n",
            "891/891 [==============================] - 0s 40us/step - loss: 0.2382 - accuracy: 0.9158\n",
            "Epoch 849/1000\n",
            "891/891 [==============================] - 0s 36us/step - loss: 0.2520 - accuracy: 0.9024\n",
            "Epoch 850/1000\n",
            "891/891 [==============================] - 0s 39us/step - loss: 0.2476 - accuracy: 0.9035\n",
            "Epoch 851/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2554 - accuracy: 0.9012\n",
            "Epoch 852/1000\n",
            "891/891 [==============================] - 0s 37us/step - loss: 0.2382 - accuracy: 0.9113\n",
            "Epoch 853/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.2560 - accuracy: 0.8967\n",
            "Epoch 854/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2497 - accuracy: 0.9035\n",
            "Epoch 855/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.2546 - accuracy: 0.9001\n",
            "Epoch 856/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2331 - accuracy: 0.9113\n",
            "Epoch 857/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2413 - accuracy: 0.9035\n",
            "Epoch 858/1000\n",
            "891/891 [==============================] - 0s 35us/step - loss: 0.2408 - accuracy: 0.9035\n",
            "Epoch 859/1000\n",
            "891/891 [==============================] - 0s 34us/step - loss: 0.2358 - accuracy: 0.9080\n",
            "Epoch 860/1000\n",
            "891/891 [==============================] - 0s 36us/step - loss: 0.2555 - accuracy: 0.9057\n",
            "Epoch 861/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.2531 - accuracy: 0.8990\n",
            "Epoch 862/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.2542 - accuracy: 0.8923\n",
            "Epoch 863/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2430 - accuracy: 0.9035\n",
            "Epoch 864/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2470 - accuracy: 0.9046\n",
            "Epoch 865/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2494 - accuracy: 0.9046\n",
            "Epoch 866/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.2371 - accuracy: 0.9147\n",
            "Epoch 867/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2433 - accuracy: 0.8967\n",
            "Epoch 868/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2509 - accuracy: 0.9091\n",
            "Epoch 869/1000\n",
            "891/891 [==============================] - 0s 35us/step - loss: 0.2599 - accuracy: 0.9012\n",
            "Epoch 870/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2564 - accuracy: 0.9001\n",
            "Epoch 871/1000\n",
            "891/891 [==============================] - 0s 35us/step - loss: 0.2562 - accuracy: 0.9091\n",
            "Epoch 872/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2349 - accuracy: 0.9035\n",
            "Epoch 873/1000\n",
            "891/891 [==============================] - 0s 34us/step - loss: 0.2410 - accuracy: 0.9046\n",
            "Epoch 874/1000\n",
            "891/891 [==============================] - 0s 39us/step - loss: 0.2678 - accuracy: 0.9024\n",
            "Epoch 875/1000\n",
            "891/891 [==============================] - 0s 37us/step - loss: 0.2405 - accuracy: 0.9091\n",
            "Epoch 876/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2565 - accuracy: 0.9046\n",
            "Epoch 877/1000\n",
            "891/891 [==============================] - 0s 39us/step - loss: 0.2347 - accuracy: 0.9091\n",
            "Epoch 878/1000\n",
            "891/891 [==============================] - 0s 34us/step - loss: 0.2391 - accuracy: 0.9169\n",
            "Epoch 879/1000\n",
            "891/891 [==============================] - 0s 37us/step - loss: 0.2726 - accuracy: 0.8979\n",
            "Epoch 880/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2482 - accuracy: 0.8934\n",
            "Epoch 881/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.2503 - accuracy: 0.8990\n",
            "Epoch 882/1000\n",
            "891/891 [==============================] - 0s 34us/step - loss: 0.2436 - accuracy: 0.9102\n",
            "Epoch 883/1000\n",
            "891/891 [==============================] - 0s 34us/step - loss: 0.2578 - accuracy: 0.9012\n",
            "Epoch 884/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2569 - accuracy: 0.9046\n",
            "Epoch 885/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2270 - accuracy: 0.9102\n",
            "Epoch 886/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2501 - accuracy: 0.9035\n",
            "Epoch 887/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2413 - accuracy: 0.9035\n",
            "Epoch 888/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2316 - accuracy: 0.9147\n",
            "Epoch 889/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2484 - accuracy: 0.8990\n",
            "Epoch 890/1000\n",
            "891/891 [==============================] - 0s 34us/step - loss: 0.2489 - accuracy: 0.8956\n",
            "Epoch 891/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2469 - accuracy: 0.9035\n",
            "Epoch 892/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2548 - accuracy: 0.9024\n",
            "Epoch 893/1000\n",
            "891/891 [==============================] - 0s 30us/step - loss: 0.2494 - accuracy: 0.9068\n",
            "Epoch 894/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2444 - accuracy: 0.8979\n",
            "Epoch 895/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2635 - accuracy: 0.9057\n",
            "Epoch 896/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2370 - accuracy: 0.9080\n",
            "Epoch 897/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.2313 - accuracy: 0.9091\n",
            "Epoch 898/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2409 - accuracy: 0.8967\n",
            "Epoch 899/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2466 - accuracy: 0.9035\n",
            "Epoch 900/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.2538 - accuracy: 0.9046\n",
            "Epoch 901/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.2467 - accuracy: 0.9091\n",
            "Epoch 902/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.2417 - accuracy: 0.9035\n",
            "Epoch 903/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.2599 - accuracy: 0.9001\n",
            "Epoch 904/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2472 - accuracy: 0.9024\n",
            "Epoch 905/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2429 - accuracy: 0.8945\n",
            "Epoch 906/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.2552 - accuracy: 0.9024\n",
            "Epoch 907/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2469 - accuracy: 0.8967\n",
            "Epoch 908/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2547 - accuracy: 0.8990\n",
            "Epoch 909/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2634 - accuracy: 0.8956\n",
            "Epoch 910/1000\n",
            "891/891 [==============================] - 0s 36us/step - loss: 0.2355 - accuracy: 0.8945\n",
            "Epoch 911/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2428 - accuracy: 0.9024\n",
            "Epoch 912/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2533 - accuracy: 0.8990\n",
            "Epoch 913/1000\n",
            "891/891 [==============================] - 0s 34us/step - loss: 0.2332 - accuracy: 0.9125\n",
            "Epoch 914/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2438 - accuracy: 0.9068\n",
            "Epoch 915/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2369 - accuracy: 0.9046\n",
            "Epoch 916/1000\n",
            "891/891 [==============================] - 0s 30us/step - loss: 0.2261 - accuracy: 0.9158\n",
            "Epoch 917/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2357 - accuracy: 0.9102\n",
            "Epoch 918/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2658 - accuracy: 0.9035\n",
            "Epoch 919/1000\n",
            "891/891 [==============================] - 0s 30us/step - loss: 0.2348 - accuracy: 0.9068\n",
            "Epoch 920/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2372 - accuracy: 0.9125\n",
            "Epoch 921/1000\n",
            "891/891 [==============================] - 0s 30us/step - loss: 0.2238 - accuracy: 0.9125\n",
            "Epoch 922/1000\n",
            "891/891 [==============================] - 0s 29us/step - loss: 0.2312 - accuracy: 0.9136\n",
            "Epoch 923/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.2376 - accuracy: 0.9068\n",
            "Epoch 924/1000\n",
            "891/891 [==============================] - 0s 30us/step - loss: 0.2434 - accuracy: 0.9001\n",
            "Epoch 925/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.2429 - accuracy: 0.9080\n",
            "Epoch 926/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2307 - accuracy: 0.9024\n",
            "Epoch 927/1000\n",
            "891/891 [==============================] - 0s 30us/step - loss: 0.2331 - accuracy: 0.9113\n",
            "Epoch 928/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.2490 - accuracy: 0.9125\n",
            "Epoch 929/1000\n",
            "891/891 [==============================] - 0s 30us/step - loss: 0.2383 - accuracy: 0.9113\n",
            "Epoch 930/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2380 - accuracy: 0.9046\n",
            "Epoch 931/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2224 - accuracy: 0.9057\n",
            "Epoch 932/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2330 - accuracy: 0.9068\n",
            "Epoch 933/1000\n",
            "891/891 [==============================] - 0s 30us/step - loss: 0.2730 - accuracy: 0.8923\n",
            "Epoch 934/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2455 - accuracy: 0.9035\n",
            "Epoch 935/1000\n",
            "891/891 [==============================] - 0s 30us/step - loss: 0.2420 - accuracy: 0.9035\n",
            "Epoch 936/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2363 - accuracy: 0.9125\n",
            "Epoch 937/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.2406 - accuracy: 0.9113\n",
            "Epoch 938/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2636 - accuracy: 0.8967\n",
            "Epoch 939/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2424 - accuracy: 0.9080\n",
            "Epoch 940/1000\n",
            "891/891 [==============================] - 0s 34us/step - loss: 0.2302 - accuracy: 0.9091\n",
            "Epoch 941/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2435 - accuracy: 0.8990\n",
            "Epoch 942/1000\n",
            "891/891 [==============================] - 0s 39us/step - loss: 0.2446 - accuracy: 0.9057\n",
            "Epoch 943/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.2244 - accuracy: 0.9125\n",
            "Epoch 944/1000\n",
            "891/891 [==============================] - 0s 40us/step - loss: 0.2278 - accuracy: 0.9136\n",
            "Epoch 945/1000\n",
            "891/891 [==============================] - 0s 34us/step - loss: 0.2411 - accuracy: 0.9091\n",
            "Epoch 946/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.2229 - accuracy: 0.9147\n",
            "Epoch 947/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.2291 - accuracy: 0.9068\n",
            "Epoch 948/1000\n",
            "891/891 [==============================] - 0s 35us/step - loss: 0.2388 - accuracy: 0.9035\n",
            "Epoch 949/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2486 - accuracy: 0.9035\n",
            "Epoch 950/1000\n",
            "891/891 [==============================] - 0s 34us/step - loss: 0.2437 - accuracy: 0.9091\n",
            "Epoch 951/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2517 - accuracy: 0.9012\n",
            "Epoch 952/1000\n",
            "891/891 [==============================] - 0s 34us/step - loss: 0.2384 - accuracy: 0.9080\n",
            "Epoch 953/1000\n",
            "891/891 [==============================] - 0s 30us/step - loss: 0.2280 - accuracy: 0.9169\n",
            "Epoch 954/1000\n",
            "891/891 [==============================] - 0s 35us/step - loss: 0.2410 - accuracy: 0.9147\n",
            "Epoch 955/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2356 - accuracy: 0.9125\n",
            "Epoch 956/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2343 - accuracy: 0.9080\n",
            "Epoch 957/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2411 - accuracy: 0.9024\n",
            "Epoch 958/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2440 - accuracy: 0.9113\n",
            "Epoch 959/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2420 - accuracy: 0.9012\n",
            "Epoch 960/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2379 - accuracy: 0.9113\n",
            "Epoch 961/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2455 - accuracy: 0.9169\n",
            "Epoch 962/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2384 - accuracy: 0.9147\n",
            "Epoch 963/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2425 - accuracy: 0.9158\n",
            "Epoch 964/1000\n",
            "891/891 [==============================] - 0s 38us/step - loss: 0.2432 - accuracy: 0.8979\n",
            "Epoch 965/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.2707 - accuracy: 0.8911\n",
            "Epoch 966/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2579 - accuracy: 0.8945\n",
            "Epoch 967/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2628 - accuracy: 0.8923\n",
            "Epoch 968/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2544 - accuracy: 0.8923\n",
            "Epoch 969/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2461 - accuracy: 0.9068\n",
            "Epoch 970/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2619 - accuracy: 0.8934\n",
            "Epoch 971/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2392 - accuracy: 0.9102\n",
            "Epoch 972/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.2536 - accuracy: 0.8945\n",
            "Epoch 973/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2529 - accuracy: 0.9024\n",
            "Epoch 974/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2361 - accuracy: 0.9068\n",
            "Epoch 975/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2666 - accuracy: 0.9046\n",
            "Epoch 976/1000\n",
            "891/891 [==============================] - 0s 34us/step - loss: 0.2389 - accuracy: 0.9001\n",
            "Epoch 977/1000\n",
            "891/891 [==============================] - 0s 37us/step - loss: 0.2340 - accuracy: 0.9102\n",
            "Epoch 978/1000\n",
            "891/891 [==============================] - 0s 34us/step - loss: 0.2367 - accuracy: 0.9024\n",
            "Epoch 979/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2377 - accuracy: 0.9024\n",
            "Epoch 980/1000\n",
            "891/891 [==============================] - 0s 30us/step - loss: 0.2352 - accuracy: 0.9125\n",
            "Epoch 981/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2360 - accuracy: 0.9057\n",
            "Epoch 982/1000\n",
            "891/891 [==============================] - 0s 30us/step - loss: 0.2283 - accuracy: 0.9057\n",
            "Epoch 983/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2357 - accuracy: 0.9035\n",
            "Epoch 984/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2326 - accuracy: 0.9125\n",
            "Epoch 985/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2370 - accuracy: 0.9136\n",
            "Epoch 986/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2477 - accuracy: 0.8967\n",
            "Epoch 987/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2432 - accuracy: 0.8990\n",
            "Epoch 988/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2329 - accuracy: 0.9113\n",
            "Epoch 989/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2426 - accuracy: 0.9102\n",
            "Epoch 990/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2695 - accuracy: 0.8956\n",
            "Epoch 991/1000\n",
            "891/891 [==============================] - 0s 33us/step - loss: 0.2555 - accuracy: 0.8956\n",
            "Epoch 992/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2506 - accuracy: 0.8967\n",
            "Epoch 993/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2285 - accuracy: 0.9080\n",
            "Epoch 994/1000\n",
            "891/891 [==============================] - 0s 32us/step - loss: 0.2419 - accuracy: 0.9035\n",
            "Epoch 995/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2455 - accuracy: 0.9012\n",
            "Epoch 996/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2381 - accuracy: 0.9080\n",
            "Epoch 997/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2400 - accuracy: 0.9057\n",
            "Epoch 998/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2343 - accuracy: 0.9136\n",
            "Epoch 999/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2241 - accuracy: 0.9080\n",
            "Epoch 1000/1000\n",
            "891/891 [==============================] - 0s 31us/step - loss: 0.2461 - accuracy: 0.9001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vpffOhaxltym",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "outputId": "503fb825-8138-4e8c-fed3-f7d24c2f76f1"
      },
      "source": [
        "Log_predict = pd.DataFrame(Log_predict, columns = ['Survived'])\n",
        "SVM_L_predict = pd.DataFrame(SVM_L_predict, columns = ['Survived'])\n",
        "SVM_NL_predict = pd.DataFrame(SVM_NL_predict, columns = ['Survived'])\n",
        "DNN_predict = pd.DataFrame(DNN_predict, columns = ['Survived'])\n",
        "Forest_predict = pd.DataFrame(Forest_predict, columns = ['Survived'])\n",
        "\n",
        "# Make submission file\n",
        "submission = pd.read_csv(\"gender_submission.csv\")\n",
        "\n",
        "Log_predict = pd.concat((submission.iloc[:, 0], Log_predict), axis = 1)\n",
        "Log_predict.to_csv('Logistic_predict.csv', index = False)\n",
        "\n",
        "SVM_L_predict = pd.concat((submission.iloc[:, 0], SVM_L_predict), axis = 1)\n",
        "SVM_L_predict.to_csv('SVM_Linear_predict.csv', index = False)\n",
        "\n",
        "SVM_NL_predict = pd.concat((submission.iloc[:, 0], SVM_NL_predict), axis = 1)\n",
        "SVM_NL_predict.to_csv('SVM_Nonlinear_predict.csv', index = False)\n",
        "\n",
        "DNN_predict = pd.concat((submission.iloc[:, 0], DNN_predict), axis = 1)\n",
        "DNN_predict.to_csv('DNN_predict.csv', index = False)\n",
        "\n",
        "Forest_predict = pd.concat((submission.iloc[:, 0], Forest_predict), axis = 1)\n",
        "Forest_predict.to_csv('RandomForest_predict.csv', index = False)"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexingError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexingError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-114-d9dc6b68e284>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0msubmission\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'PassengerId'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mLog_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubmission\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLog_predict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mLog_predict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Logistic_predict.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1760\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mKeyError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1761\u001b[0m                     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1762\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1763\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1764\u001b[0m             \u001b[0;31m# we by definition only have the 0th axis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_tuple\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m   2065\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2066\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2067\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_valid_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2068\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2069\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_lowerdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_has_valid_tuple\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    699\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mIndexingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Too many indexers\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexingError\u001b[0m: Too many indexers"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecGScoxaoBq3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}